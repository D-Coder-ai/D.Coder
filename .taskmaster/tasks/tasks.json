{
  "r1-beta": {
    "tasks": [
      {
        "id": 1,
        "title": "AI Gateway (Kong) - R1 configuration and deployment",
        "description": "Deploy and configure Kong AI Gateway 3.11+ with multi-LLM routing, Redis semantic caching, per-tenant rate limiting, alert-only guardrails, quota event emission, health/metrics, and documentation.",
        "details": "Implementation outline:\n- Tech/Infra: Kong 3.11+ with AI Gateway features, Postgres 15+ for Kong DB, Redis 7+ for semantic cache, Prometheus exporter, NATS producer sidecar for events.\n- Config model: Per-tenant routes named `llm.{provider}.{model}`; restrict upstreams to allowlisted provider endpoints (OpenAI, Anthropic, Google/Vertex, Groq). No failover.\n- Plugins:\n  - ai-proxy/route transformer for provider auth header injection from Platform API (later), or environment/secret for MVP.\n  - Semantic cache plugin backed by Redis with TTL (e.g., 24h), namespace keys by `tenantId:model:hash(prompt+params)`; compression plugin to shrink prompts 20–30% (e.g., tiktoken truncation + GPT cache key).\n  - Rate limiting plugin per tenant (X-Tenant-Id) with Redis counters.\n  - Guardrails: Install LLM Guard/LlamaGuard in detect-only mode; route plugin to log alerts to Kong logs and emit quota/alert metrics.\n- Quotas: Add custom plugin or serverless pre-function to increment quota counters and publish `quota.updated` on NATS with envelope {tenantId, tokensIn, tokensOut, cost, ts, requestId}.\n- Observability: Enable Prometheus metrics endpoint; propagate tracing headers (X-Trace-Id) and OTel if supported; redact logs.\n- Health: `/health` upstream checks and Kong node readiness.\n- Docs: Onboarding steps per provider; config examples; per-tenant snippet for route/consumer/credential.\nPseudo-code (Kong declarative snippet - YAML):\nservices:\n- name: openai\n  url: https://api.openai.com/v1\n  routes:\n  - name: llm.openai.gpt-4o\n    paths: [/v1/llm/openai/gpt-4o]\n    plugins:\n    - name: rate-limiting\n      config: {limit_by: header, header_name: X-Tenant-Id, policy: redis, redis_host: redis, minute: 600}\n    - name: response-transformer\n    - name: ai-semantic-cache\n      config: {redis_host: redis, ttl: 86400}\n    - name: opa-llm-guard\n      config: {mode: detect}\n- Publish to NATS (pseudocode, serverless function):\nfunction on_response(ctx)\n  local event = {subject='quota.updated', data={tenantId=ctx.request.headers['X-Tenant-Id'], tokensIn=..., tokensOut=..., requestId=ctx.request.headers['X-Request-Id'], ts=os.time()}}\n  nats.publish(event)\nend\n",
        "testStrategy": "- Config validation: `kong check` on declarative config; unit tests for custom plugins (Lua/Go) with mocked Redis/NATS.\n- Integration: Send requests via Kong to each provider sandbox with tenant headers; verify rate limits, cache hits (metrics), and that alerts are logged but do not block.\n- Observability: Prometheus scrape returns metrics; health endpoints return 200.\n- Quotas: Subscribe test NATS consumer, assert `quota.updated` payloads match traffic; cross-check with Platform API mirrors in later tasks.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap core infra: Postgres, Redis, NATS sidecar, and Prometheus",
            "description": "Provision base infrastructure required for Kong AI Gateway with Postgres 15+, Redis 7+, NATS, and Prometheus.",
            "dependencies": [],
            "details": "Deliverables:\n- Docker Compose or Helm charts for Postgres, Redis, NATS, Prometheus\n- Network and credentials for components; volumes for Postgres and Redis persistence\n- Makefile tasks: make up, make down, make logs\nConfig snippet (compose excerpt, minimal):\n- services:\n  - postgres: image: postgres:15, env: POSTGRES_PASSWORD, volume: pgdata\n  - redis: image: redis:7, command: redis-server --appendonly yes\n  - nats: image: nats:2, command: -js -sd /data, ports: 4222, 8222\n  - prometheus: image: prom/prometheus, config: scrape Kong /metrics\nTest cases:\n- Postgres accepts connections and initializes database kong\n- Redis responds to PING and persists data\n- NATS pub sub roundtrip using nats req and nats reply\n- Prometheus target up and scraping\nRollback plan:\n- Compose down while preserving persistent volumes; revert Helm release to previous revision\n- Keep DB snapshot before migration; restore if needed",
            "status": "done",
            "testStrategy": "Container health checks, port checks, and functional PING/pub-sub verification with scripted smoke tests."
          },
          {
            "id": 2,
            "title": "Define provider upstreams and enforce allowlist of LLM endpoints",
            "description": "Create Kong services for OpenAI, Anthropic, Google Vertex, and Groq; restrict to allowlisted upstreams only.",
            "dependencies": [
              1
            ],
            "details": "Deliverables:\n- kong.yaml entries for strictly allowlisted provider services\n- Admin API hardening to prevent dynamic upstream creation in prod\nConfig snippet (kong.yaml excerpt):\n- services:\n  - name: openai  url: https://api.openai.com/v1\n  - name: anthropic  url: https://api.anthropic.com/v1\n  - name: google  url: https://generativelanguage.googleapis.com/v1beta\n  - name: groq  url: https://api.groq.com/openai/v1\n- disable creation of other services via CI policy and deck allowlist\nTest cases:\n- Requests to non-allowlisted hosts do not route (404)\n- deck diff shows only allowlisted services; attempts to add others are rejected by policy\nRollback plan:\n- Revert kong.yaml to previous git tag and deck sync\n- Temporarily allow only previously deployed upstreams until investigation completes\n<info added on 2025-10-23T21:38:38.266Z>\nImplementation progress (Docker Compose, R1 target)\n\nDone:\n- Added declarative services in infra/kong/kong.yaml for OpenAI, Anthropic, Google Generative Language API, and Groq (no routes/plugins yet).\n- Added OPA/Conftest allowlist policy under infra/policies (kong_services.rego and allowed_provider_hosts.yaml) to restrict upstreams to the approved set.\n- Documented Admin API hardening for Compose in infra/kong/README.md (RBAC enabled, Admin listen bound to localhost, status listen read-only).\n- Lint checks passing for all new files.\n\nNext:\n- Add/run conftest validation in local and CI workflows (validate-config) once conftest is available in the environment.\n\nNotes:\n- Aligned with R1 constraints: BYO LLM, public egress allowlist, no failover; guardrails are alert-only and deferred to later subtasks.\n</info added on 2025-10-23T21:38:38.266Z>",
            "status": "done",
            "testStrategy": "Declarative config validation via kong check and deck diff; negative tests against unapproved endpoints."
          },
          {
            "id": 3,
            "title": "Author declarative config and per-tenant LLM routes",
            "description": "Create routes named llm.provider.model and base scaffolding for tenants, using Kong declarative config.",
            "dependencies": [
              1,
              2
            ],
            "details": "Deliverables:\n- kong.yaml with services and routes like llm.openai.gpt-4o and llm.anthropic.claude-3\n- Path scheme: /v1/llm/{provider}/{model}; no failover; tags for tenant management\nConfig snippet:\n- services:\n  - name: openai\n    url: https://api.openai.com/v1\n    routes:\n    - name: llm.openai.gpt-4o\n      paths: [/v1/llm/openai/gpt-4o]\n      strip_path: false\n  - name: anthropic\n    url: https://api.anthropic.com/v1\n    routes:\n    - name: llm.anthropic.claude-3\n      paths: [/v1/llm/anthropic/claude-3]\n      strip_path: false\nTest cases:\n- kong check passes; deck sync is idempotent\n- curl to route forwards to correct upstream path and host\n- No failover occurs when upstream is down\nRollback plan:\n- Keep previous kong.yaml snapshot; deck sync to revert\n- Route disable by setting protocols: [] or removing route entry and syncing\n<info added on 2025-10-23T22:14:59.674Z>\nUpdate: Declarative per-tenant LLM routes implemented and validated\n\n- Implemented 12 routes in infra/kong/kong.yaml following llm.{provider}.{model} and path /v1/llm/{provider}/{model}; providers: openai, anthropic, google, groq\n  - OpenAI (→ /v1/responses): llm.openai.gpt-5, llm.openai.gpt-5-chat, llm.openai.gpt-4-1, llm.openai.gpt-5-codex\n  - Anthropic (→ /v1/messages): llm.anthropic.claude-sonnet-4-5, llm.anthropic.claude-opus-4-1\n  - Google (→ /v1beta/models/{model}:generateContent): llm.google.gemini-2-5-pro, llm.google.gemini-2-5-flash, llm.google.gemini-2-5-flash-lite\n  - Groq (→ /openai/v1/chat/completions): llm.groq.gpt-oss-120b, llm.groq.gpt-oss-20b, llm.groq.kimi-k2-instruct\n- Each route uses request-transformer to rewrite standardized paths to provider-native endpoints; strip_path: false and preserve_host: false; no other plugins applied (auth deferred to subtask 1.4)\n- Tags applied for tenant management and governance: r1, llm-route, {provider}\n- Validation results:\n  - kong check passed; deck sync idempotent\n  - scripts/kong/deck-validate.ps1 enforces naming/path/tag conventions and provider allowlist (openai, anthropopic, google, groq)\n  - Conftest policy checks passed\n  - curl smoke tests to all 12 routes forward correctly with no failover behavior\n- Documentation updated in infra/kong/README.md with provider details, usage examples, and validation instructions\n</info added on 2025-10-23T22:14:59.674Z>",
            "status": "done",
            "testStrategy": "Static validation and live route smoke tests across all defined providers and models."
          },
          {
            "id": 4,
            "title": "Implement auth header injection using MVP secrets per provider",
            "description": "Inject provider credentials via request transformation using environment or secret references for MVP.",
            "dependencies": [
              3
            ],
            "details": "Deliverables:\n- Request-transformer based header injection per provider using ${ENV} substitution\n- Secret storage via Docker secrets or env, documented mapping\nConfig snippet (per route plugin examples):\n- plugins:\n  - name: request-transformer\n    config:\n      add:\n        headers:\n        - Authorization: Bearer ${OPENAI_API_KEY}\n- for Anthropic service:\n  - name: request-transformer\n    config:\n      add:\n        headers:\n        - x-api-key: ${ANTHROPIC_API_KEY}\n- for Google:\n  - name: request-transformer\n    config:\n      add:\n        headers:\n        - x-goog-api-key: ${GOOGLE_API_KEY}\n- for Groq:\n  - name: request-transformer\n    config:\n      add:\n        headers:\n        - Authorization: Bearer ${GROQ_API_KEY}\nTest cases:\n- Without plugin returns 401 from provider; with plugin returns 200\n- Secrets not present in logs; headers redacted in access logs\n- Rotating env secret seamlessly updates requests after reload\nRollback plan:\n- Disable plugin at route level; deck revert to previous state\n- Remove env secrets from process and rotate compromised keys\n<info added on 2025-10-23T23:01:42.047Z>\nImplementation complete: Auth header injection MVP\n\nChanges:\n- infra/kong/kong.yaml: extended all 12 LLM routes with request-transformer auth headers per provider\n  - OpenAI (4 routes): Authorization: Bearer ${OPENAI_API_KEY}\n  - Anthropic (2 routes): x-api-key: ${ANTHROPIC_API_KEY} and anthropic-version: 2023-06-01\n  - Google (3 routes): x-goog-api-key: ${GOOGLE_API_KEY}\n  - Groq (3 routes): Authorization: Bearer ${GROQ_API_KEY}\n- gateway/docker-compose.yml: added OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY, GROQ_API_KEY to Kong service using ${VAR:-} defaults\n- gateway/API_KEYS.md: documentation for obtaining/configuring keys, .env template, security notes (R1 vs R2+), troubleshooting\n- infra/kong/README.md: Authentication section describing per-provider header injection, security considerations, troubleshooting; marked subtask 1.4 complete\n- scripts/kong/deck-validate.ps1: enhanced to verify all routes have correct provider-specific auth headers; prevents configuration drift\n- scripts/kong/test-auth.ps1: new end-to-end auth testing (Kong availability, env var checks, route existence, optional live provider calls, log scanning for secret leakage); usage: ./scripts/kong/test-auth.ps1 [-SkipProviderTests]\n\nTesting results:\n- No linting errors; YAML valid for kong check\n- All auth header patterns validated by the enhanced validation script\n- Test script created for continuous validation\n\nR1 alignment:\n- BYO credentials via environment per tenant\n- Allowlist limited to OpenAI, Anthropic, Google, Groq; no failover\n- Light security posture for R1; forward-compatible with Vault/KMS and secretRef in R2+\n\nNext steps:\n- Run scripts/kong/deck-validate.ps1\n- Test with real keys via scripts/kong/test-auth.ps1\n- Verify logs contain no secrets after testing\n- Proceed to subtask 1.5 (semantic cache with Redis)\n</info added on 2025-10-23T23:01:42.047Z>",
            "status": "done",
            "testStrategy": "Route-level functional tests against provider sandboxes validating 2xx with proper auth and 401 without."
          },
          {
            "id": 5,
            "title": "Enable semantic cache with Redis and prompt normalization compression",
            "description": "Configure Kong AI semantic cache against Redis with TTL and normalized cache keys, plus optional compression.",
            "dependencies": [
              3,
              1
            ],
            "details": "Deliverables:\n- ai-semantic-cache plugin config with Redis backend and 24h TTL\n- Pre-function to normalize prompt and compute stable hash; optional compression of prompt text\nConfig snippet:\n- plugins:\n  - name: ai-semantic-cache\n    config:\n      redis_host: redis\n      ttl: 86400\n      namespace: llm\n      key_format: tenant:model:sha256\n- pre-function (Lua pseudo):\n  local t = kong.request.get_raw_body() or ''\n  local norm = t:gsub('%s+', ' ')\n  local key = sha256(norm .. ':' .. (kong.request.get_header('X-Tenant-Id') or 'anon') .. ':' .. model)\n  kong.ctx.shared.semantic_key = key\nTest cases:\n- First identical request is MISS; subsequent request HIT within TTL\n- Slightly different prompt results in MISS as expected\n- Redis keys have prefix llm and expire near TTL\nRollback plan:\n- Disable ai-semantic-cache plugin and flush llm:* keys if needed\n- Fall back to direct provider calls; monitor latency impact\n<info added on 2025-10-23T23:35:58.380Z>\nImplementation complete via Path B (proxy-cache + custom Lua) using Redis backend.\n\n- Added provider-aware cache key generator (gateway/plugins/llm-cache-key.lua) normalizing request shapes for OpenAI, Anthropic, Google, and Groq; includes parameter sensitivity (temperature, top_p, max_tokens, etc.).\n- Added post-function to emit observability headers (gateway/plugins/llm-cache-headers.lua) with X-Cache-Status: HIT/MISS.\n- Enabled proxy-cache on all 12 LLM routes in infra/kong/kong.yaml with Redis backend, 24h TTL, and tenant isolation via X-Tenant-Id.\n- Cache key format: llm:{tenantId}:{provider}:{model}:{hash}, where hash is SHA256 of the normalized request payload.\n- Updated gateway/Dockerfile to package pre-function/post-function modules and enable proxy-cache in KONG_PLUGINS.\n- Tests: scripts/kong/test-cache.ps1 validates MISS→HIT behavior, tenant and provider isolation, parameter sensitivity, and TTL; deck validation extended in scripts/kong/deck-validate.ps1 to enforce cache config on all routes; conftest policies pass.\n- Documentation: infra/kong/README.md updated with usage, troubleshooting, and performance expectations; infra/kong/IMPLEMENTATION_NOTES.md added with decisions, limitations, and R2/R3 roadmap.\n- Deferred to R2: prompt compression (pending Enterprise plugin/license), per-tenant cache isolation policies, and cache invalidation APIs.\n- Validation results: all 12 routes configured with cache; deck-validate.ps1 and conftest pass.\n- Performance targets: cache HIT latency <50ms, token savings 40–60%, expected hit rate >40%.\n- Rollback: disable proxy-cache and pre/post-function plugins on affected routes and flush Redis keys matching llm:*; revert to direct provider calls and monitor latency.\n\nReady for deployment and live-traffic testing.\n</info added on 2025-10-23T23:35:58.380Z>",
            "status": "done",
            "testStrategy": "A/B latency and hit rate checks using repeated calls; Redis inspection for key creation and expiration."
          },
          {
            "id": 6,
            "title": "Configure per-tenant rate limiting with Redis policy",
            "description": "Apply Redis-backed rate limiting per tenant using X-Tenant-Id header to isolate quotas.",
            "dependencies": [
              3,
              1
            ],
            "details": "Deliverables:\n- rate-limiting plugin enabled per LLM route; policy redis; limit_by header X-Tenant-Id\n- Limits parameterized via env for environments\nConfig snippet:\n- plugins:\n  - name: rate-limiting\n    config:\n      policy: redis\n      redis_host: redis\n      limit_by: header\n      header_name: X-Tenant-Id\n      minute: 600\n      fault_tolerant: true\nTest cases:\n- Burst over limit returns 429 with proper headers\n- Separate tenants do not affect each other\n- Redis reflects counters per tenant key\nRollback plan:\n- Increase limits or disable plugin temporarily\n- Restore previous limits via deck revert",
            "status": "pending",
            "testStrategy": "Load test per tenant to confirm isolation and proper 429 behavior; verify Redis counters and response headers."
          },
          {
            "id": 7,
            "title": "Wire guardrails in detect-only mode and log alerts",
            "description": "Install and configure LLM guardrails plugin in detect-only mode; log alerts and emit metrics without blocking traffic.",
            "dependencies": [
              3,
              1
            ],
            "details": "Deliverables:\n- Guardrails plugin (e.g., opa-llm-guard) configured with mode detect\n- Logging of alerts to Kong logs and metrics counters\nConfig snippet:\n- plugins:\n  - name: opa-llm-guard\n    config:\n      mode: detect\n      log_alerts: true\n      block: false\nTest cases:\n- Prompt containing disallowed content generates alert log but request still succeeds\n- Metrics counter increases for alerts\n- No increase in request latency beyond threshold target\nRollback plan:\n- Set mode off or disable plugin on routes\n- Revert to previous config via deck",
            "status": "pending",
            "testStrategy": "Functional prompts that trigger guardrails and verify pass-through behavior with logged alerts and metrics."
          },
          {
            "id": 8,
            "title": "Implement quota event emission via serverless function publishing to NATS",
            "description": "Add pre or post function plugin to emit quota.updated events with tokens and cost per response to NATS.",
            "dependencies": [
              3,
              1
            ],
            "details": "Deliverables:\n- Lua serverless post-function parsing provider usage fields and publishing to NATS\n- Subject quota.updated with envelope tenantId, tokensIn, tokensOut, cost, ts, requestId\nConfig snippet (pseudo):\n- plugins:\n  - name: serverless-functions\n    config:\n      access: []\n      functions:\n      - post\n- function post:\n  local h = kong.request.get_headers()\n  local body = kong.response.get_raw_body() or '{}'\n  local json = cjson.decode(body)\n  local in_t = json.usage and (json.usage.prompt_tokens or json.usage.input_tokens) or 0\n  local out_t = json.usage and (json.usage.completion_tokens or json.usage.output_tokens) or 0\n  local cost = pricing.estimate(provider, model, in_t, out_t)\n  nats.publish('quota.updated', {tenantId=h['X-Tenant-Id'], tokensIn=in_t, tokensOut=out_t, cost=cost, ts=os.time(), requestId=h['X-Request-Id']})\nTest cases:\n- Event observed by a test NATS subscriber with correct fields\n- Provider without usage fields emits tokens 0 and still publishes\n- NATS outage does not block response; errors logged\nRollback plan:\n- Disable serverless plugin; keep route pass-through\n- Revert to previous config; drain NATS consumer safely",
            "status": "pending",
            "testStrategy": "Contract tests by subscribing to NATS and invoking each route to validate event content and delivery behavior."
          },
          {
            "id": 9,
            "title": "Observability: Prometheus metrics, tracing headers, and log redaction",
            "description": "Expose metrics, propagate tracing headers including X-Trace-Id and OTel, and redact sensitive data in logs.",
            "dependencies": [
              1,
              3
            ],
            "details": "Deliverables:\n- Global prometheus plugin enabled; scrape config in Prometheus\n- OpenTelemetry plugin with traceparent propagation; correlation-id generation if missing\n- Log redaction via nginx injected config to exclude sensitive headers\nConfig snippet:\n- plugins:\n  - name: prometheus\n  - name: opentelemetry\n    config:\n      endpoint: http://otel-collector:4318\n      propagate: true\n      headers_baggage: [X-Trace-Id]\n- nginx injected conf:\n  log_format kong '$remote_addr - $request $status $request_time trace=$request_id';\n  map $http_authorization '' $auth_masked;\n  access_log logs/access.log kong;\nTest cases:\n- GET /metrics returns counters for requests, latency, cache, rate limit\n- Requests with incoming traceparent propagate to upstream; new id generated if absent\n- Logs contain no Authorization or api-key header values\nRollback plan:\n- Disable opentelemetry or prometheus plugins separately\n- Remove nginx injected redaction and revert to default log format",
            "status": "pending",
            "testStrategy": "Metrics scrape validation, trace propagation e2e with a test upstream, and log inspection to verify redaction."
          },
          {
            "id": 10,
            "title": "Health and readiness: /health route, upstream checks, and node gating",
            "description": "Provide a fast /health endpoint, enable upstream passive checks, and wire readiness probes for Kong nodes.",
            "dependencies": [
              1,
              3
            ],
            "details": "Deliverables:\n- Local /health endpoint that does not hit providers\n- Upstream healthchecks in observe-only mode; node readiness and liveness probes\nConfig snippet:\n- service: health-local\n  url: http://127.0.0.1:65535\n  routes:\n  - name: health\n    paths: [/health]\n    plugins:\n    - name: request-termination\n      config: status_code: 200  message: ok\n- upstream healthchecks example:\n  healthchecks:\n    passive:\n      unhealthy:\n        http_failures: 5\nTest cases:\n- /health returns 200 rapidly under load\n- When a provider is down, health metrics reflect state but routes do not failover\n- Readiness probe fails if Kong DB is unreachable\nRollback plan:\n- Remove passive checks if causing noise; keep /health static\n- Adjust probe thresholds or disable readiness gating temporarily",
            "status": "pending",
            "testStrategy": "Probes via kubelet or compose healthchecks; provider outage simulation to observe health signals without failover."
          },
          {
            "id": 11,
            "title": "Provider integration tests and continuous synthetic checks",
            "description": "Run integration tests across providers and models, plus schedule synthetic checks for cache, limits, guardrails, and events.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10
            ],
            "details": "Deliverables:\n- Test suite (k6 or pytest plus curl scripts) covering routing, auth, cache, rate limits, guardrails, quota events\n- CI job to run smoke tests on deploy; cron synthetic monitor\nTest cases (examples):\n- Routing: curl /v1/llm/openai/gpt-4o returns 200 and correct upstream headers\n- Auth: removing plugin yields 401; with plugin 200\n- Cache: two identical prompts yield MISS then HIT\n- Rate limit: exceed threshold returns 429\n- Guardrails: toxic prompt logs alert but returns 200\n- Quota: NATS receives quota.updated with correct fields\n- Observability: /metrics includes new counters, tracing IDs propagated\nRollback plan:\n- Disable failing synthetic checks while mitigating; keep alert-only\n- Gate releases by test results and roll back deployment if regression detected",
            "status": "pending",
            "testStrategy": "Automated CI tests post-deploy and periodic synthetic checks with assertions on status codes, headers, metrics, logs, and NATS events."
          },
          {
            "id": 12,
            "title": "Documentation: onboarding, config examples, runbooks, and ADRs",
            "description": "Produce docs for onboarding, provider setup, per-tenant route snippets, operations runbooks, and architectural decisions.",
            "dependencies": [
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10,
              11
            ],
            "details": "Deliverables:\n- README gateway overview; Onboarding per provider with key management; Tenant route and credential snippets\n- CONFIGURATION and RUNBOOK including rollback and disable procedures\n- ADRs for allowlist model, caching approach, and quota events\nConfig snippets (examples):\n- Tenant route example:\n  routes:\n  - name: llm.openai.gpt-4o\n    paths: [/v1/llm/openai/gpt-4o]\n    plugins: [rate-limiting, ai-semantic-cache, request-transformer, opa-llm-guard]\n- Quota event envelope example:\n  subject: quota.updated\n  data: {tenantId, tokensIn, tokensOut, cost, ts, requestId}\nTest cases:\n- Dry run a new-tenant onboarding strictly following the guide\n- Validate kong.yaml examples with kong check\n- Peer review for clarity and completeness\nRollback plan:\n- Versioned docs with changelog; revert to prior version if inaccuracies found\n- Keep deprecation notices for changed procedures",
            "status": "pending",
            "testStrategy": "Documentation validation by reproducing setup on a clean environment and peer reviews with checklists."
          }
        ]
      },
      {
        "id": 2,
        "title": "LLMOps Platform stack (Agenta, MLFlow, Langfuse, MinIO)",
        "description": "Deploy Agenta (prompt playground), MLFlow tracking, Langfuse observability, and MinIO for artifacts with multi-tenant separation and documentation.",
        "details": "- Deploy containers: Agenta (port 8081), MLFlow server, Langfuse app/api, MinIO (9000/9001). Use Postgres for metadata (Agenta/Langfuse) and MinIO for artifacts.\n- Configure Agenta workspaces per tenant; set provider connections via Kong endpoints.\n- MLFlow: backend store to Postgres, artifact store to MinIO bucket `mlflow-artifacts`, S3 creds from env.\n- Langfuse: connect SDKs in services through Kong; enable OTel bridge; set project per tenant.\n- Prompt template library: seed few example prompts with version tags.\n- Docs: LLMOps workflow, evaluation best practices, how to route to production via Kong.\nPseudo-code (docker-compose excerpt):\nservices:\n  agenta:\n    image: agenta/ce:latest\n    ports: [\"8081:8081\"]\n    env_file: .env\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:latest\n    command: mlflow server --backend-store-uri postgresql+psycopg2://... --default-artifact-root s3://mlflow-artifacts/\n  langfuse:\n    image: langfuse/langfuse:latest\n    env_file: .env\n  minio:\n    image: minio/minio:latest\n    command: server /data --console-address \":9001\"",
        "testStrategy": "- Health checks on all containers; create sample experiment in MLFlow and verify artifact appears in MinIO.\n- Create Agenta workspace and run A/B prompt; verify via Langfuse traces.\n- Multi-tenant separation: ensure distinct buckets/prefixes and DB schemas per tenant where applicable.\n- Documentation validation by following onboarding steps end-to-end.",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Compose deployment of Agenta, MLFlow, Langfuse, MinIO with health checks",
            "description": "Create a docker-compose stack wiring services, networks, volumes, env, ports, and basic health checks.",
            "dependencies": [],
            "details": "- Author docker-compose.yml with services: agenta (8081), mlflow (5000), langfuse (3000), minio (9000/9001), and postgres (5432) or point to external Postgres via .env.\n- Set env_file: .env for shared secrets; mount named volumes for MinIO data and Postgres data; define a shared network.\n- Commands/env:\n  - mlflow: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri ${MLFLOW_BACKEND_STORE_URI} --default-artifact-root s3://mlflow-artifacts\n  - langfuse: DATABASE_URL, NEXTAUTH_SECRET, APP_BASE_URL\n  - agenta: AGENTA_DATABASE_URL (Postgres), provider base URLs via Kong.\n  - minio: MINIO_ROOT_USER, MINIO_ROOT_PASSWORD; command: server /data --console-address \":9001\"\n- Map ports: 8081:8081 (agenta), 5000:5000 (mlflow), 3000:3000 (langfuse), 9000:9000 and 9001:9001 (minio), 5432:5432 (postgres if local).\n- Add container healthchecks:\n  - agenta: curl -f http://localhost:8081/ || exit 1\n  - mlflow: curl -f http://localhost:5000/ || exit 1\n  - langfuse: curl -f http://localhost:3000/api/health || exit 1\n  - minio: curl -f http://localhost:9000/minio/health/ready || exit 1\n  - postgres: pg_isready -U ${POSTGRES_USER}\n- Ensure service dependencies: mlflow waits for minio and postgres; agenta/langfuse wait for postgres.\n- Keep secrets only in .env; do not commit.",
            "status": "pending",
            "testStrategy": "- docker compose up -d; verify docker compose ps shows all services healthy.\n- Curl each mapped port locally to confirm responses.\n- Verify MinIO console at http://localhost:9001 and MLFlow UI at http://localhost:5000 load successfully."
          },
          {
            "id": 2,
            "title": "Provision PostgreSQL databases and per-tenant schemas; wire connections",
            "description": "Create service databases/schemas, roles, and configure connection URIs for Agenta, MLFlow, and Langfuse.",
            "dependencies": [
              1
            ],
            "details": "- Create roles and databases (or schemas) using psql or migration tooling:\n  - One database (platform) with schemas: agenta, langfuse, mlflow; or separate DBs if preferred.\n  - Create per-tenant schemas (e.g., agenta_tenant_a, langfuse_tenant_a) where the app supports schema separation; for MLFlow use shared schema with tenant tags on experiments.\n- Grant least-privilege roles per service (agenta_rw, langfuse_rw, mlflow_rw) scoped to their schemas.\n- Set env URIs in .env:\n  - AGENTA_DATABASE_URL=postgresql+psycopg2://agenta_rw:***@postgres:5432/platform?options=-c%20search_path=agenta\n  - LANGFUSE_DATABASE_URL=postgresql+psycopg2://langfuse_rw:***@postgres:5432/platform?options=-c%20search_path=langfuse\n  - MLFLOW_BACKEND_STORE_URI=postgresql+psycopg2://mlflow_rw:***@postgres:5432/platform?options=-c%20search_path=mlflow\n- Run required migrations:\n  - Langfuse: start app once to run Prisma migrations or run npm prisma migrate if applicable.\n  - Agenta: run DB init/migrations per docs.\n  - MLFlow: first server start will create tables.\n- Document naming convention and mapping from tenantId to schema and project keys.",
            "status": "pending",
            "testStrategy": "- psql to list schemas and permissions; confirm service roles cannot access other schemas.\n- Start services and confirm successful DB connections in logs.\n- For MLFlow, create a test experiment; verify tables appear in mlflow schema.\n- For multi-tenant, create a second tenant schema and validate isolation by querying search_path-scoped objects."
          },
          {
            "id": 3,
            "title": "Configure MinIO buckets and credentials for MLFlow artifact store",
            "description": "Set up MinIO with bucket(s), access keys, and MLFlow S3 configuration for artifact storage.",
            "dependencies": [
              1
            ],
            "details": "- Login to MinIO console and create bucket mlflow-artifacts.\n- Optionally create per-tenant prefixes (tenant-a/, tenant-b/) or buckets if strict isolation is required; apply bucket policy read/write only to required prefixes.\n- Create an access key pair with write access to mlflow-artifacts and store in .env as AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.\n- Set MLFLOW_S3_ENDPOINT_URL=http://minio:9000 and AWS_S3_SIGNATURE_VERSION=s3v4 for MLFlow container environment.\n- If TLS/externally exposed MinIO is used, configure CA trust and use https endpoint.\n- Optionally enable object lock/versioning and SSE-KMS if required by policy.",
            "status": "pending",
            "testStrategy": "- Using MinIO client (mc): mc alias set local http://localhost:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD; mc mb local/mlflow-artifacts; mc ls local/mlflow-artifacts.\n- From MLFlow UI or CLI, log a small artifact (e.g., text file) to an experiment and verify it appears in the bucket/prefix.\n- Attempt cross-tenant access with wrong credentials to confirm access is denied."
          },
          {
            "id": 4,
            "title": "Configure Agenta tenant workspaces and provider routing through Kong",
            "description": "Create per-tenant workspaces in Agenta and route LLM provider calls via Kong endpoints with tenant isolation.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "- For each tenant, create an Agenta workspace (via UI or API) and map it to the tenant schema/prefix policy where supported.\n- Configure provider connections in Agenta to use Kong as the base URL (e.g., https://kong.internal/llm/openai or /llm/azure) so all LLM traffic is centrally routed and governed.\n- Store provider API keys in Kong (key-auth/vault or plugin) and keep Agenta provider config keyless or using Kong-issued credentials per tenant.\n- Define Kong routes/services for each provider with per-tenant ACLs, rate limits, and observability headers (tenant-id) injected.\n- Provide a seed script to bootstrap workspaces and provider mappings from a tenants.yaml inventory.\n- Ensure Agenta outbound networking can resolve Kong DNS and reach it over TLS if applicable.",
            "status": "pending",
            "testStrategy": "- Create two workspaces (Tenant A, Tenant B) and configure different Kong routes or credentials for each.\n- Run a simple A/B prompt in each workspace; verify via Kong logs/metrics that requests include the correct tenant headers and are routed to the right provider.\n- Validate that workspaces cannot access each other's provider configs or artifacts."
          },
          {
            "id": 5,
            "title": "Set up Langfuse with OTel bridge and per-tenant projects via Kong",
            "description": "Create per-tenant projects/keys in Langfuse, enable OTel bridge, and route SDK/OTLP traffic through Kong.",
            "dependencies": [
              1,
              2
            ],
            "details": "- In Langfuse, create a project per tenant; record public/private keys and store securely (e.g., in .env or secret manager) and map to tenant inventory.\n- Enable OTel bridge ingestion in Langfuse via environment flags (e.g., LANGFUSE_ENABLE_OTEL_BRIDGE=true) or deploy an OTel Collector configured to export to Langfuse; expose OTLP http on 4318 through Kong.\n- Configure Kong routes for Langfuse SDK ingestion and OTLP endpoints; inject tenant-id headers and enforce auth where applicable.\n- Update example services and Agenta integration to send traces/events to Langfuse via Kong using the per-tenant keys.\n- Ensure Langfuse uses the Postgres connection from step 2 and migrations are applied.",
            "status": "pending",
            "testStrategy": "- From a sample service, send a test Langfuse event and confirm it appears in the correct tenant project.\n- Send a minimal OTLP trace/span to the bridged endpoint via Kong; verify the trace shows in Langfuse and carries tenant attributes.\n- Intentionally use the wrong tenant key and confirm ingestion is rejected."
          },
          {
            "id": 6,
            "title": "Seed prompt template library with versioned examples in Agenta",
            "description": "Create example prompts with version tags and A/B variants, scoped per tenant where needed.",
            "dependencies": [
              4
            ],
            "details": "- Prepare seed data (JSON/YAML) defining prompts with names, descriptions, version tags (v1, v2), and metadata (task, language, model hints).\n- Use Agenta API/CLI or UI to import templates into each tenant workspace; ensure version tags are visible and immutable once released.\n- Include at least: summarize-news v1/v2, classify-intent v1, sql-assistant v1; attach evaluation metrics where supported.\n- Store seeds under repo path seeds/agenta and make the seeding idempotent.\n- Document the promotion flow: draft -> reviewed -> production, and how Kong routes production traffic.",
            "status": "pending",
            "testStrategy": "- List templates in Agenta and verify all seeded entries exist with correct versions.\n- Run an A/B comparison between summarize-news v1 and v2; verify outputs differ and are logged.\n- Confirm Langfuse captures traces for these runs and tags include template name/version and tenant."
          },
          {
            "id": 7,
            "title": "Author documentation and end-to-end demo runbook for multi-tenant LLMOps",
            "description": "Write platform docs and a runnable E2E demo covering health checks, workflows, and tenant isolation.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "- Create docs: LLMOps workflow overview, evaluation best practices, multi-tenant separation (DB schemas, MinIO prefixes/buckets, Kong routes), and how to promote to production via Kong.\n- Add service-specific pages: configuring Agenta workspaces/providers, MLFlow backend/artifact setup, Langfuse projects/OTel, and environment variables.\n- Include a health-check matrix with commands/URLs per service and expected responses.\n- End-to-end runbook: \n  1) Bring up stack, verify health.\n  2) Create Tenant A workspace and Langfuse project, seed prompts.\n  3) Run MLFlow example experiment; confirm artifact in MinIO.\n  4) Run Agenta A/B; confirm traces in Langfuse via Kong.\n  5) Validate that Tenant B cannot access Tenant A resources.\n- Add troubleshooting section and rollback steps.",
            "status": "pending",
            "testStrategy": "- Peer-review docs; have a new engineer follow the runbook on a fresh environment.\n- Capture timing and pain points; update docs for clarity.\n- Acceptance: All health checks pass, artifacts/traces are visible in correct tenant, and isolation checks succeed."
          }
        ]
      },
      {
        "id": 3,
        "title": "Platform API (FastAPI) - Tenant & Provider domains",
        "description": "Implement core Platform API with multi-tenancy, tenant provisioning, provider config BYO storage (encrypted), and baseline endpoints under /v1.",
        "details": "- Stack: FastAPI, SQLAlchemy, Alembic, PostgreSQL; hexagonal architecture (domain, ports/adapters); Redis for sessions; OTel, Prometheus.\n- Models: Tenant, User, ProviderConfig (encrypted secret fields), Quota, UsageAggregate, AuditLog.\n- Endpoints:\n  - POST /v1/tenants: create tenant record, provision dedicated database (CREATE DATABASE or schema-by-DB), run per-tenant migrations.\n  - GET/PATCH /v1/tenants/{id}, list with cursor pagination.\n  - GET/PUT /v1/tenants/{id}/providers: store provider keys encrypted (libsodium/Fernet), future `secretRef` seam.\n  - GET/PUT /v1/tenants/{id}/quotas; GET usage aggregates.\n  - Flags proxy: GET/PUT /v1/tenants/{id}/flags -> Flagsmith API.\n- AuthN: Logto OIDC middleware; extract user/roles; headers accepted per convention; Idempotency-Key support using Redis.\n- ABAC: Casbin enforcer with policies on org/group/user via attributes.\n- Events: NATS consumer for `quota.updated` to mirror counters; emit audit log entries with hash-chain signature (prev_hash).\n- Observability: OTel traces; structured error envelope; health/readiness; OpenAPI spec.\nPseudo-code (FastAPI skeleton):\napp = FastAPI(root_path=\"/v1\")\n@app.post(\"/tenants\")\ndef create_tenant(cmd: CreateTenant):\n    db.create_database(per_tenant_name(cmd.slug))\n    alembic_upgrade(per_tenant_url)\n    return repo.add(Tenant(...))\n@app.put(\"/tenants/{id}/providers\")\ndef put_providers(id, body):\n    enc = kms.encrypt(body.secrets)\n    repo.save_provider(id, enc)\n",
        "testStrategy": "- Unit tests: domain services for tenant provisioning, encryption helpers, Casbin policy evaluation, idempotency.\n- Integration: spin Postgres test DB; POST tenant provisions DB and runs migrations; PUT providers stores encrypted values.\n- Contract tests: Validate error envelope, headers, pagination behavior.\n- NATS: mock/ephemeral server to consume `quota.updated` and reconcile counters.\n- Security: verify no secret leakage in logs; role-based access via ABAC.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold FastAPI Platform API with hexagonal architecture",
            "description": "Initialize repository and project structure for a hexagonal FastAPI service under /v1.",
            "dependencies": [],
            "details": "Set up Python 3.11 project with FastAPI, SQLAlchemy, Alembic, Redis, NATS, OTel, Prometheus. Create hexagonal layout: app/domain, app/application, app/ports, app/adapters (http, db, crypto, auth, cache, messaging), app/config. Add Pydantic Settings, uvicorn entrypoint, root_path=/v1, Dockerfile, docker-compose (Postgres, Redis, NATS, Prometheus), Makefile, pre-commit, ruff/mypy/pytest configs.",
            "status": "pending",
            "testStrategy": "Smoke test app startup via pytest and TestClient; validate /v1 root mounts; lint/type checks in CI."
          },
          {
            "id": 2,
            "title": "Design database models and Alembic base (system vs tenant)",
            "description": "Model core entities and set up base migrations for system and tenant scopes.",
            "dependencies": [
              1
            ],
            "details": "Define SQLAlchemy models: System DB: Tenant(id, slug, name, mode, db_url/schema, status, created_at, updated_at), User(id, subject, email, org_id, roles, created_at), CasbinPolicy tables if using adapter. Tenant DB/schema: ProviderConfig(id, kind, config_json, secret_blob, secret_meta, kek_ref, version, created_at, updated_at), Quota(tenant_id, daily_limit, monthly_limit, usage_daily, usage_monthly, updated_at), UsageAggregate(id, window_start, window_end, tokens_in, tokens_out, cost_minor, created_at), AuditLog(id, actor, action, resource, payload_json, prev_hash, hash, created_at). Create Alembic with dual heads: migrations_system and migrations_tenant directories, env.py supporting programmatic URL injection. Indices on slug, created_at, window ranges. Document migration strategy for system-first then tenant migrations per provisioned DB/schema.",
            "status": "pending",
            "testStrategy": "Run alembic upgrade on ephemeral Postgres for system and a mock tenant DB/schema; assert tables and indices exist."
          },
          {
            "id": 3,
            "title": "Implement tenant provisioning and /v1/tenants endpoints",
            "description": "Create and manage tenants, including DB/schema creation and per-tenant migrations.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement service to provision tenants: modes TENANCY_MODE=db (CREATE DATABASE t_{slug}) or schema (CREATE SCHEMA t_{id}). After creation, run migrations_tenant against the tenant connection URL. Endpoints: POST /v1/tenants {slug,name,mode?} -> 201 with tenant record; GET /v1/tenants/{id}; PATCH /v1/tenants/{id} (name,status); GET /v1/tenants?limit&cursor using keyset pagination. Persist tenant DB URL/schema in system DB. Emit AuditLog entry in tenant DB on create/update. Handle rollback on partial failures. Prepare hooks for idempotency and ABAC to wire in later subtasks.",
            "status": "pending",
            "testStrategy": "Integration: calling POST creates DB/schema and runs migrations; list returns cursor pagination; PATCH updates name/status and writes audit log."
          },
          {
            "id": 4,
            "title": "Provider configuration API with encryption and secretRef seam",
            "description": "Store provider keys securely with encryption; expose GET/PUT for per-tenant providers.",
            "dependencies": [
              2,
              3
            ],
            "details": "Define CryptoPort with envelope encryption: KEK via KMS or static dev key; DEK via Fernet/libsodium; store secret_blob (ciphertext + nonce + kek_ref + alg). Endpoints: GET /v1/tenants/{id}/providers -> list providers with masked fields (has_secret:true, last_updated, version). PUT /v1/tenants/{id}/providers accepts array/object: {kind:\"openai\"|\"anthropic\"|..., config:{...}, secret:{api_key:...}|{secretRef:\"vault://...\"}}; validate kinds and redact secrets in logs. Support optimistic concurrency via version or If-Match ETag. On update, write AuditLog entry with diff excluding secret. Prepare for future secretRef by persisting reference metadata. Enforce tenant scoping for DB access.",
            "status": "pending",
            "testStrategy": "Unit: crypto helpers roundtrip, bad key handling; Integration: PUT then GET returns masked fields; verify ciphertext changes when rotated; audit log entry created."
          },
          {
            "id": 5,
            "title": "OIDC (Logto) authentication middleware and Redis-backed sessions",
            "description": "Validate JWTs from Logto, extract roles/claims, and manage lightweight sessions.",
            "dependencies": [
              1
            ],
            "details": "Add AuthPort and adapter: OIDC issuer from LOGTO_ISSUER; fetch and cache JWKS; validate Authorization: Bearer tokens, aud, iss, exp. Map claims to User (subject, email, org_id, roles). Upsert User in system DB on first sight. Store short-lived session in Redis keyed by jti/sub with TTL; attach user context to request state. Accept convention headers (X-Org-Id, X-Tenant-Id) for scoping, with validation against ABAC in later step.",
            "status": "pending",
            "testStrategy": "JWT verification unit tests with mocked JWKS; integration: protected route returns 401/200 appropriately; Redis TTL and revocation by jti simulated."
          },
          {
            "id": 6,
            "title": "ABAC authorization with Casbin and policy storage",
            "description": "Integrate Casbin enforcer for attribute-based access control on org/group/user.",
            "dependencies": [
              5
            ],
            "details": "Define Casbin model (ABAC): sub = {user_id, org_id, roles, tenant_id}; obj = {route, tenant_id}; act = HTTP method. Use SQLAlchemy adapter for policy persistence (system DB). Seed baseline policies: superadmin full access; tenant_admin limited to their tenant; read-only roles. Add FastAPI dependency to check enforcer on each request, mapping path patterns to objects. Include policy management utilities and a bootstrap policy migration. Deny-by-default with clear error mapping to error envelope.",
            "status": "pending",
            "testStrategy": "Unit: policy evaluation matrix across roles and tenants; integration: requests denied/allowed as expected; policy reload on change."
          },
          {
            "id": 7,
            "title": "Idempotency-Key middleware with Redis-backed response cache",
            "description": "Provide safe retries for POST/PUT/PATCH using Idempotency-Key with Redis.",
            "dependencies": [
              1,
              5
            ],
            "details": "Implement middleware/dependency: require Idempotency-Key header for mutating endpoints; build dedupe key from method+path+SHA256(body)+principal. Use Redis SETNX with TTL (e.g., 24h) to lock; on first pass, capture status, headers (safe subset), and body (size limit) to Redis; on repeat with same key and body, return stored response; if key present with different body, return 409 idempotency_conflict. Include cleanup and streaming safeguards. Expose X-Idempotency-Key and X-Idempotent-Replay headers.",
            "status": "pending",
            "testStrategy": "Unit: key generation and conflict detection; integration: double POST /tenants returns same response; concurrent requests serialize via locks."
          },
          {
            "id": 8,
            "title": "Cursor pagination utilities and structured error envelope",
            "description": "Add consistent pagination helpers and standardized error format across APIs.",
            "dependencies": [
              1,
              3,
              4,
              6
            ],
            "details": "Implement keyset pagination helper using (created_at,id) for stable ordering; request: ?limit=1..100&cursor=base64(token); response: {data:[], next_cursor, count?}. Create error envelope: {error:{code:string, message:string, details?:object, trace_id:string}}; map common exceptions (authz, validation, not_found, conflict) with HTTP codes. Inject correlation X-Request-Id and include trace_id from OTel in responses. Update existing endpoints to use helpers and envelopes.",
            "status": "pending",
            "testStrategy": "Unit: paginate edge cases (empty, last page); error mapping tests; contract tests verify envelope shape and headers present."
          },
          {
            "id": 9,
            "title": "NATS consumer for quota.updated event mirroring",
            "description": "Consume quota.updated events and reconcile Quota/UsageAggregate per tenant.",
            "dependencies": [
              2,
              3
            ],
            "details": "Use nats-py (JetStream) to subscribe to subject platform.quota.updated with durable and queue group. Expected message: {tenant_id, window:\"day|month\", tokens_in, tokens_out, cost_minor, ts, event_id}. For each event, resolve tenant DB connection, upsert UsageAggregate by window, increment Quota usage fields, and ensure idempotency via event_id de-dup table or Redis key. Include retry/backoff, DLQ subject (platform.quota.updated.DLQ), and metrics/logs. Emit audit entries if quota thresholds crossed (optional note).",
            "status": "pending",
            "testStrategy": "Integration with Testcontainers NATS: publish sample events, assert aggregates updated correctly and idempotently; simulate failures to verify retries and DLQ."
          },
          {
            "id": 10,
            "title": "Observability: OpenTelemetry tracing, structured logging, Prometheus metrics",
            "description": "Instrument service with OTel, add Prometheus metrics, and improve logs.",
            "dependencies": [
              1
            ],
            "details": "Integrate opentelemetry-instrumentation for FastAPI, SQLAlchemy, Redis, and requests. Configure OTEL_EXPORTER_OTLP_ENDPOINT. Add structured JSON logging with trace/span IDs, user/tenant IDs when present. Expose Prometheus /metrics using prometheus_client; include counters/histograms: http_requests_total, http_request_duration_seconds, db_op_duration_seconds, nats_events_processed_total. Ensure spans propagate via W3C headers and response includes trace ID in error envelope.",
            "status": "pending",
            "testStrategy": "Run app with in-memory exporter; assert spans created on requests and DB calls; scrape /metrics locally and verify series update on requests and NATS events."
          },
          {
            "id": 11,
            "title": "OpenAPI specification, examples, and health/readiness endpoints",
            "description": "Publish accurate API contracts and add /healthz and /readyz endpoints.",
            "dependencies": [
              1,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10
            ],
            "details": "Define OpenAPI components/schemas for Tenant, ProviderConfig (masked), pagination envelope, error envelope, and security schemes (bearerAuth). Add tags per domain. Provide examples for POST /tenants, GET list with cursor, PUT providers. Enable CORS per config. Implement /healthz (process OK) and /readyz (checks: system DB connection, Redis ping, NATS connectivity, KMS key availability). Generate OpenAPI JSON at /v1/openapi.json and ensure docs reflect pagination, headers (Authorization, Idempotency-Key, X-Request-Id), and versioning.",
            "status": "pending",
            "testStrategy": "Contract tests comparing live OpenAPI to expected snapshot; curl health/readiness to verify status codes change when dependencies are down."
          },
          {
            "id": 12,
            "title": "Comprehensive unit, integration, and contract tests with CI",
            "description": "Deliver full test coverage for core flows, migrations, security, and contracts.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10,
              11
            ],
            "details": "Set up pytest with Testcontainers for Postgres (system+tenant), Redis, and NATS. Unit: tenant service, crypto, OIDC, ABAC, idempotency, pagination, error mapping. Integration: POST tenant provisions DB and runs tenant migrations; PUT providers stores encrypted secrets and returns masked config; NATS quota mirroring updates aggregates idempotently; OpenTelemetry spans emitted; Prom /metrics exposes counters. Contract: verify error envelope, headers, and OpenAPI examples. Add GitHub Actions pipeline running lint, type-check, unit, integration, and artifact upload (coverage). Document how to run tests locally.",
            "status": "pending",
            "testStrategy": "CI executes test matrix; local integration tests validate end-to-end flows; coverage thresholds enforced (e.g., 85%+)."
          }
        ]
      },
      {
        "id": 4,
        "title": "Platform API - Quota, Usage, Flags & Audit",
        "description": "Complete quota enforcement mirror, usage aggregation from Kong events, Flagsmith proxy, and hash-chain audit trail.",
        "details": "- Quotas: Reconcile counters from `quota.updated` into per-tenant daily/monthly totals; thresholds for soft alerts; expose GET/PUT.\n- Usage: Aggregation by time window with from/to; store token in/out, cost, cache hits; scheduled job to compact.\n- Flags: Integrate Flagsmith SDK; map platform defaults and tenant overrides; cache results; expose GET/PUT.\n- Audit: Write audit logs for critical actions (tenants, providers, quotas, flags) with hash(prev + record) chain; expose search in future; include OpenTelemetry span ids.\n- Jobs: background workers using APScheduler/Celery (if Celery already used in Integrations) for reconciliation.\n- Docs: API reference, onboarding guide, ABAC examples.",
        "testStrategy": "- Unit tests for aggregation math and threshold evaluation.\n- Integration tests replaying sample NATS events; verify counters and GET /usage correctness.\n- Flagsmith mock to verify proxy behavior and caching semantics.\n- Verify audit chain integrity; tamper-detection test by altering a past record and detecting mismatch.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement NATS consumer to reconcile quota.updated into per-tenant daily/monthly counters",
            "description": "Consume quota.updated events and maintain accurate per-tenant daily/monthly totals with idempotent reconciliation.",
            "dependencies": [],
            "details": "Build a durable NATS/JetStream consumer for subject(s) emitted by Task 1 (AI Gateway). Parse event payloads, validate schema, and upsert into a QuotaTotals table keyed by (tenant_id, window_type, window_start). Use INSERT ... ON CONFLICT DO UPDATE to increment counters atomically. Store event_id and last_processed_at for dedupe and recovery. Provide a replay/backfill CLI (range: tenant/date) to reprocess events. Partition by day for performance and add indexes (tenant_id, window_start). Blocked on Task 3 models being available; coordinate migrations with Task 21. Emit OTel spans/metrics and use batch acks with size/time flush to limit write amplification.",
            "status": "pending",
            "testStrategy": "Unit: reconciliation math, idempotency by event_id, conflict handling. Integration: publish synthetic NATS events and assert daily/monthly totals match. Chaos: out-of-order and duplicate events, restart recovery."
          },
          {
            "id": 2,
            "title": "Quota thresholds evaluation and soft alerting with hysteresis and debounced notifications",
            "description": "Evaluate per-tenant thresholds against reconciled counters and emit soft alerts without flapping.",
            "dependencies": [
              1
            ],
            "details": "Extend quota model to store limits and threshold rules (e.g., 80%, 95%). On each counter update from subtask 1, compute utilization and transitions. Implement hysteresis (e.g., enter at 80%, clear at 75%) and debounce windows. Send alerts via: OTel metrics (gauge/counter), Prometheus alert rules, and optional webhook/Slack. Persist last_alert_state per window to prevent duplicate notifications. Provide per-tenant overrides. Ensure CPU-light evaluation by doing incremental checks only on changed windows. Document alert semantics and rate limits.",
            "status": "pending",
            "testStrategy": "Property tests for threshold crossing and hysteresis; unit tests for debounce timer logic; integration test that simulates rising usage to trigger 80% then 95% alerts and verifies single notification per transition."
          },
          {
            "id": 3,
            "title": "Usage aggregation storage and time-window queries from Kong events (tokens, cost, cache hits)",
            "description": "Ingest Kong usage events and expose range queries with aggregation (hour/day) for analytics and billing.",
            "dependencies": [],
            "details": "Design a UsageRaw (append-only) and UsageAggregate (hour/day) schema with partitioning by day; indexes on (tenant_id, ts) and (tenant_id, bucket_start). Ingest from NATS subject(s) published by Task 1 with fields: tokens_in, tokens_out, cost, cache_hit, latency_ms, provider/model. Use batched COPY/UPSERT for throughput. Expose query service to return aggregates for from/to and granularity, with filters (provider/model, cache_hit). Optimize with covering indexes and avoiding full scans; enforce pagination/cursor for long ranges. Provide backfill from raw to aggregates, and bootstrap from historical events if available. Coordinate Alembic migrations via Task 21.",
            "status": "pending",
            "testStrategy": "Unit: aggregation math and grouping by window. Integration: replay mixed events across tenants and verify GET /usage results for hourly/daily match expectations, including cache hit rates. Load: ingest 100k events and assert p95 query latency < 100ms for 30-day window."
          },
          {
            "id": 4,
            "title": "Background jobs for compaction/rollups and reconciliation catch-up using APScheduler/Celery",
            "description": "Schedule periodic compaction of usage raw to aggregates and run quota catch-up/backfills safely and idempotently.",
            "dependencies": [
              3,
              1
            ],
            "details": "Introduce a job runner (prefer Celery if already present; fallback to APScheduler). Jobs: (a) hourly usage rollup (raw->hour), (b) daily rollup (hour->day), (c) quota reconciliation catch-up for late events, (d) TTL purge of old raw rows after retention window. Use advisory locks per tenant/window to avoid double work in multi-worker setups. Batch size and commit frequency tuned to keep under 1s lock time. Expose admin endpoints/CLI to trigger backfills by tenant/date. Emit OTel spans, Prometheus metrics (processed rows/sec), and structured logs. Ensure re-entrant idempotent rollups using upserted aggregates with monotonic window versions.",
            "status": "pending",
            "testStrategy": "Time-travel tests with freezegun to verify schedules; integration: run jobs against seeded data and validate aggregates and no duplicates; fault injection: kill worker mid-job, then resume and confirm idempotent completion."
          },
          {
            "id": 5,
            "title": "Flagsmith SDK proxy with platform defaults, tenant overrides, and Redis caching",
            "description": "Provide a flags proxy that merges platform defaults with per-tenant overrides and caches effective flags.",
            "dependencies": [],
            "details": "Integrate Flagsmith SDK; create service that fetches remote flags and merges with platform defaults and DB-stored tenant overrides. Cache effective flag sets in Redis (key: tenant_id, etag/version) with short TTL and explicit invalidation on PUT. Provide fallback mode when Flagsmith is down using last-known-good cache and defaults. Support bulk read and single-flag GET/PUT semantics. Include structured auditing hooks to log changes (to be consumed by subtask 6). Add rate limiting and circuit breaker around Flagsmith calls. Plan migrations for TenantFlagOverrides table via Task 21. Performance: avoid per-flag round-trips, prefer batch fetch and gzip responses.",
            "status": "pending",
            "testStrategy": "Unit: merge precedence (override > remote > default), cache hit/miss behavior, invalidation. Integration: mock Flagsmith latency/failures and verify fallback and circuit breaking. Contract tests for GET/PUT flags responses."
          },
          {
            "id": 6,
            "title": "Hash-chain audit log writer and periodic integrity verifier with OTel span correlation",
            "description": "Record critical actions in an append-only hash chain and periodically verify integrity to detect tampering.",
            "dependencies": [],
            "details": "Implement an audit module that writes records with fields: tenant_id, actor, action, payload, ts, span_id/trace_id, prev_hash, hash. Use SHA-256 over canonical JSON (JCS) of {prev_hash, record} to form the chain. Start a per-tenant genesis record on first write. Add a verifier job that scans by tenant in order, recomputes hashes, and stores checkpoints (e.g., every 10k records) to accelerate future runs. Optionally export checkpoint digests to an external anchor. Optimize with append-only inserts and indexes on (tenant_id, ts, id). Provide CLI to rechain after historical import. Coordinate schema via Task 21.",
            "status": "pending",
            "testStrategy": "Unit: deterministic hash reproduction and chain linkage. Tamper test: flip a past row and assert verifier flags the break and downstream segment. Concurrency test: parallel writes maintain a single linear chain via transactional locking."
          },
          {
            "id": 7,
            "title": "API endpoints: GET/PUT quotas, GET usage (from/to), GET/PUT flags with ABAC and observability",
            "description": "Expose FastAPI endpoints under /v1 with authorization, validation, and OTel for quotas, usage, and flags.",
            "dependencies": [
              1,
              2,
              3,
              5,
              6
            ],
            "details": "Implement endpoints: (a) GET/PUT /v1/tenants/{id}/quotas to fetch counters (daily/monthly) and update limits/thresholds; (b) GET /v1/tenants/{id}/usage?from&to&agg=hour|day&filters to return aggregated metrics; (c) GET/PUT /v1/tenants/{id}/flags for effective flag set and overrides. Enforce ABAC (Casbin) from Task 3, validate inputs, and return consistent error envelopes. Add ETags for flags, pagination for usage. All mutating endpoints emit audit logs via subtask 6. Document via OpenAPI and include examples for ABAC. Optimize queries with prepared statements and response compression. Integrate OTel spans and attributes for tenant/action for traceability.",
            "status": "pending",
            "testStrategy": "API integration tests covering auth (allowed/denied), schema validation, and response correctness. Contract tests for pagination and ETag behavior. Smoke tests under load for p95 latency targets."
          },
          {
            "id": 8,
            "title": "End-to-end test suite: event replay, API contracts, alerts, and audit tamper detection",
            "description": "Create E2E tests that replay events, exercise APIs, validate alerts, and verify audit chain integrity at scale.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Build a docker-compose CI harness (Postgres, Redis, NATS, worker) to run E2E tests. Replay recorded NATS streams for quota and usage, then assert counters, thresholds, and GET /usage results. Exercise flags endpoints with override updates and cache behavior. Simulate threshold crossings and assert webhook/metric alerts. Mutate an audit row to ensure verifier detects tampering. Include backfill scenarios (run backfill CLI, then re-verify). Capture performance metrics (ingest throughput, query p95) and set regression thresholds. Provide seed data for multiple tenants and providers.",
            "status": "pending",
            "testStrategy": "CI pipeline that provisions the stack, runs seed/replay scripts, executes API and alert assertions, injects tamper scenarios, and publishes performance reports. Include flaky-test guards and retries for event timing."
          }
        ]
      },
      {
        "id": 5,
        "title": "Agent Orchestration Service (FastAPI + Temporal)",
        "description": "Implement durable agent workflow execution with Temporal, LangGraph for agent graphs, tool/MCP routing, and NATS lifecycle events.",
        "details": "- Components: FastAPI app (/v1), Temporal client + worker, PostgreSQL for execution state, Redis for short-lived state, NATS producer.\n- Endpoints: POST /v1/agents/execute, GET by id, list with cursor.\n- Workflows: templates for plan-execute-review; activities for LLM calls via Kong; retries/backoff policies.\n- Tools: registry to map tool names to MCP clients; configurable per request.\n- Events: Publish agent.requested/started/completed/failed with envelope {executionId, tenantId,...}.\n- Observability: OTel traces; Prometheus metrics.\nPseudo-code:\nwith workflow.start(workflow_id) as wf:\n  emit('agent.started')\n  plan = activities.llm_call('planner', input)\n  result = activities.execute_tools(plan)\n  review = activities.llm_call('reviewer', result)\n  emit('agent.completed', metrics=...)\n",
        "testStrategy": "- Unit tests for workflow steps and retries (Temporal test harness).\n- Integration test: POST execute triggers worker and emits NATS events.\n- Contract: validate response schema and pagination.\n- Load test basic concurrency; verify state persists across restarts.",
        "priority": "high",
        "dependencies": [
          1,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold FastAPI service, configuration, Temporal client, and infra clients",
            "description": "Create base service with /v1 FastAPI app, config, and connections to Temporal, Postgres, Redis, and NATS.",
            "dependencies": [],
            "details": "Set up project structure, Pydantic Settings, and dependency injection. Initialize Temporal client (temporalio), async NATS producer, async PG pool, and Redis client. Add health/readiness endpoints, graceful shutdown, correlation ID and idempotency-key middleware (X-Idempotency-Key), and OpenAPI schema.",
            "status": "pending",
            "testStrategy": "Start app with mocked backends; assert health endpoints, successful connections, and middleware injects correlation/idempotency headers."
          },
          {
            "id": 2,
            "title": "Define Temporal workflows for plan-execute-review using LangGraph with deterministic control",
            "description": "Implement a deterministic workflow that orchestrates planning, tool execution, and review graph.",
            "dependencies": [
              1
            ],
            "details": "Create a Temporal Workflow that coordinates plan->execute->review. Integrate LangGraph for the agent graph, but keep all side effects in Activities. Add signals (cancel, pause, resume) and queries (status, progress). Use workflowId=executionId for idempotent starts. Configure timeouts, retry policies, and failure handling; persist checkpoints via workflow state.",
            "status": "pending",
            "testStrategy": "Use Temporal test harness to run the workflow deterministically. Test signal/query behavior and idempotent restart using the same workflowId."
          },
          {
            "id": 3,
            "title": "Implement Activities for Kong-based LLM calls with retries, backoff, and timeouts",
            "description": "Build planner and reviewer Activities that call LLMs via Kong using robust retry and backoff.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement activities for planner/reviewer that call Kong with mTLS/auth, request validation, and structured errors. Set schedule/start-to-close timeouts, exponential backoff retries with jitter, non-retryable codes, and circuit-breaker. Propagate tracing headers, include idempotency tokens, and support streaming where needed. Serialize outputs for workflow determinism.",
            "status": "pending",
            "testStrategy": "Activity unit tests simulating HTTP failures to assert retry/backoff and non-retryable behavior. Verify traces/spans and redaction of sensitive logs."
          },
          {
            "id": 4,
            "title": "MCP tool registry and routing with per-request configuration and policy controls",
            "description": "Create registry mapping tool names to MCP clients and route tool invocations safely.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement registry loading from config/DB to resolve tool->MCP server. Enforce per-tenant allowlists/denylists, timeouts, and concurrency limits. Route invocations via an Activity that manages MCP clients, sanitizes IO, and isolates failures. Cache clients, validate tool contracts, and include versioning. All external IO runs in Activities to maintain workflow determinism.",
            "status": "pending",
            "testStrategy": "Unit tests for resolution, policy enforcement, and error paths. Integration test with a fake MCP server to verify routing, timeouts, and retries."
          },
          {
            "id": 5,
            "title": "Execution state persistence in PostgreSQL with Redis for short-lived/idempotency",
            "description": "Design schemas and DAL for executions, steps, and pagination; use Redis for ephemeral markers.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create Postgres tables (executions, steps, events) with indexes on tenantId, created_at. Implement DAL with upserts for idempotent POSTs and transactional consistency. Store idempotency keys and in-progress markers in Redis with TTL. Provide cursor-based pagination (created_at,id). Handle retries with backoff on transient DB/Redis errors.",
            "status": "pending",
            "testStrategy": "Run migrations; DAL unit tests for upsert/idempotency and pagination. Fault-injection tests for transient errors to verify retry behavior and no duplicate rows."
          },
          {
            "id": 6,
            "title": "NATS lifecycle event publisher with at-least-once delivery and deduplication",
            "description": "Publish agent.requested/started/completed/failed events with stable envelopes and acks.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "Implement an Activity that publishes to NATS JetStream with durable/subject conventions. Envelope includes executionId, tenantId, status, timestamps, and trace context. Use executionId+stage as dedup key, enable manual acks with retry/backoff on publish. Ensure events originate from Activities (not workflow code) to preserve determinism. Version event schemas.",
            "status": "pending",
            "testStrategy": "Integration tests with NATS container to verify acks, redelivery on failure, and dedup across retries. Schema validation tests for the envelope."
          },
          {
            "id": 7,
            "title": "API endpoints: POST /v1/agents/execute, GET by id, and list with cursor pagination",
            "description": "Expose endpoints to start/resume executions idempotently, fetch by id, and list executions.",
            "dependencies": [
              1,
              2,
              5,
              6
            ],
            "details": "Implement POST /v1/agents/execute to validate input, enforce idempotency (Redis/PG), start Temporal workflow using workflowId=executionId, and publish agent.requested. Implement GET /v1/agents/{id} merging DB state with workflow query results. Implement list with stable cursor (created_at,id). Add error mapping, auth hooks, and rate limits.",
            "status": "pending",
            "testStrategy": "API contract tests (OpenAPI/JSONSchema), idempotent POST (same key returns same execution), pagination edge cases, and mocked Temporal/NATS to verify side effects."
          },
          {
            "id": 8,
            "title": "Observability: OpenTelemetry tracing, Prometheus metrics, and structured logging",
            "description": "Instrument API, workflows, activities, MCP routing, and NATS for tracing and metrics.",
            "dependencies": [
              1,
              2,
              3,
              6,
              7
            ],
            "details": "Configure OTel SDK with W3C context propagation across FastAPI, Temporal Activities, Kong HTTP, and NATS. Add spans for workflow steps and tags (executionId, tenantId). Expose Prometheus metrics for request counts, activity retries, tool latencies, queue depth, and failures. Implement structured logs with correlation and redaction. Provide basic dashboards and alerts.",
            "status": "pending",
            "testStrategy": "E2E trace continuity test via a sample execution; verify metrics scraping and counters/histograms update; snapshot log fields include correlation and execution IDs."
          },
          {
            "id": 9,
            "title": "Temporal worker process, registration, containerization, and autoscaling strategy",
            "description": "Build worker registering workflows/activities; package and define K8s deployment and scaling.",
            "dependencies": [
              2,
              3,
              4,
              6,
              8
            ],
            "details": "Implement Temporal Worker with task queue, sticky execution, and max concurrent activities. Add graceful shutdown/drain hooks. Create Dockerfile and Helm/K8s manifests with readiness/liveness probes. Configure HPA using CPU and queue depth metrics. Provide separate queues for priority classes and retry on worker errors. Document rollout/canary and backoff configs.",
            "status": "pending",
            "testStrategy": "Deploy locally (docker-compose/kind). Kill workers during runs to ensure progress resumes. Load test to trigger autoscaling and verify no dropped work or duplicate effects."
          },
          {
            "id": 10,
            "title": "Comprehensive tests: unit, integration, and load with Temporal test harness",
            "description": "Cover workflows, activities, API, persistence, events, idempotency, and failure/retry paths.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Implement unit tests for workflow logic (signals/queries, determinism) and activity retry/backoff. Integration test: POST execute triggers worker, DB updates, and all NATS lifecycle events. Contract tests for request/response schemas and pagination. Load and chaos tests (network blips, worker restarts). Verify idempotency and data durability across restarts in CI.",
            "status": "pending",
            "testStrategy": "CI pipeline runs unit+integration on each commit; scheduled load/chaos suite nightly. Assert SLIs (success rate, P95 latency) and persistence across restarts with Temporal test harness."
          }
        ]
      },
      {
        "id": 6,
        "title": "Knowledge & RAG Service (FastAPI)",
        "description": "Deliver document ingestion with Unstructured.io, vector indexing with pgvector, hybrid retrieval (BM25 + dense), LlamaIndex pipeline, and MinIO storage.",
        "details": "- Components: FastAPI app (/v1), PostgreSQL + pgvector, MinIO client, LlamaIndex orchestration.\n- Ingestion: POST /v1/documents accepts metadata + file/URI; store raw in MinIO; parse with Unstructured; chunk (code/text aware).\n- Indexing: Create index per corpus; store embeddings via Kong embedding provider; BM25 via Postgres full-text or specialized lib; dense via pgvector.\n- Search: POST /v1/search with hybrid scoring and rerank; return grounded answers with citations.\n- Multi-tenant isolation: schema/db per tenant.\n- Observability, health.\nPseudo-code:\nemb = llm.embed(text)  # via Kong route\nINSERT INTO embeddings (doc_id, vec) VALUES (...)\nSELECT ... ORDER BY 0.5*bm25 + 0.5*similarity",
        "testStrategy": "- Unit tests for chunking strategies and embedding pipeline interfaces.\n- Integration: ingest sample corpus; verify search returns relevant docs and citations.\n- Performance: measure retrieval latency and P95; validate hybrid improves MRR vs single mode.\n- Persistence: ensure documents stored in MinIO; cleanup on delete.",
        "priority": "high",
        "dependencies": [
          1,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design PostgreSQL schema with pgvector and full-text search (FTS)",
            "description": "Create DB schema for corpora, documents, chunks, and embeddings with pgvector and FTS.",
            "dependencies": [],
            "details": "Install pgvector extension; define tables: corpora, documents, chunks, embeddings. Add embeddings.vec VECTOR(d) with correct dimension, chunks.tsv TSVECTOR for FTS, GIN index on tsv, IVFFLAT/HNSW index on vec, foreign keys, created_at/updated_at, soft-delete flags. Provide upsert/staging tables for bulk loads and SQL functions to compute BM25 and normalize scores. Prepare Alembic migrations.",
            "status": "pending",
            "testStrategy": "Alembic migration tests; verify extensions exist; insert sample rows; validate IVFFLAT/HNSW creation; confirm tsvector updates via trigger; check cosine distance function returns expected ordering."
          },
          {
            "id": 2,
            "title": "Implement MinIO client, buckets, and bucket policies",
            "description": "Set up MinIO SDK, buckets, lifecycle policies, and secure access for raw document storage.",
            "dependencies": [],
            "details": "Use MinIO Python SDK with env-based credentials. Create bucket per environment (e.g., rag-docs) if missing. Configure bucket policy: private with server-side encryption optional (SSE-S3/KMS). Implement prefix scheme: tenant/{tenant_id}/corpus/{corpus_id}/doc/{doc_id}/original and /derived. Add lifecycle: expire transient uploads, noncurrent versions pruning. Provide presigned PUT/GET helpers and retry/backoff.",
            "status": "pending",
            "testStrategy": "Mock MinIO; unit test bucket creation idempotency, policy application, presigned URL generation, error handling for missing buckets and permission errors."
          },
          {
            "id": 3,
            "title": "Build ingestion API (POST /v1/documents) with file/URI, Unstructured parsing, and deletion/cleanup flows",
            "description": "FastAPI endpoint to accept metadata and files/URIs, store raw in MinIO, parse with Unstructured, and support delete.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Implement POST /v1/documents: validate tenant/corpus, accept file upload or source_uri, stream to MinIO, create document record, call Unstructured.io (local or API) to parse into elements, persist raw JSON, and enqueue chunking/indexing. Implement DELETE /v1/documents/{id}: soft-delete default; cascade remove MinIO objects, chunks, embeddings, and FTS rows; reindex corpus if necessary. Add idempotency keys, size/type limits, and virus scan seam.",
            "status": "pending",
            "testStrategy": "Integration test roundtrip (upload, parse, chunk enqueue); verify MinIO objects exist; simulate delete and assert DB rows and objects removed; idempotent re-upload; malformed file handling."
          },
          {
            "id": 4,
            "title": "Implement text- and code-aware chunking strategies",
            "description": "Create configurable chunkers for prose and source code with overlap and metadata preservation.",
            "dependencies": [],
            "details": "Provide chunkers: text (semantic/paragraph + token-aware with overlap) and code (language-aware by filetype, respecting functions/classes/markdown cells). Support max_tokens, overlap, and split heuristics; preserve source offsets and headers for citations. Expose as strategy registry; serialize chunk metadata (page, section, path, language). Fallback to length-based splitting when needed.",
            "status": "pending",
            "testStrategy": "Unit tests covering boundary conditions, token counts, overlap correctness, and metadata propagation for multiple languages and long documents."
          },
          {
            "id": 5,
            "title": "Embedder client via Kong AI Gateway (batch embeddings)",
            "description": "Create embedding pipeline using Kong route to provider, with batching, retries, and observability.",
            "dependencies": [
              1
            ],
            "details": "Implement EmbeddingsClient calling Kong route (e.g., llm.embeddings.<provider>.<model>) with tenant headers. Support batch sizing, exponential backoff, and circuit-breaker. Validate embedding dimension matches pgvector. Record costs/latency metrics. Cache identical chunk hashes to avoid re-embedding. Secure with timeouts and request signing if configured.",
            "status": "pending",
            "testStrategy": "Unit tests with mocked Kong; dimension mismatch detection; batching behavior; retry on 429/5xx; caching hit/miss statistics."
          },
          {
            "id": 6,
            "title": "Per-corpus indexing pipeline integrating LlamaIndex",
            "description": "Build an indexing job that writes FTS rows and vectors per corpus and maintains index state.",
            "dependencies": [
              1,
              3,
              4,
              5
            ],
            "details": "Implement an Indexer that takes parsed elements -> chunks -> embeddings. Write chunks (tsvector via to_tsvector) and embeddings to DB in transactions; maintain corpus_index_state (version, last_build, doc counts). Integrate LlamaIndex nodes/indices for orchestration; support incremental updates on new/updated docs and rebuilds. Add backpressure and concurrency controls. Store provenance to link chunks to MinIO URIs.",
            "status": "pending",
            "testStrategy": "Integration: ingest sample corpus, run indexer, validate counts, FTS searchability, and vector presence. Fault-injection to ensure transactional integrity and resumability."
          },
          {
            "id": 7,
            "title": "Hybrid retrieval API (POST /v1/search) with BM25 + vector and reranking",
            "description": "Implement search that combines BM25 and dense vector scores, then reranks and returns candidates.",
            "dependencies": [
              1,
              6
            ],
            "details": "Implement SQL/ORM search: compute bm25(tsvector, query) and cosine_similarity(vec, query_emb). Normalize both, combine via alpha weighting, retrieve top-k per corpus. Add optional filters (metadata). Implement reranker (e.g., cross-encoder via Kong or LlamaIndex Reranker). Expose POST /v1/search with query, corpus_id, k, alpha, rerank_k. Return scored nodes with snippet spans and metadata. Guardrails for long queries.",
            "status": "pending",
            "testStrategy": "Golden queries set verifying that hybrid > BM25-only or vector-only on MRR/nDCG; unit tests for score normalization; latency budget assertions; reranker on/off comparisons."
          },
          {
            "id": 8,
            "title": "Multi-tenant isolation with per-tenant schema and storage prefixes",
            "description": "Enforce strict tenant isolation across DB schemas and MinIO prefixes with context propagation.",
            "dependencies": [
              1,
              2,
              6
            ],
            "details": "Adopt schema-per-tenant model: create schema on tenant provisioning; set search_path dynamically from auth context; restrict cross-tenant access via RLS or app-layer checks. Namescope MinIO keys with tenant and corpus prefixes. Provide migration utility to move existing data into tenant schemas. Ensure background jobs carry tenant context. Add cleanup for tenant deletion (drop schema, delete prefixes).",
            "status": "pending",
            "testStrategy": "Tenant A/B isolation tests: attempts to access other tenant’s data fail; schema creation and teardown verified; context propagation in async tasks; MinIO prefix isolation."
          },
          {
            "id": 9,
            "title": "Grounded answer formatter with citations and attributions",
            "description": "Format final responses with evidence snippets, URIs, and provenance for auditability.",
            "dependencies": [
              7
            ],
            "details": "Create a formatter that synthesizes answers from retrieved/reranked contexts (via LlamaIndex ResponseSynthesizer or custom prompt) and returns JSON with answer text, cited_chunks including doc_id, chunk_id, score, source_uri, page/path, and highlighted spans. De-duplicate citations, cap tokens, and include safety notes. Provide markdown/plain variants and enable debug traces when requested.",
            "status": "pending",
            "testStrategy": "Unit tests verifying citation fields present and correct; truncation rules; deterministic ordering by contribution; snapshot tests for formatting."
          },
          {
            "id": 10,
            "title": "Observability, metrics, tracing, and health/readiness endpoints",
            "description": "Add Prometheus metrics, OpenTelemetry tracing/logging, and health checks for dependencies.",
            "dependencies": [
              3,
              7
            ],
            "details": "Expose /healthz and /readyz checking DB, pgvector index availability, and MinIO. Instrument API latencies, p50/p95, error rates, embedding calls, index build times, and search stages (BM25, vector, rerank). Add request IDs, structured logs, trace propagation, and redaction for PII. Export Prometheus metrics and OTLP traces; dashboards for ingestion and search performance.",
            "status": "pending",
            "testStrategy": "Automated checks for health endpoints; metrics presence assertions; trace coverage via test spans; log structure and redaction tests."
          },
          {
            "id": 11,
            "title": "Comprehensive tests, evaluation metrics, SLAs, and performance/persistence validation",
            "description": "End-to-end tests for relevance, latency, persistence; compute metrics (MRR, nDCG, Recall) and document SLAs.",
            "dependencies": [
              3,
              6,
              7,
              8,
              10
            ],
            "details": "Create an evaluation harness with a labeled QA set per corpus to compute MRR@10, nDCG@10, Recall@5, and answer faithfulness. Benchmark latency p50/p95 for search and ingestion throughput; set pass thresholds and regressions gating CI. Verify persistence and crash recovery (restart during indexing). Validate deletion/cleanup flows (document and tenant) leave no orphans in DB or MinIO. Generate periodic reports and baseline comparisons (hybrid vs single-mode).",
            "status": "pending",
            "testStrategy": "CI job runs eval suite; fail PR if metrics regress beyond thresholds. Load tests with k6/Locust for latency targets. Chaos tests for restart recovery. Data integrity checks for deletion and tenant teardown."
          }
        ]
      },
      {
        "id": 7,
        "title": "Integrations Service (FastAPI) + Plugin scaffold",
        "description": "Create plugin architecture scaffold with lifecycle APIs, per-tenant enablement via Flagsmith, Celery workers, NATS events, and example plugin.",
        "details": "- API: /v1/plugins catalog; tenant endpoints for install/configure/enable/disable/uninstall.\n- Plugin contract: manifest (capabilities, config schema, version); hooks (on_install, on_enable, on_event, on_uninstall).\n- Storage: Postgres tables for plugins and per-tenant configs; secret storage seam for R2.\n- Workers: Celery with Redis broker for async jobs; NATS distribution of plugin events.\n- Webhooks: POST /v1/webhooks/{plugin} with signature verification.\n- MCP: server scaffold to expose tools from plugins.\n- Example plugin: no-op logger or simple webhook echo (disabled by default).\n",
        "testStrategy": "- Unit tests for lifecycle transitions and config validation.\n- Integration tests invoking example plugin hooks; verify Flagsmith toggles influence enablement.\n- Webhook signature validation tests.\n- Worker job execution and retry tests.",
        "priority": "medium",
        "dependencies": [
          3,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Build FastAPI v1 plugin catalog and tenant lifecycle APIs with Flagsmith gating",
            "description": "Implement /v1/plugins catalog and tenant-scoped install/configure/enable/disable/uninstall endpoints with RBAC and tenant isolation.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create FastAPI routers for /v1/plugins and /v1/tenants/{tenant_id}/plugins. Define Pydantic models for plugin summary, install request, config update, and state transitions. Integrate Flagsmith SDK to gate per-tenant enable/disable via feature flags and environment defaults. Enforce tenant scoping on all DB reads/writes, propagate tenant_id through request context. Emit lifecycle events to NATS (fire-and-forget) and enqueue Celery tasks for async hooks. Document API versioning strategy (path-based /v1, semver in responses, backward-compatible changes), authentication (JWT with tenant claim), and authorization boundaries (only tenant admins can mutate). Include idempotency keys for install/uninstall and ETags for config updates.",
            "status": "pending",
            "testStrategy": "Use pytest-asyncio and httpx AsyncClient against ASGI app. Mock Flagsmith to validate gating logic. RBAC tests for admin vs member tokens. State machine tests across install→configure→enable→disable→uninstall with invalid transition rejection. Contract tests verify OpenAPI schema and versioned response shapes."
          },
          {
            "id": 2,
            "title": "Define plugin manifest model and config schema validation",
            "description": "Create plugin contract types and strict validation for manifest and tenant config.",
            "dependencies": [],
            "details": "Design Pydantic models for PluginManifest (name, id, version, apiVersion, capabilities, hooks list, config JSON Schema, secrets requested). Implement JSON Schema validation for tenant-provided configuration with custom validators for required/enum/range constraints. Enforce semantic versioning for manifest.version and apiVersion compatibility checks. Provide loader/registry that validates manifests on service start and rejects duplicates or incompatible versions. Include capability taxonomy and hook presence checks (on_install, on_enable, on_event, on_uninstall).",
            "status": "pending",
            "testStrategy": "Property-based tests (hypothesis-jsonschema) generating configs to validate acceptance/rejection. Unit tests for semver compatibility and backward-compat changes. Negative tests for missing hooks, unknown capabilities, and invalid JSON Schema. Snapshot tests for normalized manifest output."
          },
          {
            "id": 3,
            "title": "Implement Postgres persistence and secret storage seam for R2",
            "description": "Create relational storage for plugins and tenant configs with a secure secret indirection layer.",
            "dependencies": [],
            "details": "Add Alembic migrations for tables: plugins, plugin_versions, tenant_plugins (state, installed_at, enabled_at), plugin_configs (versioned), webhook_secrets, and outbox if needed. Use SQLAlchemy 2.0 with connection pooling and explicit transactions. Implement DAO layer with strict tenant filters and soft-deletes where appropriate. Build a secret storage interface with an R2 (S3-compatible) backend for envelope-encrypted secrets; store only opaque references in Postgres. Support rotation and audit trails of secret access. Index by tenant_id, plugin_id; add unique constraints to prevent duplicate installs.",
            "status": "pending",
            "testStrategy": "Run migrations in Testcontainers Postgres; verify up/down migration integrity. Unit tests for DAO methods enforcing tenant scoping and uniqueness. Secrets seam tests mocking R2 to ensure no plaintext persistence and correct fetch/rotate flows. Performance test basic queries with sample data."
          },
          {
            "id": 4,
            "title": "Set up Celery workers with Redis broker for async plugin hooks",
            "description": "Configure Celery app, Redis broker, and tasks for running plugin lifecycle and event processing.",
            "dependencies": [
              3
            ],
            "details": "Create Celery app module with Redis broker and result backend. Implement tasks for on_install, on_enable, on_event, and on_uninstall with tenant context propagation, idempotency keys, and retry policies (exponential backoff, max retries, acks_late, visibility timeout). Add structured logging, OpenTelemetry tracing, and metrics for latency/failures. Provide graceful shutdown and concurrency settings. Optionally include Celery Beat for scheduled plugin tasks. Tasks read/write via DAO layer and fetch secrets via the R2 seam.",
            "status": "pending",
            "testStrategy": "Spin up Redis via Testcontainers, run worker in tests, and dispatch tasks from API. Force failures to validate retry/backoff and idempotency. Validate task timeouts and that acks_late prevents loss. Verify traces/metrics are emitted."
          },
          {
            "id": 5,
            "title": "Integrate NATS for plugin event distribution with versioned subjects",
            "description": "Publish and subscribe to plugin lifecycle and webhook events via NATS (JetStream optional).",
            "dependencies": [
              4
            ],
            "details": "Wire NATS client with connection pooling and NKEYS auth. Define subject conventions (v1.plugins.{plugin_id}.{eventType}) and include event schema version in headers for evolution. Enable JetStream streams for durable retention and replay where needed; use per-tenant or per-plugin consumer groups. Enforce ACLs so services can only publish/subscribe to allowed subjects. Ensure at-least-once delivery semantics with deduplication keys. Document security boundaries (tenant isolation via subject prefixing and claims-based auth).",
            "status": "pending",
            "testStrategy": "Use Testcontainers NATS; publish events from API/tasks and assert subscribers receive them with correct headers. Verify durable consumers and replay. Negative tests for unauthorized subjects. Schema validation on received events."
          },
          {
            "id": 6,
            "title": "Implement webhook endpoint with HMAC signature verification and replay protection",
            "description": "Expose POST /v1/webhooks/{plugin_id} to receive external events securely and enqueue processing.",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "Add FastAPI route to accept raw body, parse minimal metadata, and verify signatures using HMAC-SHA256 with per-tenant/plugin secret from the R2 seam. Support headers like X-Plugin-Signature, X-Plugin-Timestamp, and X-Idempotency-Key; enforce clock skew and nonce replay window via Postgres store. On success, publish to NATS and enqueue Celery on_event. Return 202 for async processing. Rotate secrets without downtime by supporting multiple active keys. Log audit entries without leaking sensitive data.",
            "status": "pending",
            "testStrategy": "Unit tests for signature computation and verification, including tampered body, wrong key, and expired timestamp. Replay attack tests using the same nonce. Integration test ensures 202 response, NATS publish, and Celery task execution. Load test basic RPS with signature checks."
          },
          {
            "id": 7,
            "title": "Create MCP server scaffold to expose plugin tools safely",
            "description": "Stand up an MCP server that enumerates tools from manifests and mediates safe invocation per tenant.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement an MCP server process within the service using a Python MCP library. Build a tool registry that maps manifest-declared capabilities to MCP tools, enforcing tenant scoping and RBAC. Add version negotiation and backward-compatible tool schemas. Route tool calls through Celery when long-running, and sandbox execution contexts to prevent cross-tenant data access. Emit audit logs and metrics for tool listings and invocations. Document security boundaries and capability whitelisting.",
            "status": "pending",
            "testStrategy": "MCP client contract tests list tools for a tenant and invoke a no-op safely. Permission tests ensure a tenant cannot access another tenant’s tools. Schema version negotiation tests between client and server."
          },
          {
            "id": 8,
            "title": "Implement example plugin (logger/echo) with full lifecycle and defaults disabled",
            "description": "Ship a simple plugin demonstrating manifest, hooks, events, webhooks echo, and MCP tool exposure.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create a plugin package with manifest (id, version, apiVersion, capabilities=['logging','webhook-echo']), default config, and disabled-by-default flag. Implement hooks: on_install (store default state), on_enable (subscribe/init), on_event (log and echo payload), on_uninstall (cleanup). Register an MCP tool to echo input. Use NATS to publish plugin-specific events. Store secrets via the seam and route webhook verification. Add catalog seeding to auto-register the plugin for demo.",
            "status": "pending",
            "testStrategy": "End-to-end flow: call API to install/configure/enable, send a signed webhook, observe Celery processing and NATS events, and invoke the MCP echo tool. Validate logs and state updates. Negative tests for disabled-state behavior and invalid configs."
          },
          {
            "id": 9,
            "title": "Build comprehensive test suite and QA harness (lifecycle, webhooks, retries, Flagsmith)",
            "description": "Create automated tests, fixtures, and CI wiring to validate functionality, security, and versioning.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Assemble pytest suite with async fixtures for Postgres, Redis, NATS, and Flagsmith mocks. Cover lifecycle state machine, manifest/config validation, webhook signature and replay protection, Celery retry/backoff, NATS delivery guarantees, and MCP tool exposure. Add multi-tenant isolation tests and RBAC checks. Validate API versioning stability (/v1) and event schema version headers. Provide docker-compose for local integration, seed data, and GitHub Actions job matrix. Report coverage and publish test artifacts.",
            "status": "pending",
            "testStrategy": "Integration tests spin real containers (testcontainers); contract tests verify OpenAPI and event schemas; security tests for tenant isolation and authZ; chaos tests for worker crashes and NATS outages; CI executes full matrix with flaky test quarantine."
          }
        ]
      },
      {
        "id": 8,
        "title": "Client App: Open WebUI Doc Chat (Port 3000)",
        "description": "Deploy and configure Open WebUI for document-focused chat integrated with Platform API auth and RAG backend, with tenant branding and citations.",
        "details": "- Deploy Open WebUI instance; configure OIDC through Platform API/Logto; set RAG pipeline endpoint to Knowledge Service.\n- Enable document upload UI; persist chat history; render citations; apply tenant brand assets.\n- Route LLM calls via Kong; pass headers (tenant, request id, trace id).\n- Documentation: user guide.\n",
        "testStrategy": "- Sign-in via Logto; upload docs; chat returns grounded answers with citations.\n- Verify chat history persistence per tenant; ensure headers present in backend calls.\n- Basic load test for 100 concurrent chat sessions.",
        "priority": "medium",
        "dependencies": [
          3,
          6,
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy Open WebUI Doc Chat instance on port 3000",
            "description": "Provision and run an Open WebUI instance for document-focused chat, exposed on port 3000 with persistent storage and basic observability.",
            "dependencies": [],
            "details": "- Deploy via Kubernetes (Deployment, Service, Ingress) or Docker Compose targeting port 3000.\n- Configure persistent volume for uploads and local state; prefer external DB for chat history later.\n- Set environment variables placeholders for OIDC, RAG endpoints, Kong, and tenant keys.\n- Add liveness/readiness probes, basic logging, and structured JSON logs.\n- Expose behind TLS with DNS (e.g., doc-chat.<env>.<domain>), rate-limit at ingress.\n- Prepare k8s Secret/ConfigMap for config values; keep secrets out of images.",
            "status": "pending",
            "testStrategy": "Smoke test: container is healthy, port 3000 responds, TLS valid, logs show no startup errors. Verify readiness/liveness endpoints return 200."
          },
          {
            "id": 2,
            "title": "Configure OIDC SSO via Logto and Platform API (per-tenant)",
            "description": "Set up OIDC authentication with Logto through Platform API, map tenant/roles claims, and implement secure session handling for all tenants.",
            "dependencies": [
              1
            ],
            "details": "- Create OIDC client(s) in Logto; set redirect/callback/logout URLs to the WebUI origin.\n- Configure issuer, client ID/secret, scopes (openid,email,profile) and audience for Platform API.\n- Map ABAC claims (tenantId, roles) into ID/JWT tokens; enforce per-tenant session isolation.\n- Secure cookie/session settings (HttpOnly, SameSite=Lax/Strict), token refresh and logout.\n- Provide per-tenant config templates: OIDC_ISSUER, OIDC_CLIENT_ID, OIDC_CLIENT_SECRET, OIDC_REDIRECT_URI.\n- Store secrets in k8s Secret; rotate via CI/CD variables.",
            "status": "pending",
            "testStrategy": "Interactive: login/logout with Logto; verify tokens contain tenantId and roles and are validated by Platform API. Negative tests: invalid client/secret, expired token. Check session revocation."
          },
          {
            "id": 3,
            "title": "Wire RAG Knowledge Service and route LLM via Kong with headers",
            "description": "Connect the UI to the Knowledge Service for RAG and ensure all LLM calls are routed through Kong with tenant, request, and trace headers.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Set RAG pipeline endpoint to Knowledge Service; configure base URL and timeouts.\n- Route LLM traffic via Kong gateway; enable retries, circuit breaking, and sane timeouts.\n- Inject required headers on outbound requests: X-Tenant-Id, X-Request-Id, X-Trace-Id; forward W3C traceparent if present.\n- Align response schema to include citations metadata for later rendering.\n- Provide per-tenant templates: KONG_URL, RAG_BASE_URL, HEADER_TENANT_KEY, TIMEOUT_MS.\n- Add observability: correlate logs with trace ids; verify through Kong access logs.",
            "status": "pending",
            "testStrategy": "Use a request inspector or Kong logs to confirm header propagation. Call a sample RAG query and validate 200 responses, retries on 5xx, and that requests contain X-Tenant-Id, X-Request-Id, X-Trace-Id."
          },
          {
            "id": 4,
            "title": "Enable document upload UI and chat history persistence (per-tenant)",
            "description": "Turn on document uploads with size/type limits and implement per-tenant chat history persistence with isolation and retention policies.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Enable upload UI; accept PDFs, DOCX, TXT with size limits; validate and virus-scan if available.\n- Store uploads in S3/MinIO or persistent volume; tag objects with tenantId for isolation.\n- Connect ingestion to Knowledge Service indexing endpoint; include tenant scoping in payloads.\n- Configure chat history persistence in Postgres (preferred) with schemas or tenantId columns and indexes.\n- Add retention and export/delete endpoints to satisfy compliance.\n- Migrations and seed scripts included; backfill from existing data if needed.",
            "status": "pending",
            "testStrategy": "Upload various file types, verify successful indexing and that chat history persists across sessions for the same tenant. Ensure cross-tenant isolation by attempting forbidden access and receiving 403/404."
          },
          {
            "id": 5,
            "title": "Implement citations rendering and UX with source previews",
            "description": "Render grounded citations in chat responses with clear numbering, source metadata, hover previews, and graceful fallbacks.",
            "dependencies": [
              3,
              4
            ],
            "details": "- Parse citations from RAG responses (doc id, title, page/section, URL/snippet).\n- Display numbered references inline; clickable to open source; hover to preview snippet.\n- Highlight answer spans linked to sources; handle multi-source answers and missing metadata.\n- Add toggles: show/hide citations; indicate confidence/grounding status if provided.\n- Ensure accessibility (keyboard focus, ARIA labels) and responsive layout.\n- Defensive parsing to avoid UI breaks on malformed payloads.",
            "status": "pending",
            "testStrategy": "Snapshot and visual tests for rendering with 0, 1, and multiple citations. Contract tests using a mocked RAG response schema. Manual check that links open correctly and previews render."
          },
          {
            "id": 6,
            "title": "Apply tenant branding and theme (logos, colors, typography)",
            "description": "Implement per-tenant theming based on claims/config, including assets, color tokens, and typography with accessibility compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Resolve tenant from session claims; load theme config dynamically at runtime.\n- Support logo, primary/secondary colors, font stack, favicon, and custom footer links.\n- Provide per-tenant theme templates (tenant-theme.json) and asset placement guidelines.\n- Use CSS variables/tokens; support light/dark modes and high-contrast.\n- Fallback to default theme if tenant assets missing; cache-bust on updates.\n- Validate contrast ratios (WCAG AA) and ensure no brand overrides break UX.",
            "status": "pending",
            "testStrategy": "Visual regression for multiple tenants; automated contrast checks; manual review to confirm branding applies after login and does not leak across tenants."
          },
          {
            "id": 7,
            "title": "E2E and load tests (100 concurrent sessions) with header/tracing validation",
            "description": "Create end-to-end tests for auth → upload → chat with citations and run load tests with 100 concurrent sessions validating headers, tracing, and persistence.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "- E2E with Playwright/Cypress: login via Logto, upload docs, ask questions, verify grounded citations, and chat history persists.\n- Validate header propagation by intercepting network calls and checking X-Tenant-Id, X-Request-Id, X-Trace-Id; confirm traceparent spans.\n- Load test using k6 or Artillery: ramp to 100 concurrent virtual users, 10–15 min soak; set SLOs (p95<2s UI, p95<1.5s API) and error rate <1%.\n- Capture metrics from Kong and app logs; export JUnit/HTML reports and flamegraphs if tracing available.\n- Provide per-tenant test config templates and seeded test users; run in CI pipeline with nightly schedule.",
            "status": "pending",
            "testStrategy": "Run E2E in CI on each commit; store artifacts (videos, traces). Execute load test in staging; assert thresholds for latency and error rate; verify no cross-tenant data exposure under stress."
          }
        ]
      },
      {
        "id": 9,
        "title": "Client App: Open WebUI Code Chat (Port 3001)",
        "description": "Deploy Open WebUI configured for code-centric chat with syntax highlighting, MCP tool integration via Agent Orchestration, and Platform API auth.",
        "details": "- Deploy second Open WebUI; OIDC via Platform API/Logto; connect to Agent Orchestration endpoints; enable MCP tools for code actions; persist chat history; route LLM via Kong.\n- Tenant branding and docs.\n",
        "testStrategy": "- Login, run an agent-based task through UI; verify MCP tool calls reach Agent Orchestration; syntax highlighting renders; history persists.\n- Headers and tracing validated.",
        "priority": "medium",
        "dependencies": [
          3,
          5,
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy second Open WebUI instance for Code Chat on port 3001",
            "description": "Provision and run a distinct Open WebUI instance dedicated to code-centric chat on port 3001.",
            "dependencies": [],
            "details": "Create Kubernetes/Docker deployment named webui-code-chat on port 3001 with Service/Ingress. Mount ConfigMap/Secrets, enable health probes, and set base env (DATA_DIR/PV for persistence). Apply tenant branding assets (logo/colors) and link to docs entry point. Expose behind Kong/ingress with TLS. Prepare placeholders for OIDC, agent endpoints, and Kong LLM routing.",
            "status": "pending",
            "testStrategy": "Smoke test deployment: readiness/liveness probes pass, homepage loads on 3001 over TLS, static assets and branding render, and config variables are mounted."
          },
          {
            "id": 2,
            "title": "Configure OIDC via Platform API/Logto for Code Chat UI",
            "description": "Enable user authentication using Platform API/Logto for the new WebUI instance.",
            "dependencies": [
              1
            ],
            "details": "Set OIDC provider to Logto via Platform API, configure discovery URL, client id/secret, and redirect/logout URIs for port 3001. Map tenant/role claims for ABAC and ensure tokens are stored securely. Update UI environment to forward Authorization bearer tokens to backend calls. Document sign-in flow and tenant switch UX.",
            "status": "pending",
            "testStrategy": "Auth flow test: login/logout, token refresh, and role/tenant claim presence. Validate 401→302 redirect to Logto and callback success. MSW/Playwright to simulate expired token refresh."
          },
          {
            "id": 3,
            "title": "Connect WebUI to Agent Orchestration Service endpoints",
            "description": "Wire the UI to call Agent Orchestration (Task 5) APIs for agent execution and status.",
            "dependencies": [
              1,
              2
            ],
            "details": "Configure API base URL for Agent Orchestration FastAPI (/v1) and route through Kong. Ensure Authorization and tenant headers (X-Tenant-Id) are forwarded, plus trace headers (traceparent/request-id). Tune timeouts/retries. Coordinate with Task 5 on schemas for POST /v1/agents/execute and GET by id. Document endpoint config and Kong route requirements.",
            "status": "pending",
            "testStrategy": "Integration tests with MSW/mock Agent API to verify request/response schemas, header propagation, retry on 5xx, and timeout behavior. Contract test against staging Agent service when available."
          },
          {
            "id": 4,
            "title": "Enable MCP tools for code actions via Agent Orchestration",
            "description": "Expose code-related MCP tools in the UI and ensure they are executed through the agent workflow.",
            "dependencies": [
              3
            ],
            "details": "Define an allowlist of MCP tools (e.g., repo ops, code search, formatter, linter) and surface tool invocation controls in the chat composer/toolbar. Ensure requests include tool configuration for Agent Orchestration, which routes to MCP clients. Handle tool result rendering (diffs, code blocks). Coordinate with Task 5 tool registry. Update docs explaining available tools and permissions.",
            "status": "pending",
            "testStrategy": "Run agent tasks that require tools and assert tool call intents are present in requests and tool results render correctly. Use a test MCP server or Task 5 sandbox to validate end-to-end tool execution."
          },
          {
            "id": 5,
            "title": "Implement chat history persistence per tenant and user",
            "description": "Persist conversations for the Code Chat UI with per-tenant scoping and retention controls.",
            "dependencies": [
              1,
              2
            ],
            "details": "Back the WebUI with persistent storage (volume or external DB) and enable history save/load APIs. Scope history by tenantId and userId. Implement retention/cleanup policies and export. Ensure encryption at rest where applicable. Migrate schema if needed. Add UI affordances to restore sessions and label threads.",
            "status": "pending",
            "testStrategy": "Create chats across two tenants and users; verify isolation and retrieval after logout/login. Restart pod to confirm persistence. Automated tests assert CRUD operations and retention trimming."
          },
          {
            "id": 6,
            "title": "Configure syntax highlighting and code-focused UX",
            "description": "Enable robust syntax highlighting and code-friendly features across the chat UI.",
            "dependencies": [
              1
            ],
            "details": "Activate syntax highlighting (e.g., Prism/Highlight.js), code fences, copy-to-clipboard, line numbers, and monospace font. Choose light/dark themes aligned with tenant branding. Ensure large snippets virtualize or collapse. Document supported languages and how to switch themes. Validate rendering for diffs from MCP tools.",
            "status": "pending",
            "testStrategy": "Render a curated set of code blocks (TS, Python, JSON, diff) and visually compare. Automated DOM assertions for tokens/classes. Snapshot tests for dark/light themes."
          },
          {
            "id": 7,
            "title": "E2E tests verifying MCP tool calls, tracing headers, and UX",
            "description": "End-to-end tests that cover auth, agent runs with MCP tools, header propagation, syntax highlighting, and history.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Using Playwright/Cypress: sign in via OIDC, start a code task invoking MCP tools, observe network calls to Agent Orchestration with Authorization, X-Tenant-Id, X-Request-Id, and traceparent. Verify tool results appear, syntax highlighting renders, and history persists after reload. Include basic load run and collect traces in APM. Add links to user/admin docs.",
            "status": "pending",
            "testStrategy": "CI E2E suite: record network assertions for headers, compare UI snapshots for highlighted code, and validate chat restored after refresh. Run against staging with Task 5 active and Kong routing enabled."
          }
        ]
      },
      {
        "id": 10,
        "title": "Client App: Admin Dashboard (Next.js, Port 3002)",
        "description": "Implement admin dashboard with tenant CRUD, provider config, quota/usage charts, flags management, plugin toggles, KPIs, and real-time updates.",
        "details": "- Stack: Next.js (App Router), TanStack Table, Recharts/Chart.js, SSE/WebSocket for live KPIs, OIDC with Logto.\n- Pages: Tenants, Providers, Quotas, Usage Analytics (filters), Flags, Plugins, Users/Roles.\n- Components: Provider forms with encrypted key submission; quota editor with thresholds; KPI widgets (cost, latency P95, cache hit, errors, active users).\n- API client for Platform API; role-based UI using ABAC claims.\n- Responsive design.\n",
        "testStrategy": "- Integration tests with MSW mocking Platform API; form validation; auth flows.\n- Visual tests for dashboards; real-time stream renders updates.\n- E2E happy paths in Playwright: create tenant → set provider → send traffic → observe KPIs.",
        "priority": "medium",
        "dependencies": [
          3,
          4,
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Next.js app (port 3002) with OIDC (Logto), App Router, and ABAC guards",
            "description": "Create the admin dashboard base app with authentication and routing.",
            "dependencies": [],
            "details": "Initialize Next.js (App Router, TypeScript) on port 3002; configure Logto OIDC (PKCE) with SSR-safe token retrieval; set up protected routes and middleware; implement useSession/usePermission hooks that read ABAC claims from ID token; add main layout, responsive nav for pages (Tenants, Providers, Quotas, Usage, Flags, Plugins, Users/Roles, KPIs); set env vars (PLATFORM_API_BASE_URL, LOGTO config); basic error boundary and toast system.",
            "status": "pending",
            "testStrategy": "Unit tests for auth guards and hooks (React Testing Library + Jest); mock Logto using test provider; verify unauthorized users are redirected; snapshot layout and navigation."
          },
          {
            "id": 2,
            "title": "Build typed API client SDK with auth headers, TanStack Query, and error mapping",
            "description": "Implement a typed client for Platform API and shared query utilities.",
            "dependencies": [
              1
            ],
            "details": "Generate TypeScript types from OpenAPI (or handwrite) and wrap with a fetch/axios client; inject bearer token and tenant headers; centralize error handling (4xx/5xx mapping to domain errors); expose hooks via TanStack Query (caching, retries, pagination); provide SSE/WebSocket helpers with auth; include rate limit/backoff; ensure ABAC claims are exposed for UI decisions.",
            "status": "pending",
            "testStrategy": "MSW-backed integration tests for CRUD endpoints and error paths; verify auth headers present; test query invalidation and pagination; contract tests against example OpenAPI fixtures."
          },
          {
            "id": 3,
            "title": "Implement Tenants CRUD page with TanStack Table, forms, and ABAC controls",
            "description": "List, create, update, and delete tenants with role-aware UX.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create /tenants with TanStack Table (sorting, filtering, pagination); add drawer/modal forms for create/edit with zod validation; support soft-delete/restore where applicable; show statuses and quotas summary; restrict actions (create/delete) via ABAC guard; optimistic updates with rollback on failure; handle empty/error/loading states; add search and column visibility; responsive layout.",
            "status": "pending",
            "testStrategy": "MSW tests for list/create/update/delete; form validation tests; Playwright E2E: create tenant, edit name, delete and restore; verify action buttons hidden for insufficient roles."
          },
          {
            "id": 4,
            "title": "Create Provider configuration forms with encrypted key submission and rotation",
            "description": "Securely configure provider credentials per tenant with masking and rotation.",
            "dependencies": [
              1,
              2
            ],
            "details": "Build /providers with cards/forms for each provider; fetch server public key and encrypt secrets client-side (WebCrypto/JWE) before POST; mask stored values and show last 4 chars only; support rotate/revoke flows and test connection; ABAC to restrict who can view/edit; surface validation and network errors; audit notes text area; responsive and accessible form controls.",
            "status": "pending",
            "testStrategy": "Unit tests for encryption utility (happy/edge cases); MSW verifies ciphertext is sent (no plaintext); UI tests for mask/unmask behavior and rotation; permission tests for view/edit restrictions."
          },
          {
            "id": 5,
            "title": "Build Quotas editor and Usage Analytics with filters and charts",
            "description": "Edit quota thresholds and visualize usage with filterable charts.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement /quotas with editable thresholds, limits, and alerts (per tenant/provider/model); optimistic save with rollback; add /usage with date range, tenant, provider, model filters; use Recharts or Chart.js for time series and breakdowns; display error/loading/empty states; ABAC guards for editing; export CSV for usage table; ensure responsive charts.",
            "status": "pending",
            "testStrategy": "MSW tests for quota PATCH and usage GET with filters; visual regression for charts (Playwright screenshots); verify CSV export correctness; form validation and optimistic update rollback tests."
          },
          {
            "id": 6,
            "title": "Implement Flags management with segments and per-tenant overrides",
            "description": "Manage feature flags, segments, and overrides with preview.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create /flags to list flags, define segments (rules on attributes), and set per-tenant overrides; support draft/publish workflow if API allows; include evaluation preview by entering sample attributes; ABAC checks for edit vs view; show audit metadata and history; robust empty/error states; integrate with Platform Flags API (e.g., Flagsmith-compatible).",
            "status": "pending",
            "testStrategy": "MSW mocks for flags/segments endpoints; tests for override precedence and preview evaluation; permission-based UI visibility tests; E2E to create a segment and override a flag."
          },
          {
            "id": 7,
            "title": "Develop Plugins management page for enabling/disabling per tenant",
            "description": "Toggle plugins on/off and configure minimal settings where needed.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create /plugins to list available plugins with status, descriptions, and required scopes; implement enable/disable toggles with confirm dialogs; support minimal per-plugin config forms; optimistic toggle with rollback on error; ABAC to restrict management; surface dependency or validation errors; include usage hints and links.",
            "status": "pending",
            "testStrategy": "MSW tests verifying correct endpoints called on toggle; error rollback behavior; UI tests that toggles are hidden/disabled without permissions; E2E enable plugin and verify state persists."
          },
          {
            "id": 8,
            "title": "Users and Roles management with ABAC-driven UI and invites",
            "description": "Manage users, assign roles, and enforce ABAC-based visibility.",
            "dependencies": [
              1,
              2
            ],
            "details": "Build /users to list users, roles, last active; add invite user flow and resend; assign/remove roles per tenant; ABAC-driven UI hides restricted actions and disables when partially allowed; show error states for conflicting role changes; add search/filter; ensure accessibility and responsive design.",
            "status": "pending",
            "testStrategy": "Unit tests for usePermission and guard components; MSW tests for invite, role assignment, and error cases; Playwright E2E: invite user, assign role, verify UI changes for different claims."
          },
          {
            "id": 9,
            "title": "Live KPI widgets and dashboards via SSE/WebSockets with charts",
            "description": "Render real-time KPIs (cost, P95 latency, cache hit, errors, active users).",
            "dependencies": [
              1,
              2
            ],
            "details": "Add /kpis dashboard with widgets and charts fed by SSE/WebSocket; implement EventSource/WebSocket client with auth, exponential backoff, and offline fallback to polling; allow tenant/provider filters; aggregate and smooth series; show last updated, connection status, and error banners; use Recharts/Chart.js; ensure performance on large streams.",
            "status": "pending",
            "testStrategy": "Mock EventSource/WebSocket in tests; simulate bursts, gaps, and reconnects; verify UI throttling and chart updates; Playwright E2E: observe live updates after actions; visual tests for widgets."
          },
          {
            "id": 10,
            "title": "Testing and QA suite: MSW integration, Playwright E2E, visual and a11y",
            "description": "Establish comprehensive test harnesses and CI pipelines.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Set up MSW for all Platform API and flags/plugin endpoints; add Playwright E2E flows (create tenant → set provider → configure quotas → enable plugin → observe KPIs); include visual snapshots, network tracing, and test retries; integrate axe accessibility checks; configure CI (parallel shards, artifacts, coverage thresholds); seed test data utilities.",
            "status": "pending",
            "testStrategy": "Run MSW integration suite in CI; Playwright E2E on PRs and nightly; enforce >80% coverage on critical modules; include visual diff thresholds; a11y checks must pass; produce test reports and traces as artifacts."
          }
        ]
      },
      {
        "id": 11,
        "title": "Client App: Deloitte Dashboard (Next.js, Port 3003)",
        "description": "Build Deloitte superadmin dashboard with cross-tenant aggregates, platform KPIs, audit log viewer, compliance and alert management.",
        "details": "- Stack same as admin; additional data sources: Prometheus, Grafana/Loki APIs, Platform API superadmin endpoints.\n- Features: aggregates (total cost/forecasts, tenant count/health, guardrail alerts), audit search/filter, compliance indicators, alert management UI, export CSV/PDF.\n",
        "testStrategy": "- Integration tests mocking Prometheus/Loki queries and Platform API endpoints.\n- E2E navigation and export tests.\n- Performance: ensure charts render smoothly with large datasets.",
        "priority": "low",
        "dependencies": [
          10,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Next.js Deloitte Superadmin Dashboard (port 3003) with OIDC/Logto and RBAC",
            "description": "Create the base app, routing, layouts, and superadmin-only access with OIDC.",
            "dependencies": [],
            "details": "Initialize Next.js (App Router, TypeScript) on port 3003, reuse Admin app UI patterns, set envs for Logto OIDC and Platform API base URLs. Add auth guard reading ABAC claims to restrict to superadmin. Wire TanStack Query provider with sensible staleTime/cacheTime defaults for caching. Create base pages: Overview, KPIs, Compliance, Audit, Alerts, Exports. Add API route scaffolds (/api/*) for server-side proxies. Provide shared pagination components and URL state sync.",
            "status": "pending",
            "testStrategy": "Auth route protection unit tests with MSW; smoke load app and verify redirect-to-login; check RBAC hides pages for non-superadmin."
          },
          {
            "id": 2,
            "title": "Implement data connectors to Platform API (superadmin), Prometheus, and Grafana/Loki with proxy, retries, and caching",
            "description": "Build robust server-side connectors for all data sources with pagination-friendly queries.",
            "dependencies": [
              1
            ],
            "details": "Create typed clients exposed via Next.js Route Handlers: /api/platform/*, /api/prom/*, /api/loki/*. Handle OIDC token passthrough/refresh, timeouts, exponential backoff, and circuit breakers. Prometheus: range and instant queries, predict_linear for forecasts. Loki: query_range with direction=backward, limit, and continuation tokens. Platform: cursor-based pagination for superadmin endpoints. Add in-memory + Redis caching with TTL, ETags, and cache keys including query params and time windows. Enforce rate limits and request budgets.",
            "status": "pending",
            "testStrategy": "Integration tests using MSW/nock to mock APIs; validate caching TTL, error retries, and pagination cursors for Platform and Loki; contract tests for response shape mapping."
          },
          {
            "id": 3,
            "title": "Build cross-tenant aggregate KPIs and cost forecasts dashboard widgets and tables",
            "description": "Show totals, tenant counts/health, costs/forecasts with filters and paginated tenant breakdowns.",
            "dependencies": [
              2
            ],
            "details": "Server-side compute aggregates via connectors: total cost, forecasted cost (Prom predict_linear), tenant health, error rates, P95 latency, cache hit rate. Render charts (Recharts/Chart.js) and paginated tables (TanStack Table with row virtualization). Provide global filters (time range, tenant tag). Cache results by filter key, use ISR or SSR with revalidate for heavy aggregates. Implement pagination with cursor/offset as appropriate and prefetch next pages.",
            "status": "pending",
            "testStrategy": "Mock data to validate aggregate math and forecast continuity; verify charts render and tables paginate 50k+ tenants smoothly; snapshot and accessibility checks."
          },
          {
            "id": 4,
            "title": "Implement guardrail alerts and compliance indicators with drilldowns and filters",
            "description": "Surface guardrail breaches and compliance status across tenants with paginated grids and details.",
            "dependencies": [
              2
            ],
            "details": "Fetch guardrail metrics from Prometheus and policy/compliance states from Platform API. Compute indicator rollups (pass/warn/fail) and render a paginated, filterable grid with per-tenant drilldowns. Support real-time updates via SSE/WebSocket if available; otherwise poll with background refresh. Cache per-filter results, invalidate on alert state changes. Persist filters in URL. Provide thresholds and mapping rules in config.",
            "status": "pending",
            "testStrategy": "Simulate rules firing via mocked Prometheus/Platform endpoints; verify indicator computation, filters, and pagination; confirm cache invalidation on state updates."
          },
          {
            "id": 5,
            "title": "Audit log viewer with advanced search, LogQL builder, filters, and infinite pagination",
            "description": "Create a searchable audit UI using Loki and Platform API with robust pagination and UX.",
            "dependencies": [
              2
            ],
            "details": "Implement LogQL query builder with filters for tenant, user, action, resource, status, and time range. Use Loki query_range with direction and limit, handling continuation tokens for infinite scroll. Merge with Platform audit endpoints when needed. Add virtualized list, highlight matches, JSON detail drawer, and export current view CSV. Cache recent queries, debounce typing, and support server-side pagination with backpressure to avoid OOM on large ranges.",
            "status": "pending",
            "testStrategy": "Integration tests mocking Loki responses; verify filter-to-LogQL mapping, pagination token handling, virtualized rendering, and CSV of current view; stress test with 1M synthetic lines."
          },
          {
            "id": 6,
            "title": "Alert management workflows: assign, acknowledge, mute, escalate with RBAC and audit trail",
            "description": "Provide full alert lifecycle UI and APIs with optimistic updates and paginated lists.",
            "dependencies": [
              2,
              4
            ],
            "details": "Build views for all alerts with filters and pagination. Implement actions: assign to user, acknowledge with comment, mute with duration, escalate (webhook/ticket). Use Platform API endpoints with idempotency keys and ETags. Show per-alert timeline/audit trail. Apply optimistic updates with rollback on failure. Cache filtered lists and prefetch next pages. Subscribe to SSE for live changes when available.",
            "status": "pending",
            "testStrategy": "Playwright E2E covering assign/ack/mute/escalate; verify RBAC guards, optimistic update behavior, and pagination consistency across filters."
          },
          {
            "id": 7,
            "title": "Export pipeline for CSV and PDF with streaming, background jobs, and secure downloads",
            "description": "Enable exports for aggregates, audit logs, and alerts via chunked server-side jobs.",
            "dependencies": [
              3,
              5,
              6
            ],
            "details": "Create server routes to start export jobs, snapshot current filters, and stream results in chunks (CSV) to avoid memory spikes. Generate PDFs using headless Chromium (Playwright/Puppeteer) or react-pdf. Track job state in Redis with signed URLs and expiration. Respect pagination by iterating cursors/time windows. Apply rate limiting and per-tenant scoping. Provide UI progress, retry, and webhooks/email on completion.",
            "status": "pending",
            "testStrategy": "E2E export of large datasets; verify CSV chunk ordering, PDF layout fidelity, permissions on signed URLs, and resume/retry semantics on failures."
          },
          {
            "id": 8,
            "title": "Performance tuning and caching strategy for large datasets, charts, and log queries",
            "description": "Optimize fetch, render, and memory for 100k+ rows and high-cardinality metrics with clear budgets.",
            "dependencies": [
              3,
              4,
              5,
              6
            ],
            "details": "Profile queries and UI; add server-side aggregation windows and parallelized time-sliced Loki/Prom queries with concurrency limits. Enable HTTP compression and streaming. Use React virtualization, memoization, and selective re-renders. Tune TanStack Query (staleTime, GC, cache size). Set sane pagination defaults and backpressure. Offload CSV formatting to Web Workers. Establish Lighthouse and FPS budgets; document caching keys and invalidation rules.",
            "status": "pending",
            "testStrategy": "k6/Artillery perf tests for connectors and pages; Lighthouse/React Profiler runs with thresholds; regression budgets enforced in CI with alerts on degradation."
          },
          {
            "id": 9,
            "title": "Comprehensive test suite: integration mocks, E2E flows, contracts, and performance",
            "description": "Implement full coverage tests for connectors, KPIs, audit, alerts, exports, and perf budgets.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Set up MSW for browser and Node, nock for server routes. Write contract tests for Platform, Prometheus, and Loki mappings. Build Playwright E2E flows for auth, navigation, filters, alert actions, and exports. Add visual regression tests. Create synthetic data generators (100k+ rows). Parallelize CI, collect coverage, and schedule nightly heavy perf runs. Include pagination and caching edge cases across suites.",
            "status": "pending",
            "testStrategy": "Playwright E2E, MSW/nock integrations, contract schema checks, and k6 perf tests with CI gating and nightly long-run schedules."
          }
        ]
      },
      {
        "id": 12,
        "title": "PostgreSQL Cluster setup (8.1)",
        "description": "Deploy Postgres 15+ with platform DB and per-tenant provisioning capability, pgvector, backups, PITR, PgBouncer, monitoring.",
        "details": "- Compose services: postgres, pgbouncer, backup sidecar (pgBackRest/pg_dump cron), exporters.\n- Initialize platform database; enable pgvector; document per-tenant DB creation flow; SSL/TLS.\n- PITR/backups: daily encrypted backup to MinIO `backups` bucket; retention 30d.\n- Monitoring: slow query logs; Prometheus exporter.\n",
        "testStrategy": "- Run provisioning script to create tenant DB; verify pgvector; restore from backup to meet RPO/RTO; connection pooling via PgBouncer works.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy PostgreSQL 15+ with Docker Compose, persistent volumes, SSL/TLS, and hardened config",
            "description": "Stand up the base PostgreSQL service with durable storage, TLS, and secure defaults using Docker Compose.",
            "dependencies": [],
            "details": "Create a docker-compose.yml with services: postgres (official 15+ image). Mount volumes for data and a separate pg_wal directory. Supply custom postgresql.conf and pg_hba.conf via a config volume. Enable ssl=on with ssl_cert_file and ssl_key_file mounted from secrets; generate an internal CA and node/server certs (or use existing PKI). Enforce scram-sha-256, disable trust, restrict pg_hba to platform networks. Tune sizing: shared_buffers≈25% RAM, effective_cache_size≈50–75% RAM, work_mem per-connection budgeted with PgBouncer, max_connections set moderately (e.g., 200). Keep fsync=on, full_page_writes=on, synchronous_commit=on (or per-RPO). Create initial platform database. Run container as non-root; set read-only configs where possible.",
            "status": "pending",
            "testStrategy": "Boot container; connect with psql using sslmode=require; verify SHOW ssl='on', SELECT current_setting('password_encryption')='scram-sha-256'; restart container and confirm data persists; confirm unauthorized hosts are rejected by pg_hba."
          },
          {
            "id": 2,
            "title": "Enable pgvector extension and create a tenant template with required extensions",
            "description": "Install/enable pgvector and prepare a template DB for tenants with default extensions and settings.",
            "dependencies": [
              1
            ],
            "details": "Install pgvector (use image with pgvector prebuilt or install package). In docker-entrypoint-initdb.d add SQL to CREATE EXTENSION vector in the platform DB. Create a tenant_template database and enable vector and any required extensions (e.g., pg_trgm). Set recommended parameters for vector indexes (e.g., maintenance_work_mem for IVFFLAT builds). Ensure ownership and default privileges are set for future tenant roles. Version-lock extension versions for reproducibility.",
            "status": "pending",
            "testStrategy": "Connect and run SELECT extname FROM pg_extension WHERE extname='vector'; create a sample table with a vector column, build an ivfflat index, insert rows, and run a test ANN query to confirm functionality."
          },
          {
            "id": 3,
            "title": "Deploy and configure PgBouncer with secure auth, TLS termination, and pool sizing",
            "description": "Add PgBouncer service for connection pooling with SCRAM auth, TLS, and right-sized pools.",
            "dependencies": [
              1
            ],
            "details": "Add a pgbouncer service to docker-compose. Configure pgbouncer.ini: auth_type=scram-sha-256, pool_mode=transaction, default_pool_size and max_client_conn sized to workload and Postgres max_connections, server_reset_query=DISCARD ALL. Use databases= mappings to the postgres service. Generate userlist.txt from Postgres credentials (avoid plaintext; store SCRAM secrets). Enable client and server TLS (mount certs/keys); restrict listen_addr to internal networks. Add admin/stats user. Include a pgbouncer_exporter for Prometheus. Harden container (non-root, minimal permissions).",
            "status": "pending",
            "testStrategy": "Connect via PgBouncer with sslmode=require; run SHOW POOLS to verify pooling; simulate concurrent clients and confirm server connections stay bounded; restart Postgres and verify PgBouncer recovers cleanly."
          },
          {
            "id": 4,
            "title": "Implement encrypted daily backups to MinIO via pgBackRest/cron with 30-day retention",
            "description": "Set up a backup sidecar using pgBackRest to push encrypted backups to the MinIO backups bucket with retention.",
            "dependencies": [
              1
            ],
            "details": "Add a backup sidecar container running pgBackRest configured with repo1-type=s3 and MinIO endpoint (minio:9000), bucket 'backups', and path prefix for this cluster/env. Enable TLS verification, configure repo1-cipher-type=aes-256-cbc with a strong cipher pass stored as a secret. Configure archive_command to pgBackRest. Schedule: weekly full, daily incremental, and retain >=30 days of restore points (retention-full and retention-diff/incr tuned accordingly). Enable compression (lz4/zstd). Optionally add a logical pg_dump cron per database as a secondary export. Ensure MinIO bucket policy allows write from backup user only; rotate keys.",
            "status": "pending",
            "testStrategy": "Initialize stanza and run a full backup; verify backup manifests in MinIO and encryption at rest (downloaded object is unreadable). Create test data, run incremental backup, and confirm new objects appear. Validate retention by simulating older backups and observing pruning."
          },
          {
            "id": 5,
            "title": "Configure WAL archiving and document/test Point-In-Time Recovery (PITR)",
            "description": "Enable WAL archiving and create a repeatable procedure to restore to a target timestamp; verify RPO/RTO.",
            "dependencies": [
              4
            ],
            "details": "Update postgresql.conf: wal_level=replica, archive_mode=on, archive_command='pgbackrest --stanza=pg archive-push %p'. Prepare restore configs with restore_command='pgbackrest --stanza=pg archive-get %f %p' and recovery_target_action=promote. Write a runbook and automation script to stop DB, move data dir, run pgbackrest restore with --type=time --target, and start DB to validate recovery. Conduct a controlled test: create table, note timestamp, drop it, and restore to pre-drop time. Capture measured RPO (last archived WAL) and RTO (restore duration). Secure access to repo keys and run tests in non-prod.",
            "status": "pending",
            "testStrategy": "Execute the PITR test script end-to-end; verify the restored database contains pre-drop data and logs show recovery ended at the target time; record RPO/RTO metrics and compare to objectives."
          },
          {
            "id": 6,
            "title": "Enable monitoring: exporters, slow query logs, dashboards, and alerts",
            "description": "Deploy Prometheus exporters for Postgres and PgBouncer, enable slow query logging, and set alert rules.",
            "dependencies": [
              1,
              3
            ],
            "details": "Add postgres_exporter configured with a least-privilege monitoring role. Enable pg_stat_statements (shared_preload_libraries) and create extension in platform DB. Configure log settings: logging_collector=on, log_min_duration_statement=500ms, log_line_prefix with tracing IDs, log_checkpoints=on; optionally enable auto_explain for slow plans. Add pgbouncer_exporter. Wire scrape configs in Prometheus and import Grafana dashboards for Postgres and PgBouncer. Create alerts: exporter down, backup age >25h, high connection utilization, slow query rate, WAL disk usage, and low free disk. Ship logs to centralized logging if available. Harden exporters with read-only access.",
            "status": "pending",
            "testStrategy": "Confirm Prometheus scrapes both exporters; verify Grafana dashboards populate; trigger a slow query and observe log entries and metrics; fire a test alert (e.g., raise backup age metric) and validate notification routing."
          },
          {
            "id": 7,
            "title": "Implement per-tenant provisioning scripts (DB/schema), RBAC, and quotas",
            "description": "Create idempotent scripts/CLI to provision tenants with isolation, roles, timeouts, and PgBouncer entries.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Provide a provisioning tool (e.g., Python/Go CLI or SQL scripts) to create tenant databases from tenant_template (DB-per-tenant) or schemas (schema-per-tenant). Create tenant_<id> login role with strong password (stored in secret manager), grant least privileges, and optional read-only role. Apply sane defaults: statement_timeout, idle_in_transaction_session_timeout, lock_timeout, search_path. Register tenant DB in PgBouncer databases mapping and reload. Apply required extensions per tenant. Enforce quotas via connection limits and monitoring; document storage quotas/process. Include migrate and deprovision flows. Ensure naming conventions and audit logging triggers. Align total pool sizing with Postgres max_connections.",
            "status": "pending",
            "testStrategy": "Provision a sample tenant; connect through PgBouncer using tenant credentials; verify isolation and permissions; run a simple migration; confirm timeouts and connection limits are enforced; deprovision and ensure cleanup."
          },
          {
            "id": 8,
            "title": "Documentation and runbooks: deployment, hardening, operations, and recovery",
            "description": "Produce comprehensive docs for setup, security, tenant provisioning, monitoring, backups, and PITR.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create an architecture overview and Docker Compose setup guide. Write a sizing guide (CPU/RAM/I/O, shared_buffers, pool sizing formulas) and a security hardening checklist (TLS everywhere, SCRAM-only, least-privilege roles, secret storage/rotation, network policies). Document tenant provisioning flow and scripts. Provide backup and PITR runbooks with exact commands and safety checks. Add monitoring dashboards usage and alert response playbooks. Include maintenance (VACUUM/ANALYZE/autovacuum tuning), upgrade procedures, and incident response. Store docs with version control and keep examples/code snippets up to date.",
            "status": "pending",
            "testStrategy": "Have a new engineer deploy the stack and perform tenant creation, backup, and PITR using the docs; capture gaps and update. Peer-review for clarity and completeness."
          }
        ]
      },
      {
        "id": 13,
        "title": "Redis Cluster setup (8.2)",
        "description": "Deploy Redis 7+ for semantic cache, sessions, rate limiting counters, Celery broker, with persistence and monitoring.",
        "details": "- Compose service for Redis with AOF+RDB; eviction policies (allkeys-lru) for cache; expose metrics via exporter.\n- Namespaces/DB indices for distinct uses; security config; TLS optional.\n",
        "testStrategy": "- TTL and persistence behavior validated; benchmark for cache hit; confirm Celery broker connectivity; rate limits increment correctly.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy Redis 7+ with AOF+RDB, HA (Cluster or Sentinel), persistence tuning, and MinIO backups",
            "description": "Provision a production-grade Redis with durable storage, AOF+RDB, high availability, and documented backup and failover procedures.",
            "dependencies": [],
            "details": "Stand up Redis 7+ via Docker Compose or Kubernetes with persistent volumes. Choose HA topology: Redis Cluster (3 masters with 3 replicas) for cache/counters and, if Celery requires, a separate primary–replica pair managed by Sentinel for broker/sessions. Enable AOF and RDB together (appendonly yes, appendfsync everysec, aof-use-rdb-preamble yes; rdb save intervals). Configure maxmemory and cluster or replica settings, TCP keepalive, and auto-aof-rewrite thresholds. Mount fast SSD storage; tune OS fs and vm overcommit. Backups: schedule a sidecar/cron that runs BGSAVE (and periodic BGREWRITEAOF), then uploads dump.rdb and appendonly.aof to the MinIO backups bucket with SSE and 30-day retention. Document restore steps and RPO/RTO. Failover guidance: for Cluster verify replica promotion and slot coverage; for Sentinel verify automatic primary failover and client reconnection strategy. Output connection endpoints for each role.",
            "status": "pending",
            "testStrategy": "Validate persistence by writing data, restarting nodes, and confirming durability from AOF+RDB. Force AOF rewrite and verify files rotate. Execute a simulated failover (kill a master/primary) and ensure replicas promote and clients reconnect. Run a full backup to MinIO, then restore into a fresh instance and confirm keys and TTLs match expectations."
          },
          {
            "id": 2,
            "title": "Define namespaces/DB indices, key schemas, TTLs, and eviction policy per workload",
            "description": "Design logical separation and lifecycles for cache, sessions, counters, and Celery queue keys.",
            "dependencies": [
              1
            ],
            "details": "For Redis Cluster: use DB 0 with strict key prefixes (cache:, session:, ratelimit:, celery:) and documented key schemas. For Sentinel/standalone: optionally map DB indices (db0 cache, db1 sessions, db2 rate limits, db3 celery) while still using prefixes. Specify TTLs: sessions 12–24h, rate limits 1m–1h per policy bucket, semantic cache 5m–7d based on data freshness, Celery queue keys managed by Celery without TTL. Apply global eviction policy allkeys-lru on the cache-focused instance; if differing policies are required, split workloads into separate Redis instances. Define memory budgets per namespace and expected cardinality. Recommend data types (strings and hashes), pipelining for hot paths, and Lua scripts only where needed.",
            "status": "pending",
            "testStrategy": "Seed representative keys for each namespace, verify DB index and prefixing, and assert TTLs are set. Lower maxmemory temporarily to trigger eviction and confirm cache keys are preferentially evicted (or isolated instance protects sessions and queues). Sample keyspace and hit/miss via INFO and key scans without KEYS."
          },
          {
            "id": 3,
            "title": "Configure TLS and ACL-based authentication with least privilege and secret rotation",
            "description": "Enable TLS, create per-service users with minimal commands, disable default access, and restrict network paths.",
            "dependencies": [
              1
            ],
            "details": "Generate a private CA, server certs, and client certs as needed. Enable tls-port 6380 and optionally disable plaintext 6379; set tls-auth-clients yes for mTLS where required. Create ACL users: kong_cache, api_sessions, rate_limit, celery_broker, readonly_admin. Assign minimal commands and key patterns (e.g., kong_cache: get,set,expire,ttl,del on cache:*; rate_limit: incrby,pexpire,get on ratelimit:*; api_sessions: get,set,expire,ttl on session:*; celery_broker: list and stream commands needed by Celery on celery:*). Disable the default user and set strong passwords; store secrets in the platform secret manager. Update clients to use rediss:// URIs with ACL user names. Apply firewall/NetworkPolicy rules to permit only approved clients and monitoring.",
            "status": "pending",
            "testStrategy": "Attempt unauthenticated and wrong-user access and expect denial. Validate mTLS handshake and cipher suites. Confirm each client user can perform only its intended commands on allowed key patterns and is blocked on FLUSHALL, CONFIG, and other dangerous commands. Rotate one user password and verify seamless client credential rollover."
          },
          {
            "id": 4,
            "title": "Expose metrics via redis_exporter and build Grafana dashboards and alerting",
            "description": "Publish Redis metrics to Prometheus, create dashboards, and define SLO-oriented alerts for persistence, HA, and performance.",
            "dependencies": [
              1,
              3
            ],
            "details": "Deploy redis_exporter with authentication and TLS settings targeting each Redis instance or role. Scrape with Prometheus and label by role and namespace. Build or import Grafana dashboards (e.g., popular Redis overviews) showing memory usage, hit ratio, ops, latency, replication lag, key expirations, evictions, AOF and RDB stats, Cluster/Sentinel state. Configure alerts for high evictions, low hit ratio, high latency, replication lag or down replica, exporter down, AOF rewrite stalled, memory fragmentation ratio, and role changes. Add runbooks for common alerts and link to failover and backup procedures.",
            "status": "pending",
            "testStrategy": "Confirm Prometheus scrapes exporter endpoints with valid metrics and labels. Validate dashboards render expected counters. Trigger a controlled eviction by lowering maxmemory and writing keys; observe alerts. Pause a replica to simulate lag and verify alert firing. Check AOF rewrite metrics during forced BGREWRITEAOF."
          },
          {
            "id": 5,
            "title": "Validate Celery broker connectivity, throughput, and HA reconnection behavior",
            "description": "Wire Celery to Redis, run sample tasks, and ensure robustness across node restarts and failovers.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Configure Celery broker_url to Redis with ACL user and TLS (rediss). Prefer Sentinel-managed primary–replica for Celery to simplify HA; if using Cluster, verify the client library supports required commands or keep Celery on the Sentinel pair. Set transport_options (visibility_timeout, socket_timeout, retry_on_timeout) and prefetch/concurrency. Run a sample Celery app with workers and enqueue various tasks including long-running and retrying tasks. Observe Redis queue keys and ensure durability and timely acknowledgements. Perform failover by promoting replica or killing primary and confirm workers reconnect and continue processing without message loss.",
            "status": "pending",
            "testStrategy": "Execute an automated test that enqueues N tasks, asserts completion and ordering, and measures end-to-end latency and throughput. Kill the primary mid-run to force failover and verify zero lost tasks and bounded retry delays. Inspect exporter metrics (commands, errors) and Celery logs for reconnection and retries."
          },
          {
            "id": 6,
            "title": "Benchmark rate limiting counters and semantic cache; verify TTLs, eviction, and hit ratio",
            "description": "Load test INCR-based counters and cache patterns, validate TTL behavior, and tune for target performance.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement scripts for rate limiting using INCR or INCRBY with PEXPIRE on keys like rate:{tenant}:{route}:{period}, plus optional Lua for atomic multi-counter updates. For semantic cache, use prefixed keys storing response payloads with appropriate TTLs and size caps. Drive workloads using memtier_benchmark and redis-benchmark with mixed read/write ratios, pipelines, and concurrency matching production. Measure throughput and p50–p99 latency, cache hit ratio, expired and evicted keys, and memory usage under different maxmemory and allkeys-lru settings. Document recommended TTLs, key sizes, and pipeline batch sizes. Confirm eviction under pressure does not affect session or Celery keys per the isolation design.",
            "status": "pending",
            "testStrategy": "Run repeatable benchmarks and capture Prometheus metrics and Redis INFO snapshots. Assert counters increment and expire on schedule, cache achieves target hit rate after warm-up, and eviction rate and latency remain within SLOs. Produce a short report with tuning changes (maxmemory, pipeline size, shard count) and before–after metrics."
          }
        ]
      },
      {
        "id": 14,
        "title": "MinIO setup (8.3)",
        "description": "Deploy MinIO with buckets, IAM policies, encryption, versioning, lifecycle and monitoring.",
        "details": "- Buckets: documents (per-tenant prefixes), mlflow-artifacts, backups; enable SSE, versioning on critical; lifecycle for older objects; audit logs.\n- Console enabled on 9001; create service users/keys.\n",
        "testStrategy": "- CRUD objects via SDKs; verify bucket policies and versioning; backup job writes and restores from MinIO.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy MinIO server and enable admin console on port 9001",
            "description": "Provision MinIO (single node or HA) with persistent storage, TLS, and the web console exposed on port 9001. Configure admin credentials and basic networking.",
            "dependencies": [],
            "details": "- Platform: Docker Compose or Kubernetes. Use official image minio/minio.\n- Command: start with \"minio server /data --console-address ':9001'\" on ports 9000/9001.\n- Persist data to volumes or PVs; size per capacity plan.\n- Configure MINIO_ROOT_USER and MINIO_ROOT_PASSWORD via secrets.\n- Enable TLS (certs mounted via secret) and health probes.\n- Network: restrict access to 9000/9001 to trusted subnets/ingress.\n- Output connection info for later subtasks (endpoint, admin login).",
            "status": "pending",
            "testStrategy": "- Health: curl /minio/health/ready and /minio/health/live.\n- Console: login at :9001 using admin credentials.\n- Verify data persists across restart."
          },
          {
            "id": 2,
            "title": "Create buckets and IAM policies with per-tenant prefixes and service users/keys",
            "description": "Create buckets documents, mlflow-artifacts, backups. Define IAM policies enforcing least-privilege, add per-tenant prefixes for documents, and create service users/keys.",
            "dependencies": [
              1
            ],
            "details": "- Buckets: create documents, mlflow-artifacts, backups using mc mb.\n- Per-tenant prefix scheme for documents: documents/tenants/<tenant-id>/*.\n- Policies:\n  - docs-tenant-<id>-rw: Allow s3:GetObject, PutObject, DeleteObject on arn:aws:s3:::documents/tenants/<tenant-id>/* and ListBucket with prefix constraint.\n  - mlflow-svc-rw: Allow read/write on arn:aws:s3:::mlflow-artifacts/* (or per-tenant prefix if required later).\n  - backup-job: Allow write to arn:aws:s3:::backups/* and read from documents/* and mlflow-artifacts/* for restore jobs (narrow as needed).\n- Create service users: docs-<tenant-id>, mlflow-svc, backup-svc; generate access/secret keys and store in secret manager.\n- Apply policies to users via mc admin policy and mc admin user.\"",
            "status": "pending",
            "testStrategy": "- mc ls to verify buckets exist.\n- Attempt to write outside allowed prefixes and ensure AccessDenied.\n- For a docs-<tenant-id> user, list only documents/tenants/<tenant-id>/ prefix.\n- Store and retrieve credentials from secret manager successfully."
          },
          {
            "id": 3,
            "title": "Configure server-side encryption (SSE) and key management for all buckets",
            "description": "Enable default SSE per bucket and integrate with a KMS. Create and assign distinct keys for documents, mlflow-artifacts, and backups, with rotation procedures.",
            "dependencies": [
              2
            ],
            "details": "- Choose SSE-KMS with MinIO KES (backed by Vault or file keystore) or SSE-S3 as fallback.\n- Create keys: sse-documents, sse-mlflow, sse-backups in KMS.\n- Set default encryption per bucket: use mc encrypt set sse-kms <key> on each bucket so clients do not need headers.\n- Store KMS credentials/config in secrets; restrict KMS access to MinIO.\n- Document key rotation process and schedule; test re-encryption on new uploads.\n- Ensure compatibility with SDKs (no client-side changes required).",
            "status": "pending",
            "testStrategy": "- Upload objects without SSE headers and verify encryption via mc stat (sse: kms) or object metadata.\n- Confirm GET/PUT operations work with standard SDKs.\n- Rotate a key and verify subsequent uploads use the new version."
          },
          {
            "id": 4,
            "title": "Enable bucket versioning and lifecycle rules for aging and noncurrent objects",
            "description": "Turn on versioning for critical buckets and configure lifecycle policies to expire or transition older/noncurrent objects and incomplete multipart uploads.",
            "dependencies": [
              2,
              3
            ],
            "details": "- Enable versioning: documents and mlflow-artifacts (backups optional per retention policy). Use mc version enable <alias>/<bucket>.\n- Lifecycle rules:\n  - documents: expire noncurrent versions after 90 days; delete incomplete multipart after 7 days.\n  - mlflow-artifacts: expire noncurrent after 30 days; optionally delete objects in stale runs after N days using prefix filters.\n  - backups: delete objects older than 30–90 days based on retention requirements.\n- Implement with mc ilm add JSON rules and prefix filters for tenants where applicable.\n- Document recovery steps using version IDs.",
            "status": "pending",
            "testStrategy": "- Upload, modify, and delete a test object; list versions with mc ls --versions and restore a prior version.\n- In non-prod, set short lifecycles (e.g., 1 day) to observe rule application.\n- Verify incomplete multipart uploads are cleaned up after threshold."
          },
          {
            "id": 5,
            "title": "Configure audit logging and expose Prometheus metrics with basic alerts",
            "description": "Enable detailed audit logs for API actions and set up metrics scraping for monitoring. Ship audit logs to a central sink and provision dashboards/alerts.",
            "dependencies": [
              1
            ],
            "details": "- Audit: enable MINIO_AUDIT_WEBHOOK_* (or NATS/syslog) to send logs to logging backend (e.g., Loki/ELK). Include object key, requester, IP, action, status.\n- Ensure logs contain tenant prefixes for traceability.\n- Metrics: expose /minio/v2/metrics/cluster for Prometheus; configure ServiceMonitor/target and authentication as needed.\n- Dashboards: import Grafana MinIO dashboards; add alerts for drive space, S3 errors, and high 5xx.\n- Optionally enable bucket-level event notifications for security monitoring.",
            "status": "pending",
            "testStrategy": "- Generate S3 operations and verify corresponding audit entries arrive in the log sink with correct fields.\n- Confirm Prometheus target is up and metrics such as minio_s3_requests_total and minio_disk_storage_used_bytes are populated.\n- Trigger a synthetic alert threshold to ensure alerting pipeline works."
          },
          {
            "id": 6,
            "title": "End-to-end validation: SDK CRUD, policy enforcement, and backup/restore drill",
            "description": "Execute validation tests that exercise CRUD via SDKs, confirm IAM restrictions, verify SSE and versioning behavior, and run a backup and restore simulation.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "- SDK tests: use boto3 or MinIO SDK to PUT/GET/LIST/DELETE in each bucket.\n- Policy checks: as docs-<tenant-id> user, attempt access outside documents/tenants/<tenant-id>/ and expect deny.\n- SSE: verify uploaded objects report server-side encryption; fetch without supplying SSE headers.\n- Versioning: update and delete objects; restore older versions by version ID.\n- Lifecycle: confirm rules applied in non-prod with shortened durations.\n- Backup drill: run job that copies documents and mlflow artifacts to backups; simulate loss and restore from backups; verify integrity with checksums.",
            "status": "pending",
            "testStrategy": "- Automated test script exits non-zero on any failure and outputs a report.\n- Acceptance criteria: all CRUD succeed within scope; unauthorized access denied; SSE present; versioning restore works; lifecycle acts as configured; backup/restore returns dataset to pre-failure state."
          }
        ]
      },
      {
        "id": 15,
        "title": "Logto SSO setup (8.4)",
        "description": "Deploy Logto, configure OIDC provider, register applications (Open WebUI x2, Admin, Deloitte, Platform API), roles, ABAC claims, MFA, webhooks.",
        "details": "- Configure clients with redirects; map roles (superadmin, admin, user); include ABAC attributes in ID token; session policies; lifecycle webhooks to Platform API.\n",
        "testStrategy": "- Login flows across all apps; role mapping verification; MFA where enabled; webhook delivery observed.",
        "priority": "high",
        "dependencies": [
          3,
          8,
          9,
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Provision and deploy Logto service with production-ready baseline",
            "description": "Deploy Logto in the target environment with secure networking, storage, and access to the admin console.",
            "dependencies": [],
            "details": "Install Logto via Helm or Docker with managed Postgres, configure base URL/issuer, TLS, admin account, secrets, and networking. Enable OIDC discovery, JWKS, observability, backup/restore, and restrict admin endpoints. Prepare dev/stage/prod configs and IaC templates.",
            "status": "pending",
            "testStrategy": "Verify /.well-known/openid-configuration and JWKS endpoints, admin login works, TLS valid, health/metrics emit, and backups complete/restore on a sandbox."
          },
          {
            "id": 2,
            "title": "Register OIDC applications (Open WebUI x2, Admin, Deloitte, Platform API resource)",
            "description": "Create client entries for both Open WebUI instances, Admin app, Deloitte partner app, and the Platform API as a protected resource.",
            "dependencies": [
              1
            ],
            "details": "Create clients: WebUI (general) and WebUI Code Chat (port 3001) as public SPA with Authorization Code + PKCE; Admin app as confidential or public per architecture; Deloitte partner as OIDC RP with restricted scopes; define Platform API as a resource/audience and optional M2M client for internal use. Set names, logos, grant types, and secret policies for confidential clients.",
            "status": "pending",
            "testStrategy": "From each client, initiate authorization flow to confirm client registration, ensure only configured grant types are allowed, and that confidential clients require a secret."
          },
          {
            "id": 3,
            "title": "Configure redirect URIs, post-logout URIs, scopes, and audiences per app",
            "description": "Add exact redirect and logout URIs for each environment and define required scopes and API audiences.",
            "dependencies": [
              2
            ],
            "details": "Set allowed redirect and post-logout URIs for WebUI (general), WebUI Code Chat (3001), Admin, Deloitte, across dev/stage/prod. Require PKCE for public clients. Define scopes: openid, profile, email, offline_access, and custom api scopes (platform.read, platform.write). Map Platform API audience identifier and tie scopes to that audience.",
            "status": "pending",
            "testStrategy": "Attempt login with an unknown redirect (expect failure), then with valid redirects (success). Confirm access tokens contain the correct audience and scopes; verify refresh tokens issued only when offline_access is requested."
          },
          {
            "id": 4,
            "title": "Define roles, permissions, and ABAC claim mapping into tokens",
            "description": "Create roles (superadmin, admin, user), set permissions, add ABAC attributes, and map them into ID/access tokens with least privilege.",
            "dependencies": [
              1
            ],
            "details": "Implement role-to-permission sets and ABAC attributes (tenant_id, org_id, project_ids, partner=true). Use a custom claims namespace and expose minimal claims based on requested scopes. Add claim transformation rules and per-app claim filtering. Ensure compatibility with Platform API authorization logic.",
            "status": "pending",
            "testStrategy": "Issue tokens for test users in each role; decode and verify roles and ABAC claims presence only when appropriate scopes are requested. Confirm claims are omitted for apps that do not need them."
          },
          {
            "id": 5,
            "title": "Configure MFA enforcement, token lifetimes, and session policies",
            "description": "Enable TOTP and WebAuthn, enforce MFA by role/app, and set token/session lifetimes and rotation with secure defaults.",
            "dependencies": [
              1,
              4
            ],
            "details": "Enable TOTP and WebAuthn; enforce always-on MFA for admin/superadmin, conditional for Deloitte, optional for standard users. Set ID token 5m, access token 15m, refresh 30d with rotation and reuse detection, session idle 30m and absolute 12h. Require PKCE, disable implicit/hybrid flows, secure cookies, and enable logout (front/back channel). Configure step-up MFA for high-privilege scopes.",
            "status": "pending",
            "testStrategy": "Test MFA enrollment and recovery, verify step-up prompts for platform.write, observe refresh rotation and reuse revocation, confirm session idle/absolute timeouts, and ensure non-PKCE flows are rejected."
          },
          {
            "id": 6,
            "title": "Configure secure webhooks to Platform API for identity lifecycle events",
            "description": "Send signed lifecycle events (user, session, MFA) to Platform API with retries and idempotency.",
            "dependencies": [
              1,
              3,
              4,
              5
            ],
            "details": "Create webhook endpoint to POST to Platform API (e.g., /v1/identity/webhooks) with HMAC signature, timestamp, and idempotency key. Emit events: user.created/updated/deleted, session.started/ended, mfa.enrolled/challenged, consent changes. Configure retry/backoff, DLQ/alerts, IP allowlists, and payload contracts with tenant context.",
            "status": "pending",
            "testStrategy": "Use webhook test sender to deliver events; verify signature and idempotency handling in Platform API logs. Simulate failures to observe retries and DLQ behavior; confirm only allowlisted source IPs succeed."
          },
          {
            "id": 7,
            "title": "Validate per-app OIDC integration and perform security checks",
            "description": "Run end-to-end flows for each app, verify claims, MFA, API access, and execute an OIDC security checklist.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "For both WebUIs, Admin, and Deloitte: complete auth code + PKCE flow, verify minimal ID token claims, correct audience/scopes on access tokens, and successful calls to Platform API. Security checks: no implicit/hybrid grants, PKCE required, exact redirect matches, strong client secrets, RS256/ES256 signatures, JWKS reachable/rotated, nonce handled by clients, CORS/cookie flags correct, logout works, and session/token lifetimes enforced.",
            "status": "pending",
            "testStrategy": "Automate flows with Playwright/Cypress and OIDC conformance tools; attempt malicious redirect URIs, weak grants, and expired tokens; verify failures occur as expected and privileges are enforced by role/ABAC."
          },
          {
            "id": 8,
            "title": "Document SSO configuration and integration guidance for application teams",
            "description": "Create comprehensive docs and runbooks for app onboarding, scopes/claims, MFA, webhooks, and troubleshooting.",
            "dependencies": [
              7
            ],
            "details": "Publish per-app config examples, redirect URI patterns, scope matrices, audience usage, token validation snippets (frontend and backend), step-up MFA guidance, secret rotation, incident response, and change management. Include environment-specific settings and a security checklist. Store docs in repo with versioning and ownership.",
            "status": "pending",
            "testStrategy": "Peer review with app teams; have a new sample app follow the guide to integrate in a sandbox and capture feedback for improvements."
          }
        ]
      },
      {
        "id": 16,
        "title": "Flagsmith setup (8.5)",
        "description": "Deploy Flagsmith, define platform and tenant-level flags, plugin enablement, webhooks, and SDK guides.",
        "details": "- Create segments per tenant; define toggles; integrate webhooks to Platform API; document SDK usage for services.\n",
        "testStrategy": "- Flag updates propagate to Platform API cache; per-tenant overrides verified in services and UI.",
        "priority": "medium",
        "dependencies": [
          3,
          7,
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy Flagsmith (staging and production) with secure access and backups",
            "description": "Provision and configure a self-hosted Flagsmith with staging and production environments, secure admin access, observability, and backups.",
            "dependencies": [],
            "details": "Deploy Flagsmith via Helm or Docker on the existing Kubernetes cluster. Use Postgres 15+ (managed or in-cluster) and enable TLS. Create a project named \"Platform\" with environments: staging and prod. Restrict admin via SSO or SSO-ready local admin. Configure Prometheus metrics, health checks, and alerts. Set backup/restore for Postgres and Flagsmith exports. Document base URLs, secrets, and RBAC for editors vs viewers.",
            "status": "pending",
            "testStrategy": "- Smoke: /health and admin login.\n- Create dummy flag and toggle across envs to verify persistence.\n- Backup/restore drill of the project to confirm disaster recovery."
          },
          {
            "id": 2,
            "title": "Define platform and tenant-level segments and flags with example matrices and rollback plan",
            "description": "Create segments per tenant and plan tier, define platform defaults and tenant overrides, and capture example matrices and rollback procedures.",
            "dependencies": [
              1
            ],
            "details": "Create segments: tenant-{tenantId}, plan-enterprise, plan-pro, beta-testers. Define flags: plugin.agents.enabled (bool), plugin.mcp.enabled (bool), ui.experimental.enabled (bool), llm.default_provider (string), llm.allowed_models (list), sdk.cache_ttl_seconds (int), feature.webhooks.enabled (bool).\nExample matrix (Default | TenantA | TenantB):\n- plugin.agents.enabled: true | true | false\n- plugin.mcp.enabled: false | true | false\n- ui.experimental.enabled: false | true | false\n- llm.default_provider: openai | openai | anthropic\n- llm.allowed_models: gpt-4o,gpt-4o-mini | gpt-4o | claude-3-5\n- sdk.cache_ttl_seconds: 5 | 5 | 10\nRollback plan: enable approvals (if available), keep nightly project exports in Git, use feature state history to revert, and maintain a runbook to restore a known-good export. Document precedence: identity > segment > environment default.",
            "status": "pending",
            "testStrategy": "- Evaluate flags for sample identities (tenantA, tenantB, beta) using SDK evaluate APIs.\n- Verify segment rules and precedence.\n- Perform a dry-run export and import to confirm rollback viability."
          },
          {
            "id": 3,
            "title": "Integrate Flagsmith webhooks to Platform API for cache invalidation and events",
            "description": "Implement inbound webhook endpoint in Platform API that verifies signatures, invalidates caches, and emits events on flag/segment changes.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create endpoint POST /internal/flags/webhook that verifies HMAC signature with a shared secret and enforces IP allowlist. Parse events (feature_state.updated, segment.updated). Ensure idempotency using delivery_id stored in Redis with TTL. Invalidate relevant cache keys (e.g., flags:environment, flags:tenant:{id}) and publish a flags.updated event with {tenantId, changedKeys, timestamp}. Add observability: trace spans, counters, and structured logs. Configure Flagsmith webhooks for staging and prod with retry policy.",
            "status": "pending",
            "testStrategy": "- Unit: signature verification, idempotency, malformed body.\n- Integration: replay sample Flagsmith payload to staging; assert Redis invalidation and event emission.\n- Faults: simulate duplicate deliveries and ensure single cache bust."
          },
          {
            "id": 4,
            "title": "Author SDK integration guides for backend services and UIs (Python, Node, React)",
            "description": "Provide developer guides and examples to initialize SDKs, set identity traits, read flags, handle caching/fallbacks, and UI gating.",
            "dependencies": [
              2,
              3
            ],
            "details": "Produce docs and sample snippets for Python (flagsmith-client), Node/TypeScript (flagsmith-nodejs), and React (flagsmith/react). Cover initialization with environment keys, setting identity traits {tenantId, userId, plan}, evaluating flags, and caching (sdk.cache_ttl_seconds). Include offline mode and fallback values if SDK or network unavailable. Show backend feature toggles (e.g., plugin.agents.enabled) and UI gating. Include an example matrix reference and a quickstart repo with ready-to-run Docker Compose.",
            "status": "pending",
            "testStrategy": "- Doc tests: run sample apps and verify they show expected behavior for TenantA vs TenantB.\n- Change a flag in staging; confirm code reflects changes within TTL.\n- Lint links and code blocks in CI."
          },
          {
            "id": 5,
            "title": "End-to-end tests for propagation, overrides, caching semantics, and rollback of bad pushes",
            "description": "Automate E2E scenarios covering flag update propagation to services/UI, per-tenant overrides, cache TTL, and rollback of erroneous changes.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create test tenants A and B. Use Flagsmith API to toggle flags and measure propagation to Platform API and sample services (target SLA e.g., <10s with TTL=5s). Validate precedence: identity > segment > environment default. Verify webhook idempotency under retries. Introduce a bad push (e.g., disable plugin.agents.enabled for all) and execute rollback using export history/runbook; confirm restoration. Capture metrics and produce a dashboard/report for latency and failure rate.",
            "status": "pending",
            "testStrategy": "- E2E script: 1) set flags, 2) wait TTL+epsilon, 3) assert service/UI state, 4) verify events emitted.\n- Chaos: duplicate webhooks and transient network errors.\n- Rollback drill: revert to last known-good export and re-assert states."
          }
        ]
      },
      {
        "id": 17,
        "title": "Temporal server setup (8.6)",
        "description": "Deploy Temporal server and UI, register worker from Agent Orchestration, configure retry policies and monitoring.",
        "details": "- Compose services for temporal, visibility DB; networking with Agent Orchestration; health and metrics.\n",
        "testStrategy": "- Start sample workflow; verify persistence and retries; UI shows executions.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy Temporal server (core + visibility) and Web UI via Docker Compose",
            "description": "Provision Temporal core services, visibility store, and the Web UI using Docker Compose.",
            "dependencies": [],
            "details": "Create a Docker Compose stack with services: temporal-server (gRPC 7233), temporal-ui, postgres for default persistence, and OpenSearch (or Elasticsearch) for advanced visibility. Configure networks so the stack shares an overlay with Agent Orchestration. Expose health endpoints and metrics (port 8000). Set temporal-sql-tool migration on init for Postgres schemas. Provide an upgrade runbook: back up Postgres and visibility indices; apply schema migrations with maintenance window; roll temporal-server by component; verify cluster health; rollback steps documented.",
            "status": "pending",
            "testStrategy": "docker compose up -d; verify gRPC on 7233 and UI loads; run `tctl cluster health`; confirm advanced visibility index creation; check /metrics endpoints expose Temporal metrics."
          },
          {
            "id": 2,
            "title": "Configure Temporal namespaces, retention, and visibility/archival settings",
            "description": "Create and tune namespaces with appropriate history retention and visibility options.",
            "dependencies": [
              1
            ],
            "details": "Using tctl or API, create the default and environment-specific namespaces (e.g., prod, staging). Set workflow history retention (e.g., 7–30 days based on environment). Enable and validate advanced visibility for each namespace. If an object store is available, configure archival (history/visibility) to S3-compatible storage with correct URIs and credentials. Document naming conventions and per-tenant namespace strategy.",
            "status": "pending",
            "testStrategy": "tctl namespace list; tctl namespace describe to confirm retention/archival; run a short workflow and verify it appears in visibility queries and in UI; confirm retention enforcement via TTL after config change in a test namespace."
          },
          {
            "id": 3,
            "title": "Establish networking and connectivity with Agent Orchestration workers",
            "description": "Enable secure network paths and worker registration from Agent Orchestration to Temporal.",
            "dependencies": [
              1
            ],
            "details": "Create a shared Docker/network segment (e.g., agent_net) and attach Temporal services and Agent Orchestration to it. Expose gRPC 7233 internally and lock external access via firewall/security groups. Configure TLS for Temporal frontend if required (certs and CA distribution to workers). Provide connection parameters to Agent Orchestration workers (address, namespace, task queues). Validate liveness/readiness probes and document connection troubleshooting steps.",
            "status": "pending",
            "testStrategy": "From an Agent Orchestration container, run grpcurl/tctl against temporal-frontend:7233; start a sample worker and verify pollers appear in UI; test TLS handshake success and failure with incorrect certs."
          },
          {
            "id": 4,
            "title": "Define task queues, default retry/backoff policies, and worker timeouts",
            "description": "Standardize task queues and set sane defaults for retries, backoff, and timeouts.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create task queue naming conventions per service/tenant (e.g., svc.{name}.v1). Define default WorkflowOptions/ActivityOptions including StartToClose and ScheduleToClose timeouts, heartbeat intervals, and retry policies (initialInterval, backoffCoefficient, maxInterval, maximumAttempts). Apply server-side dynamic config where applicable and document overrides at worker level. Set queue rate limits and concurrency caps to prevent thundering herds. Document DLQ/escalation approach if used.",
            "status": "pending",
            "testStrategy": "Run a workflow with an activity that intentionally fails to confirm exponential backoff and max attempts; inspect queue stats in UI; change dynamic config and verify it takes effect without restart; validate worker timeouts cause retries as expected."
          },
          {
            "id": 5,
            "title": "Set up monitoring: metrics scraping, dashboards, logs, and alerts for Temporal",
            "description": "Expose metrics, create dashboards, and configure alerts for Temporal health and SLOs.",
            "dependencies": [
              1
            ],
            "details": "Enable Prometheus metrics in temporal-server and system components; add scrape jobs for ports 8000. Import/curate Grafana dashboards for frontend/history/matching/worker metrics (latency, backlog, failures). Centralize logs with labels for services; include gRPC error codes. Add alert rules: service unavailability, queue backlog growth, workflow failure rate, persistence errors. Provide a monitoring/upgrade runbook for dashboard versioning and alert tuning across releases.",
            "status": "pending",
            "testStrategy": "Prometheus discovers and scrapes targets; dashboards show live data; induce a small failure to confirm alert firing and suppression behaviors; verify logs contain correlated request IDs; confirm UI health panel matches metrics."
          },
          {
            "id": 6,
            "title": "End-to-end validation with sample workflows and failure injection; finalize upgrade runbooks",
            "description": "Validate persistence, retries, and visibility using sample workflows and controlled failures.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Implement a sample workflow + activities (e.g., heartbeat, idempotent activity) in the Agent Orchestration worker. Inject failures: activity panic, timeout, heartbeat miss, worker crash, and temporary network partition. Observe retries/backoff, state persistence across restarts, and visibility correctness in UI/queries. Document a comprehensive upgrade/rollback runbook covering DB backups, schema migrations, component order, compatibility matrix, and smoke checks post-upgrade.",
            "status": "pending",
            "testStrategy": "Execute the sample workflow suite; verify successful completions after transient failures; confirm persisted progress after worker/container restarts; run chaos tests (kill -9 worker, stop network) and ensure Temporal resumes; perform a dry-run upgrade in staging using the runbook and pass smoke checks."
          }
        ]
      },
      {
        "id": 18,
        "title": "NATS JetStream setup (8.7)",
        "description": "Deploy NATS with JetStream, define subjects, streams and consumers, retention and dedup, monitoring and tracing.",
        "details": "- Subjects: quota.updated, agent.* , tenant.*, plugin.*, conversation.* (stub).\n- Configure streams and consumers; set dedup window; enable Prometheus metrics; auth and TLS.\n",
        "testStrategy": "- Publish/consume tests for each subject; verify dedup; retention policies enforced.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy HA NATS cluster with JetStream, TLS, and authenticated access",
            "description": "Provision a 3-node NATS cluster with JetStream enabled, secured via TLS and authenticated users/accounts.",
            "dependencies": [],
            "details": "Use Kubernetes with the official NATS Helm chart. Enable JetStream with file storage and persistent volumes, replicas=3. Generate TLS certs via cert-manager and restrict client access to TLS on 4222. Create Operator/Account/User via nsc (JWT/NKey), separate SYS and APP accounts, and least-privilege users per service. Enable cluster routes, liveness/readiness probes, and monitoring port 8222 (internal). Disable no-auth, enforce TLS, and document kube manifests and secrets layout.",
            "status": "pending",
            "testStrategy": "nats status and nats server report jetstream show 3 peers and JS enabled. Connect using NKey over TLS succeeds; unauth or plaintext is rejected. Kill a pod and verify quorum and message availability persist."
          },
          {
            "id": 2,
            "title": "Define JetStream streams/consumers for subjects; document naming and DLQ strategy",
            "description": "Create streams and durable consumers for quota.updated, agent.*, tenant.*, plugin.*, conversation.*. Document conventions and DLQ.",
            "dependencies": [
              1
            ],
            "details": "Map subjects to streams: STREAM_QUOTA (quota.updated), STREAM_AGENT (agent.*), STREAM_TENANT (tenant.*), STREAM_PLUGIN (plugin.*), STREAM_CONV (conversation.*). Use file storage, replicas=3. For events (quota/tenant/plugin/conversation) use Limits retention; for agent.* use WorkQueue retention. Create consumer templates: durable names service-purpose-v1, AckExplicit, AckWait 30s, MaxAckPending tuned per service, DeliverPolicy=New, optional Pull or Push with queue groups. Naming: subjects are lower-case dot-separated nouns and past-tense events; streams UPPER_SNAKE; consumers kebab-case with version suffix. DLQ strategy: app-level terminal failure publishes original payload+headers to dlq.<stream>.<consumer> and a dedicated DLQ stream STREAM_DLQ captures dlq.> with 14d retention. Document examples and JSON configs.",
            "status": "pending",
            "testStrategy": "nats stream add/list shows defined streams; nats consumer add/list shows durables. Publish sample messages to each subject; verify correct stream assignment and delivery to matching consumers. Publish to dlq.<...> and confirm STREAM_DLQ capture."
          },
          {
            "id": 3,
            "title": "Configure deduplication windows and retention policies per stream",
            "description": "Set per-stream duplicate windows and retention/limit settings; standardize message id usage.",
            "dependencies": [
              2
            ],
            "details": "Enable stream-level duplicates window: 2m for events, 5m for agent work-queue. Clients must set Nats-Msg-Id from a stable event_id/idempotency key. Retention: events keep 7d or 10M msgs/20GB (whichever first), DiscardOld, MaxMsgSize 1MB. agent.* work-queue uses MaxAge 24h, MaxMsgs 5M, MaxAckPending tuned to consumer throughput. Configure BackOff for retries (e.g., 1s,5s,30s,2m) and MaxDeliver=5. Document per-stream config and rationale, including subject roll-up considerations.",
            "status": "pending",
            "testStrategy": "Publish duplicates (same Nats-Msg-Id) and verify only one stored/delivered. Lower limits in a test stream and confirm DiscardOld evicts oldest. For work-queue, simulate slow consumer and verify redeliveries respect BackOff and MaxDeliver."
          },
          {
            "id": 4,
            "title": "Enable Prometheus metrics, dashboards, and tracing hooks for NATS/JetStream",
            "description": "Expose NATS and JetStream metrics to Prometheus, add Grafana dashboards/alerts, and wire tracing context propagation.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Deploy nats-exporter to scrape varz/connz/routez/jetstreamz and expose /metrics. Add ServiceMonitor/PodMonitor and Grafana dashboards for cluster health, stream storage, consumer lag, redeliveries, and DLQ rate. Create alert rules (e.g., consumer lag > N for M minutes, near-capacity storage). For tracing, standardize W3C traceparent in NATS headers and instrument producer/consumer middleware to start/propagate spans via OpenTelemetry SDKs; route to an OTel Collector and backend (e.g., Tempo/Jaeger).",
            "status": "pending",
            "testStrategy": "Prometheus shows nats_* metrics and dashboards render correctly. Force a lag and see alert fire. Publish/consume a traced message and verify a single distributed trace with producer and consumer spans linked via traceparent."
          },
          {
            "id": 5,
            "title": "Bootstrap standardized NATS/JetStream client libraries for all services",
            "description": "Provide shared client modules (Go, Python, Node) for secure connect, JS context, idempotent publish, and durable consume.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create reusable client packages: TLS/NKey auth, connection pooling, auto-reconnect/backoff, JS context helpers. Implement idempotent publishers that set Nats-Msg-Id from event IDs. Provide consumer helpers with AckExplicit, backoff, dead-letter publish to dlq.<stream>.<consumer>, and optional batch pull. Add message schema contracts, header conventions (traceparent, tenant-id), and env-based config (NATS_URLS, TLS paths, account creds). Deliver examples and minimal service integration PRs.",
            "status": "pending",
            "testStrategy": "Unit-test wrappers with mocks; integration tests against dev NATS to publish/consume each subject. Simulate disconnects to verify reconnect. Verify headers set (traceparent, tenant-id, Nats-Msg-Id) and DLQ publish on terminal errors."
          },
          {
            "id": 6,
            "title": "End-to-end publish/consume, dedup, retention, and DLQ validation suite",
            "description": "Build automated tests and scripts to validate E2E behavior across subjects, including dedup, retention expiry, and DLQ flows.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Create CI job (Docker Compose or test namespace) to run nats CLI and service-based tests. Scenarios: publish to each subject and verify consumer receipt; inject duplicates to confirm single delivery; force NACKs to trigger retries then DLQ publish; validate retention by setting low limits and observing eviction; confirm trace context continuity and metrics emission. Produce a runbook and documentation of naming conventions and DLQ usage with acceptance criteria.",
            "status": "pending",
            "testStrategy": "CI pipeline runs suite and exports reports: counts match expectations, duplicates suppressed, DLQ contains terminal failures, traces correlate, and Prometheus shows expected metrics. Fail pipeline on any unmet acceptance criteria."
          }
        ]
      },
      {
        "id": 19,
        "title": "Observability stack (8.8)",
        "description": "Deploy Loki, Prometheus, Grafana, OTel Collector, and Langfuse with dashboards, alerts, and tenant-level isolation.",
        "details": "- Prometheus scraping for all services; Loki with retention; OTel Collector exporters; Grafana dashboards for KPIs; alerts for downtime, quotas, error spikes, latency; log redaction policies; tenant labels.\n",
        "testStrategy": "- Verify scrapes; dashboards render KPIs; alert rules fire on synthetic failures; logs redact PII; traces correlate across services.",
        "priority": "high",
        "dependencies": [
          1,
          3,
          5,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define tenant labeling and isolation strategy across traces, logs, and metrics",
            "description": "Establish a consistent tenant ID schema and isolation approach across all observability data types.",
            "dependencies": [],
            "details": "Specify canonical keys (tenant.id, tenant_name) and propagation rules: W3C trace-context headers from edge → services; OTel resource attributes; Prometheus relabel to add tenant labels from k8s annotations; Loki multi-tenant via X-Scope-OrgID; enforce data access via RBAC, folders, and per-tenant query filters. Include PII minimization guidelines.",
            "status": "pending",
            "testStrategy": "Create a test app emitting metrics/logs/traces with tenant headers; verify labels/attributes appear consistently in Prometheus, Loki, and Langfuse. Attempt cross-tenant queries with a restricted Grafana role and confirm access is denied."
          },
          {
            "id": 2,
            "title": "Configure Prometheus scraping targets, discovery, and relabeling",
            "description": "Set up Prometheus to scrape all services with label hygiene and tenant-aware relabeling.",
            "dependencies": [
              1
            ],
            "details": "Use Prometheus Operator/Helm with kubernetes_sd_configs for services, pods, and endpoints. Implement relabel rules to drop noisy labels, cap cardinality, and attach tenant labels from k8s annotations and static scrape configs. Enable TLS for scrape where supported; store credentials in secrets. Add recording rules for latency histograms and cache hit rates.",
            "status": "pending",
            "testStrategy": "Run promtool check config/rules; verify target discovery and scrape success. Inspect example metrics contain tenant label. Confirm metric_relabel drops PII-like labels. Validate recording rule outputs via PromQL."
          },
          {
            "id": 3,
            "title": "Deploy Loki with per-tenant retention and log redaction pipelines",
            "description": "Install a multi-tenant Loki cluster with storage, retention, and PII redaction in ingestion.",
            "dependencies": [
              1
            ],
            "details": "Use loki-distributed Helm chart; back storage by S3/GCS with server-side encryption. Configure limits_config for per-tenant retention and ingestion limits. Deploy Promtail/OTel->Loki exporter with pipeline stages to mask emails, tokens, secrets, and IDs. Enforce TLS, auth (OIDC or tokens), and X-Scope-OrgID. Enable audit logs and at-rest encryption.",
            "status": "pending",
            "testStrategy": "Ingest sample logs containing synthetic PII; query via logcli/Grafana to confirm redaction. Validate tenants cannot read each other’s logs. Run loki-canary and alert on ingestion failures."
          },
          {
            "id": 4,
            "title": "Deploy Langfuse and integrate with platform services",
            "description": "Stand up Langfuse for LLM tracing/analytics and link it into the observability stack.",
            "dependencies": [
              1
            ],
            "details": "Provision Langfuse with Postgres and object storage; enable TLS and OIDC SSO. Configure SDK/API keys per tenant; define project mapping to tenants. Expose ingestion endpoint and connect from services/OTel (where supported). Add outbound links from Grafana panels to Langfuse traces/sessions. Secure secrets in vault/Secrets Manager.",
            "status": "pending",
            "testStrategy": "Send sample traces/events from a test service; verify they appear in Langfuse under the correct tenant. Check access control by tenant/team. Run backup/restore and failover drills."
          },
          {
            "id": 5,
            "title": "Build OTel Collector pipelines and exporters (traces/logs/metrics)",
            "description": "Configure OTel Collector to receive OTLP and export to Loki, Prometheus/Alerting, and Langfuse.",
            "dependencies": [
              1,
              3,
              4
            ],
            "details": "Define receivers (otlp http/grpc), processors (batch, memory_limiter, attributes to inject tenant.id, redaction/transform), and exporters (loki for logs, otlphttp to Langfuse for traces, prometheusremotewrite if needed). Enforce TLS mTLS between collectors and clients. Set routing by tenant or signal type. Use resource detection and tail_sampling by tenant/importance.",
            "status": "pending",
            "testStrategy": "Run otelcontribcol --dry-run; use test telemetry generator to send traffic. Verify exports reach Loki/Langfuse with correct labels. Confirm redaction removes secrets. Load test for backpressure and memory limits."
          },
          {
            "id": 6,
            "title": "Create Grafana dashboards for KPIs: latency, cost, cache hit rate, errors",
            "description": "Design per-tenant Grafana dashboards visualizing core KPIs across Prometheus, Loki, and Langfuse.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Configure data sources (Prometheus, Loki, Langfuse links). Build templated dashboards with tenant variable. Panels: latency P50/P95/P99, error rate, cost per tenant/model, cache hit ratio, throughput, quota usage. Add logs panels with redaction verified. Apply folder permissions per tenant/team and SSO groups. Enable alert annotations and runbook links.",
            "status": "pending",
            "testStrategy": "Open dashboards and validate queries render correctly per tenant. Compare values against synthetic generators. Run access tests to ensure users only see their tenant data. Lighthouse check for dashboard performance."
          },
          {
            "id": 7,
            "title": "Implement alert rules and synthetic checks (downtime, quotas, errors, latency)",
            "description": "Define alerting for availability and reliability with synthetic probes and safe messaging.",
            "dependencies": [
              2,
              5,
              6
            ],
            "details": "Deploy blackbox-exporter for HTTP probes to key endpoints per tenant. Create Prometheus alert rules: uptime, 4xx/5xx spikes, latency SLO breaches, quota thresholds, ingestion failures. Use multi-window burn-rate formulas. Ensure alert text excludes sensitive data and includes tenant, runbook URLs, and trace/log correlation links.",
            "status": "pending",
            "testStrategy": "Trigger synthetic failures and quota exhaustion in staging; confirm alerts fire, dedupe correctly, and include expected context. Validate no PII in notifications. Use promtool to unit test rules."
          },
          {
            "id": 8,
            "title": "Define SLOs and configure Alertmanager routing and escalation",
            "description": "Set SLOs per service/tenant and wire Alertmanager routes to on-call channels.",
            "dependencies": [
              7
            ],
            "details": "Document SLOs (availability, latency, error ratio, freshness) and error budgets per tenant. Configure Alertmanager routing by tenant labels and severity to Slack/PagerDuty/Email. Set inhibit rules, silences, maintenance windows, and rate limits. Store Alertmanager secrets securely; enable TLS/webhooks validation.",
            "status": "pending",
            "testStrategy": "Simulate burn-rate conditions to validate paging vs warning paths. Test silences and maintenance windows. Verify per-tenant routing and escalation policies behave as designed."
          },
          {
            "id": 9,
            "title": "Enable correlation between traces and logs/metrics in Grafana",
            "description": "Attach trace IDs across signals and add deep links to accelerate debugging.",
            "dependencies": [
              3,
              4,
              5,
              6
            ],
            "details": "Propagate traceparent headers from edge; inject trace_id/span_id into logs via OTel processors or app logger. Configure exemplars on latency/error metrics with trace IDs for Grafana Explore. Add Loki queries that extract trace_id and link to Langfuse trace/session pages. Document correlation workflow and guard against leaking payloads.",
            "status": "pending",
            "testStrategy": "From a Grafana panel, jump from a high-latency time series to a specific trace and related logs. Validate that trace/log correlation works per tenant and that sensitive fields are still redacted."
          },
          {
            "id": 10,
            "title": "Run validation drills: end-to-end tests, security and isolation checks",
            "description": "Execute fire drills to validate dashboards, alerts, redaction, and tenant isolation before go-live.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9
            ],
            "details": "Plan and run scenarios: service outage, error spike, latency regression, quota breach, log PII injection attempt, and cross-tenant access attempt. Verify alerting, dashboards, and correlations. Check encryption in transit/at rest, RBAC, and least-privilege for data sources. Capture results and fix gaps.",
            "status": "pending",
            "testStrategy": "Red-team style tests plus automated chaos and load. Confirm all acceptance criteria: alerts within SLA, no PII exposure, no cross-tenant reads, recovery dashboards accurate. Produce a sign-off report."
          }
        ]
      },
      {
        "id": 20,
        "title": "Docker Compose (cross-cutting)",
        "description": "Create docker-compose.yml and docker-compose.dev.yml covering all services with networks, volumes, env templating, health checks, and startup order.",
        "details": "- Define services for Services 1–7 and Infra 8.1–8.8; networks for isolation; volumes for persistence; depends_on with health conditions; resource limits; .env templates; compose profiles.\n- Health checks for each service; shared labels for OTel.\n",
        "testStrategy": "- `docker compose up` starts entire stack; all health checks pass; services discover each other; dev overrides functional.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Author base docker-compose.yml with all services, networks, and volumes",
            "description": "Create the primary compose file enumerating Services 1–7 and Infra 8.1–8.8, including isolated networks and persistent volumes.",
            "dependencies": [],
            "details": "Use Compose spec v3.9. Define named volumes for stateful components (e.g., postgres-data, redis-data, nats-data, minio-data, kong-data, elastic-data as needed). Create multiple networks for isolation (edge, core, data, observability) with internal flags where appropriate. Declare all service blocks with stable service names and network attachments, restart policies, ports, and shared OTel labels (service.name, env). Add anchors/aliases for common options to reduce duplication.",
            "status": "pending",
            "testStrategy": "Run: docker compose -f docker-compose.yml config to validate. Then docker compose up -d and docker network ls/docker volume ls to confirm creation. Verify services show in docker compose ps."
          },
          {
            "id": 2,
            "title": "Implement environment templating and Compose profiles (.env templates)",
            "description": "Add .env templates and profile-based service grouping to control which services run per environment.",
            "dependencies": [
              1
            ],
            "details": "Provide .env.example and .env.dev with all required variables and safe defaults. Use ${VAR:-default} substitutions and env_file for sensitive values. Introduce compose profiles (e.g., dev, infra, observability, workers, llmops) and tag services accordingly. Ensure ports, credentials, and image tags are driven by env. Validate no secrets committed. Provide sample values for local use.",
            "status": "pending",
            "testStrategy": "docker compose --profile dev config to verify substitutions. Start a minimal set: docker compose --profile dev up -d and confirm only profiled services run. Validate env precedence by overriding a value and re-running config."
          },
          {
            "id": 3,
            "title": "Add health checks and depends_on health conditions for startup order",
            "description": "Define robust healthchecks for each service and gate dependencies using condition: service_healthy.",
            "dependencies": [
              1,
              2
            ],
            "details": "For FastAPI services use HTTP GET /health or /live/ready endpoints. For Postgres use pg_isready; Redis use redis-cli ping; NATS a TCP check on 4222 or HTTP /healthz if enabled; Kong use :8001/status; MinIO use /minio/health/ready; MLFlow/Langfuse/Agenta use their HTTP health endpoints. Set interval, timeout, retries, and start_period per service. Apply depends_on with condition: service_healthy to enforce correct ordering. Keep shared healthcheck anchors for reuse.",
            "status": "pending",
            "testStrategy": "docker compose up -d; then docker ps --format '{{.Names}} {{.Status}}' and ensure all target services report (healthy). Intentionally delay a dependency and verify dependents wait. Use docker inspect to verify Health.Status transitions."
          },
          {
            "id": 4,
            "title": "Configure resource limits and logging options for containers",
            "description": "Apply conservative CPU/memory limits and logging options to improve local stability.",
            "dependencies": [
              1,
              3
            ],
            "details": "Set per-service resource constraints (cpus and mem_limit where supported by docker compose; document that deploy.resources may be ignored outside Swarm). Define log options (max-size, max-file) to prevent disk bloat. Tune reservations for DBs and search if present. Ensure critical infra has sufficient memory. Provide overridable env-based defaults for developer machines.",
            "status": "pending",
            "testStrategy": "docker inspect <container> to confirm HostConfig.Memory and NanoCpus reflect desired limits. Run a brief load (e.g., ab/k6) to ensure containers remain responsive and no OOM kills occur. Check docker logs size rotation."
          },
          {
            "id": 5,
            "title": "Set up local TLS certificates and Kong HTTPS configuration",
            "description": "Generate and mount local TLS certs for Kong, enable HTTPS listeners, and document trust steps.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create a script using mkcert or openssl to generate a local CA and leaf certs (e.g., localhost, *.local.test). Store certs in a named volume or bind mount. Configure Kong via env (KONG_SSL_CERT, KONG_SSL_CERT_KEY) and DB-less/declarative config to load certs/SNIs. Expose 8443 for TLS. Optionally add upstream certificate verification toggles for local. Provide OS trust instructions for macOS/Windows/Linux.",
            "status": "pending",
            "testStrategy": "Run the cert script; start stack; curl -vk https://localhost:8443 to confirm TLS handshake. After trusting the CA, curl https://localhost:8443 without -k. Route a sample service through Kong TLS and verify 200."
          },
          {
            "id": 6,
            "title": "Create docker-compose.dev.yml overrides for local development",
            "description": "Provide dev overrides with bind mounts, hot-reload, debug tooling, and port mappings.",
            "dependencies": [
              1,
              2,
              3,
              5
            ],
            "details": "Author docker-compose.dev.yml to extend the base. For app services, switch to local build contexts and commands (e.g., uvicorn --reload or node --watch). Bind-mount source directories, map debug ports, and add extra env toggles. Keep networks/depends_on aligned with base. Add profiles: [dev] and ensure compose picks overrides when provided. Provide volume mounts for package caches where helpful.",
            "status": "pending",
            "testStrategy": "docker compose -f docker-compose.yml -f docker-compose.dev.yml --profile dev up -d. Edit a file and verify hot reload. Confirm debug ports are reachable. Ensure Kong TLS still functions in dev."
          },
          {
            "id": 7,
            "title": "Implement Makefile and orchestration scripts for start/stop/wait/seed",
            "description": "Add developer-friendly scripts to orchestrate startup, wait for health, run migrations/seeds, and teardown.",
            "dependencies": [
              1,
              2,
              3,
              6
            ],
            "details": "Create a Makefile with targets: init, up, down, restart, logs, ps, wait, smoke, seed, reset. Implement a cross-platform bash/python script that polls docker inspect for Health.Status and times out with helpful diagnostics. Add hooks to run DB migrations (Task 21) and minimal seed data after dependencies are healthy. Ensure non-zero exits on failures and colorized, timestamped output.",
            "status": "pending",
            "testStrategy": "make up; then make wait to ensure all dependencies are healthy in order. Verify that make seed runs only after health. Simulate a failure and ensure scripts exit with clear messages. Confirm idempotency of repeated runs."
          },
          {
            "id": 8,
            "title": "Verify service discovery, connectivity, and startup order via smoke tests",
            "description": "Add automated smoke tests to validate DNS discovery, inter-service HTTP/DB/message connectivity, and Kong routing.",
            "dependencies": [
              1,
              2,
              3,
              5,
              6,
              7
            ],
            "details": "Introduce a small test container (alpine/busybox) or Python probe job that executes: getent hosts <service>, curl http://service:port/health, DB connection attempts, Redis PING, NATS publish/subscribe round-trip, MinIO bucket list, and requests via Kong (HTTP and HTTPS). Capture and aggregate results, failing fast on errors. Confirm OTel headers/labels propagate and traces appear if observability profile is enabled.",
            "status": "pending",
            "testStrategy": "make smoke or docker compose run smoke-tester to execute checks. Success criteria: all health endpoints return 200, DB accepts connections, pub/sub returns message, and routed calls through Kong succeed. Validate order by asserting dependent services only test after wait gate."
          },
          {
            "id": 9,
            "title": "Write end-to-end documentation for the compose stack and operations",
            "description": "Document setup, profiles, envs, TLS, orchestration commands, and troubleshooting for developers and ops.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8
            ],
            "details": "Create docs/compose.md and update README with: prerequisites, .env setup, available profiles, how to start/stop, logs, wait and smoke commands, TLS generation/trust steps for Kong, resource limits, common errors, adding a new service, and OTel labeling conventions. Include quick-start and staging parity notes. Provide copy-paste commands and expected outputs.",
            "status": "pending",
            "testStrategy": "Have a teammate follow the guide on a clean machine to bring the stack up from scratch. Track time-to-first-success, ensure smoke tests pass, and capture feedback for doc fixes."
          }
        ]
      },
      {
        "id": 21,
        "title": "Database migrations (cross-cutting)",
        "description": "Set up Alembic per FastAPI service with initial schemas and migration scripts and rollback procedures.",
        "details": "- Platform API: tenants, users, providers, quotas, audit_logs.\n- Agent Orchestration: executions, workflows.\n- Knowledge & RAG: documents, indexes, embeddings.\n- Integrations: plugins, plugin_configs.\n- Provide `make migrate` scripts and docs.\n",
        "testStrategy": "- Run migrations up/down on fresh DBs; verify schemas; rollback works; migrations idempotent across tenants.",
        "priority": "high",
        "dependencies": [
          3,
          5,
          6,
          7,
          12
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap Alembic in each FastAPI service (Platform, Agent, RAG, Integrations)",
            "description": "Initialize Alembic environments and version directories for all four services with proper DB URLs and target metadata.",
            "dependencies": [],
            "details": "Create per-service alembic.ini and versions/ directories. Wire env.py to each service’s SQLAlchemy metadata and load DB URLs from env (PLATFORM_DB_URL, AGENT_DB_URL, RAG_DB_URL, INTEGRATIONS_DB_URL). Enable autogenerate and logging, and add minimal Makefile stubs to invoke Alembic per service.",
            "status": "pending",
            "testStrategy": "Run `alembic current` for each service to ensure environments load, metadata imports succeed, and no exceptions occur."
          },
          {
            "id": 2,
            "title": "Author initial schemas and baseline migration scripts for all services",
            "description": "Define ORM models and create baseline Alembic revisions that build the first schema for each service.",
            "dependencies": [
              1
            ],
            "details": "Platform API: tenants, users, providers, quotas, audit_logs. Agent Orchestration: executions, workflows. Knowledge & RAG: documents, indexes, embeddings. Integrations: plugins, plugin_configs. Include PKs/UUIDs, created_at/updated_at, indexes, and needed FKs. Generate upgrade/downgrade scripts with explicit server defaults and deterministic ordering.",
            "status": "pending",
            "testStrategy": "Apply `upgrade head` on fresh DBs and verify tables/constraints exist; run `downgrade base` and confirm clean drop with no residual objects."
          },
          {
            "id": 3,
            "title": "Establish migration conventions, versioning, and idempotency guard rails",
            "description": "Standardize naming conventions, revision ID strategy, reversible patterns, and checks that prevent unsafe or non-idempotent migrations.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set SQLAlchemy naming_convention for PK/FK/IX/CK/UK. Define revision ID prefixes per service and consistent message format. Configure version_table and (where needed) version_table_schema. Add pre-commit checks to require reversible migrations, no implicit type casts, and explicit server_default changes. Document cross-service compatibility expectations for additive-first changes.",
            "status": "pending",
            "testStrategy": "Run lint to fail on non-reversible migrations; upgrade to head twice to confirm no-op on second run; autogenerate produces no spurious diffs after round-trip."
          },
          {
            "id": 4,
            "title": "Implement per-tenant migration runner using schema-per-tenant strategy",
            "description": "Create a CLI that enumerates tenants and applies service migrations to each tenant schema safely and repeatedly.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Query Platform tenants to derive schema names (e.g., t_{slug}). For tenant-aware services (Agent, RAG, Integrations) set search_path and version_table_schema to the tenant schema; create schema if missing. Use pg_advisory_lock to serialize runs. Support include/exclude tenant filters and dry-run. Make runner re-entrant and resilient to partial failures with retry.",
            "status": "pending",
            "testStrategy": "Create two test tenants, run the runner, verify versions table per schema and tables exist; re-run to confirm idempotency; exclude one tenant and confirm only targeted schemas change."
          },
          {
            "id": 5,
            "title": "Define rollback and backup/restore procedures with safe downgrades",
            "description": "Provide downgrade scripts, transactional guards, and automated backups so migrations can be safely reversed.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add `make backup` to pg_dump per service and per-tenant schemas before upgrades. Enforce transactional DDL where supported; block non-reversible operations unless a compensating path exists. Document canary rollout, N/N+1 write-compat patterns, and emergency downgrade steps with restore from backup.",
            "status": "pending",
            "testStrategy": "Run upgrade to N, trigger backup, then downgrade stepwise to base and restore to N; verify schema parity and sample data integrity via checksums."
          },
          {
            "id": 6,
            "title": "CI validation pipeline for migrations: up/down, idempotency, cross-service compatibility",
            "description": "Set up CI jobs that validate migrations per service and across tenants, enforce idempotency, and run cross-service smoke checks.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Create matrix CI job per service on ephemeral Postgres. Steps: upgrade head, upgrade head again (no-op), downgrade base, upgrade head, run per-tenant runner. Add branch-head detection to prevent divergent histories. Execute cross-service smoke: create a tenant in Platform, then ensure Agent can create a workflow for that tenant and RAG can index a doc—verifying compatibility across revisions.",
            "status": "pending",
            "testStrategy": "CI must pass for PRs: all upgrade/downgrade/idempotency checks green; cross-service smoke endpoints succeed; schema-diff outputs show no unexpected changes."
          },
          {
            "id": 7,
            "title": "Deterministic, idempotent data seeding for dev and demo",
            "description": "Provide repeatable seed scripts to populate example tenants, users, workflows, documents, and plugins for demos.",
            "dependencies": [
              2,
              4
            ],
            "details": "Implement per-service seeding using UPSERTs keyed by natural IDs (e.g., tenant slug, user email). Seed examples: tenant acme, admin user, provider entries, sample agent workflow, RAG documents/index, and a plugin with config. Guard with ENV flags (DEV/DEMO) and allow tenant targeting. Ensure seeds can run multiple times without duplicates.",
            "status": "pending",
            "testStrategy": "Run seeds twice and assert row counts unchanged; perform demo smoke (login, run workflow, query RAG) to confirm seeded data usable."
          },
          {
            "id": 8,
            "title": "Documentation and Makefile targets for migrations, tenants, rollback, and seeding",
            "description": "Write clear docs and provide Make targets that wrap common operations across services and tenants.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Add docs/migrations.md and per-service READMEs with command examples. Make targets: migrate, downgrade, revision, stamp, migrate-tenant, seed-dev, backup, check-migrations. Include env templates for DB URLs, tenant filters, and CI expectations. Document idempotency guarantees and cross-service compatibility checks and how to run them locally.",
            "status": "pending",
            "testStrategy": "Follow the docs on a clean environment to run all targets successfully; validate links via markdown linter and ensure `.PHONY` targets exist and function."
          }
        ]
      },
      {
        "id": 22,
        "title": "Security hardening (cross-cutting)",
        "description": "TLS at Kong, secrets via Docker secrets, env encryption, network segmentation, security headers/CORS, vuln scans, and docs.",
        "details": "- Terminate TLS at Kong; manage certs; Docker secrets for provider keys; encrypt env files at rest; network policies; set strict headers and CORS; run Trivy/Grype scans in CI.\n",
        "testStrategy": "- SSL tests; secrets not readable by containers; CORS behaves as intended; CI security scan passes without criticals.",
        "priority": "high",
        "dependencies": [
          1,
          20
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure TLS termination at Kong with strong ciphers and automated cert management",
            "description": "Set up Kong to terminate TLS using TLS 1.2/1.3, strong cipher suites, and automated certificate issuance and renewal.",
            "dependencies": [],
            "details": "- Enable TLS termination on Kong with min_version TLSv1_2 and prefer TLSv1_3.\n- Disable TLS 1.0/1.1, weak ciphers, compression, and session tickets; enable OCSP stapling and SNI.\n- Configure allowed ciphers (e.g., TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, ECDHE-ECDSA-AES256-GCM-SHA384, ECDHE-RSA-AES256-GCM-SHA384).\n- Automate certificate management via ACME (DNS-01/HTTP-01) or internal PKI; mount certs via volumes; reload Kong on renewal.\n- Enforce HTTP->HTTPS redirects at edge (plugin or proxy config); document cert rotation runbook.",
            "status": "pending",
            "testStrategy": "- Run SSL Labs test and testssl.sh; target A/A+ grade.\n- Validate no TLS 1.0/1.1 and no weak ciphers with nmap --script ssl-enum-ciphers.\n- Verify OCSP stapling present; verify HTTP->HTTPS redirect."
          },
          {
            "id": 2,
            "title": "Implement Docker secrets for sensitive material and define rotation procedures",
            "description": "Use Docker secrets to supply provider API keys and credentials, and provide a safe rotation process.",
            "dependencies": [],
            "details": "- Define secrets in compose (or Swarm) for provider keys, DB creds, and signing keys; mount as files with strict perms (0400) to relevant services.\n- Remove sensitive env vars; read secrets from files at runtime.\n- Create rotation procedure: versioned secret names, rolling restart of services, and atomic switch-over.\n- Store original secret values encrypted (e.g., SOPS) and restrict maintainer access.\n- Add pre-commit checks to block committing plaintext secrets.",
            "status": "pending",
            "testStrategy": "- Exec into containers to confirm secrets are mounted as files with restricted perms and not present as env vars.\n- Simulate rotation: update secret, restart service, verify apps use new value without downtime.\n- Check audit logs/CI to ensure secrets are not printed."
          },
          {
            "id": 3,
            "title": "Encrypt environment files at rest and enforce access controls",
            "description": "Protect .env and config files at rest using encryption and role-based access controls.",
            "dependencies": [
              2
            ],
            "details": "- Use SOPS with age/GPG (or KMS) to encrypt .env and config files; commit only encrypted versions.\n- Provide make targets for sops-edit and CI decryption using OIDC/KMS or repository secrets.\n- Enforce filesystem permissions (0600) and gitignore decrypted artifacts.\n- Limit decryption to least-privileged roles; document key escrow and recovery.\n- Add pre-commit hook to forbid plaintext .env and common secret patterns.",
            "status": "pending",
            "testStrategy": "- Attempt decrypt without proper keys and confirm failure.\n- CI pipeline decrypts during build then cleans up; verify no decrypted files in artifacts.\n- Pre-commit rejects plaintext .env files."
          },
          {
            "id": 4,
            "title": "Define network segmentation and host/container firewall rules",
            "description": "Create isolated networks, restrict lateral movement, and allow only required ingress/egress.",
            "dependencies": [],
            "details": "- Establish Docker networks: public (edge), internal (services), data (DB/minio); mark internal networks as internal: true.\n- Only expose Kong on 80/443; remove host-published ports for internal services.\n- Add host firewall (ufw/iptables) default-deny with explicit allows for 80/443 and required egress.\n- Restrict service-to-service communication via network membership and container-level policies.\n- Document allowed flows and verify with diagrams and policy files.",
            "status": "pending",
            "testStrategy": "- From an unprivileged container, attempt to reach DB/minio and confirm blocked; allowed services should succeed.\n- nmap/ss to confirm only 80/443 open on host.\n- docker network inspect shows correct isolation."
          },
          {
            "id": 5,
            "title": "Set strict security headers and fine-grained CORS policies at the edge",
            "description": "Add secure HTTP headers and CORS rules in Kong, minimizing exposure while preserving app functionality.",
            "dependencies": [
              1
            ],
            "details": "- Configure Kong plugins to set HSTS (includeSubDomains; preload if eligible), X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy, and robust CSP.\n- Implement per-route CORS: explicit allowlist of origins, restricted methods/headers, short max-age, and credentials only when required.\n- Ensure WebSocket and SSE routes are correctly allowed without overbroad CORS.\n- Provide per-environment configuration and safe defaults.\n- Document header rationales and rollback plan.",
            "status": "pending",
            "testStrategy": "- curl -I and OWASP ZAP baseline to verify headers and values.\n- Test allowed origin succeeds (preflight + actual), and a disallowed origin is blocked.\n- Verify HSTS present over HTTPS; confirm no mixed content issues via Lighthouse."
          },
          {
            "id": 6,
            "title": "Integrate Trivy/Grype vulnerability scans in CI with severity gating",
            "description": "Scan images and dependencies in CI using Trivy and Grype, enforce gates, and manage exceptions with expiry.",
            "dependencies": [],
            "details": "- Add CI jobs to run Trivy (image, filesystem, config) and Grype on build artifacts and published images.\n- Cache vulnerability DBs for speed; upload SARIF to code hosting for PR annotations.\n- Set policy to fail on Critical/High; track allowlist with justification and expiry dates.\n- Schedule nightly scans to catch newly disclosed CVEs.\n- Provide dashboards and build badges for visibility.",
            "status": "pending",
            "testStrategy": "- Create a PR with a known vulnerable base image to confirm gate blocks merge.\n- Validate SARIF annotations appear on PRs.\n- Remove a vulnerability and ensure pipeline goes green."
          },
          {
            "id": 7,
            "title": "Add secrets scanning and generate SBOMs with attestations",
            "description": "Detect committed secrets and produce SBOMs for images and code with signed attestations.",
            "dependencies": [],
            "details": "- Enable Gitleaks/ggshield as pre-commit and CI jobs to scan repo and diffs for secrets.\n- Generate SBOMs with Syft for each image/module; store as build artifacts and publish via OCI registry attachments.\n- Sign SBOM attestations with Cosign; verify in CI.\n- Feed SBOMs to Grype/Dependency-Track for continuous monitoring.\n- Document remediation workflow for findings.",
            "status": "pending",
            "testStrategy": "- Commit a seeded test secret to confirm scanners detect and block.\n- Verify SBOM artifacts exist per image and cosign verify-attestation passes.\n- Confirm Grype can consume generated SBOMs."
          },
          {
            "id": 8,
            "title": "Author security documentation and threat model with verification checklists",
            "description": "Create SECURITY.md, runbooks, data flow diagrams, STRIDE threat model, and verification steps.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "- Write SECURITY.md summarizing controls, responsibilities, and SLAs; add runbooks for cert and secret rotation.\n- Produce DFDs and trust boundaries; perform STRIDE threat model for edge and internal paths; record mitigations and residual risks.\n- Add checklists for SSL Labs, header verification, scan gates, and network isolation tests.\n- Include exception management process with expiry and review cadence.\n- Store docs in repo and knowledge base; require peer review.",
            "status": "pending",
            "testStrategy": "- Conduct a peer review of the docs and threat model; address comments.\n- Validate that all referenced checks (SSL Labs, headers, scans, network tests) are executable and documented with evidence.\n- Ensure docs link to CI pipelines and dashboards."
          }
        ]
      },
      {
        "id": 23,
        "title": "Testing strategy (cross-cutting)",
        "description": "Author unit, integration, E2E, load tests and test data generators aligned to AC1–AC10; integrate Langfuse traces.",
        "details": "- Unit tests target domain logic (80%); API integration tests; E2E flows for tenant onboarding, BYO provider, RAG, agent execution; Load tests with k6/Locust for Kong routing, RAG search, multi-tenant isolation; optional chaos tests documented.\n- CI integration to run suites; seed datasets for demo.\n",
        "testStrategy": "- CI runs green; acceptance criteria verified; load targets hit 100 users/1000 req/min; isolation confirmed.",
        "priority": "high",
        "dependencies": [
          1,
          3,
          5,
          6,
          8,
          9,
          10,
          11,
          19,
          20
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish unit test baselines across all services",
            "description": "Create robust unit test suites for each service targeting domain logic with ≥80% coverage and alignment to AC1–AC10.",
            "dependencies": [
              5
            ],
            "details": "Implement pytest/pytest-asyncio + coverage for FastAPI services and Vitest/Jest for the Next.js client. Use factory-based fixtures from the shared generators, mock external I/O (HTTP, DB, queues) via responses/httpretty and dependency overrides. Add property-based tests (Hypothesis) for critical functions, configure coverage gates (per-file and global ≥80%), and include Makefile/npm scripts. Document conventions and structure.",
            "status": "pending",
            "testStrategy": "Run `make test:unit` per service with coverage thresholds enforced in CI. Tests must be deterministic, isolate side effects, and verify domain invariants. Acceptance: coverage ≥80% per service, zero flaky tests across 3 reruns."
          },
          {
            "id": 2,
            "title": "Build API integration test suites per service",
            "description": "Author black-box HTTP integration tests for each API to validate endpoints, auth, RBAC, multi-tenant behavior, and happy/sad paths.",
            "dependencies": [
              5,
              1
            ],
            "details": "Spin up services against ephemeral databases using docker compose profiles. Use pytest + httpx (Python) and Supertest (Node) to call real endpoints behind Kong or service ports. Seed data via generators. Validate auth (OIDC), ABAC claims, pagination, idempotency, and error contracts. Capture OpenAPI contract drift with schema checks.",
            "status": "pending",
            "testStrategy": "CI job boots service + DB, runs tests in parallel shards, and uploads JUnit XML. Acceptance: 100% endpoint coverage for critical resources, auth and tenant isolation verified, contract checks pass with no schema drift."
          },
          {
            "id": 3,
            "title": "Author E2E flows (onboarding, BYO provider, RAG, agent execution)",
            "description": "Implement browser-level E2E tests that exercise end-to-end user journeys through Kong into backend services.",
            "dependencies": [
              2,
              5
            ],
            "details": "Use Playwright with Chromium headed/CI headless. Flows: tenant onboarding, BYO provider key/config, RAG ingestion + query, and agent execution with tools. Run against docker-compose stack; record video/traces; use seeded tenants/providers. Assert UI + API states, audit logs, and cross-service effects. Tag tests with AC1–AC10 mappings.",
            "status": "pending",
            "testStrategy": "Run `playwright test` in CI with retries=1 and video on failure. Acceptance: all 4 flows pass consistently in 3 consecutive runs; total suite time ≤10 minutes; no PII leaks across tenants."
          },
          {
            "id": 4,
            "title": "Implement load testing with k6/Locust for Kong routing, RAG search, and multi-tenant isolation",
            "description": "Create performance and scalability tests to validate throughput, latency, and isolation under load for key paths.",
            "dependencies": [
              2,
              3
            ],
            "details": "k6 for HTTP paths (Kong routing, core APIs) using ramping-arrival-rate to 1000 req/min and 100 concurrent users; Locust for stateful RAG queries and agent flows. Parameterize tenants to validate isolation. Collect metrics to Prometheus/InfluxDB. Define SLAs: Kong added p95 <30ms; core API p95 <300ms; RAG search p95 <1200ms; error rate <1%; success rate ≥99%; sustain 100 users/1000 req/min for 15 min.",
            "status": "pending",
            "testStrategy": "Automate via `make test:load` with dockerized runners. Enforce k6 thresholds; fail build on SLA breach. Acceptance: all thresholds met; per-tenant metrics show no cross-tenant data access; resource usage within limits."
          },
          {
            "id": 5,
            "title": "Create test data generators and seeders",
            "description": "Provide reusable, deterministic generators and seed datasets to support unit, integration, E2E, and load tests.",
            "dependencies": [],
            "details": "Implement Python factories (factory_boy/Faker) and seed scripts for tenants, users, providers, docs, embeddings, and agent workflows. Include CLI flags for size, tenant, and determinism (fixed random seeds). Provide teardown/cleanup and idempotent upserts. Ship demo seeds for local and CI with small/medium/large profiles.",
            "status": "pending",
            "testStrategy": "Dry-run mode validates data shapes; round-trip tests ensure seeded entities are queryable and unique constraints respected. Acceptance: generators work across services; CI seeding completes <60s; idempotency proven by 2 consecutive runs yielding stable counts."
          },
          {
            "id": 6,
            "title": "Define chaos and failure injection plan",
            "description": "Design and automate controlled fault scenarios to test resilience and graceful degradation.",
            "dependencies": [
              4
            ],
            "details": "Use Toxiproxy to inject latency, jitter, packet loss on DB/provider calls; Pumba/Litmus for container restarts and CPU/memory stress. Scenarios: DB outage, provider timeouts, queue backlog, rate limit bursts, Kong upstream errors. Define abort/rollback criteria and safety controls. Document playbooks and expected fallbacks/circuit breaker behavior.",
            "status": "pending",
            "testStrategy": "Run chaos profiles after baseline load passes. Verify alerts fire, timeouts respected, and error budgets not exceeded. Acceptance: no data loss; degraded modes return actionable errors; recovery within 2 minutes post-fault."
          },
          {
            "id": 7,
            "title": "Configure CI matrix and stages for all test suites",
            "description": "Implement CI pipelines to run unit, integration, E2E, load (nightly), and chaos (on-demand) with caching and artifacts.",
            "dependencies": [
              1,
              2,
              3,
              4,
              6,
              5
            ],
            "details": "Use GitHub Actions/GitLab CI matrix per service and Python/Node versions. Jobs: lint/typecheck, unit, integration (docker services), E2E (Playwright), load (nightly), chaos (manual). Enable parallel shards, container reuse, caching (pip/npm), and artifacts (videos, traces, JUnit). Gate merges on unit/integration/E2E. Tag runs with commit/PR and AC1–AC10.",
            "status": "pending",
            "testStrategy": "Trigger on PR and main. Validate concurrency, retry-on-flake, and artifact retention. Acceptance: median PR pipeline ≤15 minutes; required checks enforced; reruns auto for flaky tests once before fail."
          },
          {
            "id": 8,
            "title": "Integrate Langfuse trace instrumentation into automated tests",
            "description": "Add Langfuse client instrumentation to tests to capture spans/traces with rich metadata and CI linkage.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Initialize Langfuse in test harnesses with env-driven keys; attach test IDs, tenant IDs, commit SHA, CI run URL, and AC mappings as tags. Wrap model/tool calls and key API steps; export trace URLs in test reports. Provide local/dev toggles and redaction for sensitive data.",
            "status": "pending",
            "testStrategy": "Smoke run verifies traces appear in Langfuse with correct tags and timings. Acceptance: ≥95% of E2E and ≥80% of integration tests emit traces; traces correlate to CI runs and link back to failing tests."
          },
          {
            "id": 9,
            "title": "Set up reporting dashboards and SLA monitoring for tests",
            "description": "Provide consolidated visibility over functional, performance, and reliability results with SLA tracking and trend analytics.",
            "dependencies": [
              7,
              8,
              4
            ],
            "details": "Publish Allure/JUnit for functional tests; expose k6/Locust metrics to Grafana with panels for p50/p95, RPS, errors, and resource usage by tenant/path. Create Langfuse dashboards for trace latency by scenario. Define SLAs and acceptance criteria inline: meet load targets (100 users/1000 req/min), Kong p95 <30ms, API p95 <300ms, RAG p95 <1200ms, error rate <1%. Automate weekly trend reports.",
            "status": "pending",
            "testStrategy": "Validate dashboards populate from CI runs; add synthetic alert rules on SLA breaches. Acceptance: dashboards load within 3s, show last 30 days trends, and generate a shareable PR comment summary per run."
          }
        ]
      },
      {
        "id": 24,
        "title": "CI/CD pipeline",
        "description": "Create GitHub Actions for lint/format, tests, Docker builds, scans, staging deploy, DB migrations, rollback docs.",
        "details": "- Workflows: build, test, scan, build-push images, deploy compose to staging, run Alembic migrations, rollback strategy; blue-green design documented.\n",
        "testStrategy": "- PRs trigger workflows; images published; staging environment updated automatically; rollback tested in dry-run.",
        "priority": "medium",
        "dependencies": [
          20,
          21,
          22,
          23
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish GitHub Environments and secrets with OIDC and access policies",
            "description": "Create staging environment with required reviewers, define secrets/variables, and set up GitHub OIDC to cloud roles for registry and infra access.",
            "dependencies": [],
            "details": "- Create GitHub Environment \"staging\" with required reviewers and deployment protection rules.\n- Define secrets (DB_URL, REGISTRY, SLACK_WEBHOOK, LANGFUSE/OTEL keys if needed) and env variables (.env templating).\n- Configure GitHub OIDC to assume cloud role (AWS/GCP/Azure) for pulling/pushing images and accessing managed services without long‑lived keys.\n- Document secret ownership, rotation cadence, and least-privilege scopes.\n- Grant GITHUB_TOKEN permissions (contents: read, packages: write) at workflow/job level.\n- Add environment-scoped variables for compose profiles and ACTIVE_SLOT toggles.",
            "status": "pending",
            "testStrategy": "Dry-run a minimal workflow using OIDC to fetch a short-lived token/role and read a non-sensitive resource. Verify environment protection prompts for approval and that secrets are only available in staging jobs."
          },
          {
            "id": 2,
            "title": "Create lint, format, and test workflows with branch protections and dependency caching",
            "description": "Implement GitHub Actions for pre-commit/linters, format checks, unit tests, and set branch protection rules requiring these checks.",
            "dependencies": [],
            "details": "- Workflows: lint.yml (ruff/flake8/eslint), format.yml (black/pretty), test.yml (pytest/coverage) running on PR and push.\n- Use actions/setup-* with caching: pip/poetry (cache keyed by lockfiles), npm/pnpm caches, and pre-commit cache.\n- Parallel matrix for Python/Node versions if applicable; collect coverage artifact and report summary.\n- Configure Branch Protection Rules for main: require PRs, 1-2 reviews, dismiss stale reviews, require status checks (lint, format, tests, scans), signed commits optional.\n- Auto-cancel redundant runs via concurrency groups and use workflow_call to reuse jobs.",
            "status": "pending",
            "testStrategy": "Open a PR with intentional lint and format issues and failing tests; verify checks block merge and caches warm on reruns (reduced duration). Confirm branch protection lists required checks."
          },
          {
            "id": 3,
            "title": "Implement Docker build and multi-arch publish to GHCR with layer caching and provenance",
            "description": "Build images with Buildx for amd64/arm64, push to GHCR with semantic tags, and enable registry and GHA cache for fast rebuilds.",
            "dependencies": [
              1,
              2
            ],
            "details": "- Use docker/setup-qemu-action and docker/setup-buildx-action; build via docker/build-push-action.\n- Targets: linux/amd64, linux/arm64; tags: ghcr.io/ORG/APP:{sha,branch,semver,date}.\n- Enable BuildKit cache-from/to type=gha and registry-backed cache as fallback; mount pip/npm caches during build stages.\n- Include labels (org.opencontainers.*), healthcheck, and SBOM/provenance attestation if supported.\n- Push only on protected branches; PRs build without push. Store digest and manifest list as outputs for downstream jobs.",
            "status": "pending",
            "testStrategy": "Trigger build on a branch; verify manifest contains both architectures, image appears in GHCR, cache hits on subsequent builds (check logs), and outputs expose image digest."
          },
          {
            "id": 4,
            "title": "Add security scans and SBOM generation/upload (CodeQL, Trivy, Syft)",
            "description": "Integrate code and container security scanning and publish SBOM artifacts to GitHub Security and releases/artifacts.",
            "dependencies": [
              2,
              3
            ],
            "details": "- Code scanning: GitHub CodeQL for languages in repo; schedule weekly and on PRs.\n- Container/file scans: Trivy (fs, config, image) and/or Grype; fail on high/critical with allowlist for known issues.\n- SBOMs: Syft or Trivy to generate CycloneDX/SPDX; upload to dependency graph and attach to workflow artifacts.\n- Gate merges: add scan checks to branch protection required list.\n- Store SARIF results; create issue on new critical findings with assignee and labels.",
            "status": "pending",
            "testStrategy": "Inject a known vulnerable package in a throwaway branch; verify CodeQL/Trivy detect it, SARIF appears in Security tab, SBOM is uploaded, and branch protection blocks merge until resolved."
          },
          {
            "id": 5,
            "title": "Staging deploy via Docker Compose with environment provisioning and health gates",
            "description": "Deploy latest images to staging using Compose profiles, environment-scoped secrets, and health checks with automatic rollback trigger points.",
            "dependencies": [
              1,
              3,
              4
            ],
            "details": "- Use self-hosted runner or remote docker context; run docker login to GHCR using GITHUB_TOKEN.\n- Pull images by digest, template .env from staging secrets/vars, and run docker compose up -d with profiles.\n- Health gates: wait for healthcheck status, run smoke tests against /healthz and key endpoints.\n- Concurrency group \"deploy-staging\" to avoid overlapping releases; record deployed manifest digest as an artifact.\n- Provision ephemeral resources if needed (volumes/networks) and clean up on failure.",
            "status": "pending",
            "testStrategy": "Push a commit to main; verify workflow deploys, services become healthy, smoke tests pass, and deployment metadata (digest, version) is stored. Simulate failure to ensure job halts before traffic switch."
          },
          {
            "id": 6,
            "title": "Alembic migration runner with backup and automatic rollback",
            "description": "Add a gated job to run Alembic migrations against staging DB with backup, revision tracking, and downgrade-on-failure.",
            "dependencies": [
              5
            ],
            "details": "- Pre-deploy step: capture current alembic_version; create timestamped DB snapshot/backup.\n- Run alembic upgrade head with statement timeouts and online migrations; log applied revisions.\n- On error: run alembic downgrade -1 (or to captured revision), restore from backup if necessary, and fail job.\n- Persist revision file and logs as artifacts; expose outputs (from_rev, to_rev) for later steps.\n- Add manual approval for destructive ops and idempotency guard to skip if already at head.",
            "status": "pending",
            "testStrategy": "Use a test migration that adds and removes a column. Verify upgrade applies, downgrade restores schema, and that backup can be restored in a dry-run environment."
          },
          {
            "id": 7,
            "title": "Blue‑green deployment toggles and rollback/runbook documentation",
            "description": "Document blue-green design and add toggles to switch active slot and rollback to prior image digest quickly.",
            "dependencies": [
              5,
              6
            ],
            "details": "- Define two Compose stacks (slot A/B) or services with labels; route via proxy labels/env ACTIVE_SLOT.\n- Add workflow inputs: target_slot, rollback_to_digest; implement switch step that re-points traffic after health checks.\n- Write runbooks in docs/ci/ for cutover, rollback, and failure scenarios; include RACI and time-to-restore targets.\n- Add feature flags or env toggles for risky components; record decisions and known-good digests.\n- Ensure audit trail by logging switches and digests to releases or a CHANGELOG entry.",
            "status": "pending",
            "testStrategy": "Perform a staged deployment to inactive slot, validate health, switch traffic, then trigger rollback using stored previous digest. Confirm minimal downtime and that docs match actual steps."
          },
          {
            "id": 8,
            "title": "Pipeline observability, summaries, and notifications (Slack/Email)",
            "description": "Add job summaries, metrics, and notifications for failures/success across workflows with PR comments and chat alerts.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6
            ],
            "details": "- Post rich job summaries (coverage, image tags, SBOM links) and PR comments with next steps.\n- Integrate Slack (or Teams) webhook for failed deploys and critical findings; allow channel override via env.\n- Emit workflow_run events to an aggregator workflow for trend metrics (durations, success rate) and store in artifacts or external sink.\n- Tag releases/deployments with GitHub Deployments API for a timeline; link to logs and dashboards.\n- Add on-call escalation for staging deploy failures (PagerDuty/opsgenie) guarded behind environment secret.",
            "status": "pending",
            "testStrategy": "Force a failing stage in a test branch; verify Slack alert, PR comment with diagnostics, summary artifacts created, and deployment status tracked in GitHub Deployments timeline."
          }
        ]
      },
      {
        "id": 25,
        "title": "Documentation suite",
        "description": "Produce ADRs, OpenAPI docs, user/admin guides, onboarding, runbooks, DR, plugin SDK, release notes.",
        "details": "- Generate OpenAPI from services; write ADRs for key choices; SERVICE_CONTRACTS.md, CONFIGURATION.md, PLUGIN_ARCHITECTURE.md, GUARDRAILS_AND_DLP.md per PRD; troubleshooting and DR procedures; tenant onboarding playbook; release notes template.\n",
        "testStrategy": "- Peer review docs; validate by onboarding a new developer following the guide; OpenAPI renders correctly.",
        "priority": "medium",
        "dependencies": [
          3,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          20
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Author ADRs for key architectural and product decisions",
            "description": "Create and curate ADRs covering authentication, tenancy, events, plugins, data storage, and release/versioning.",
            "dependencies": [],
            "details": "Use MADR template in docs/adr with sequential numbering (ADR-0001...). Cover: tenant model, auth/OIDC, service boundaries, event bus (NATS), plugin contract, data residency/PII, observability, DR/RPO-RTO, versioning and release policy. Add PR template and CODEOWNERS for mandatory reviews. Enable doc linting (Vale, markdownlint, prettier) via pre-commit. Publish ADR index to docsite (Docusaurus/MkDocs) via CI on main.",
            "status": "pending",
            "testStrategy": "ADR PRs require approvals from tech lead and product; lint passes (Vale/markdownlint); docsite build succeeds; cross-links resolve (lychee link check)."
          },
          {
            "id": 2,
            "title": "Generate, lint, and publish OpenAPI specs for all services",
            "description": "Automate OpenAPI schema generation, validation, and publishing with human-friendly rendering.",
            "dependencies": [],
            "details": "Export OpenAPI from each service (e.g., FastAPI /openapi.json). Add Spectral ruleset for linting, bundle with Redocly CLI, render via Redoc/Stoplight. Set CI to regenerate schemas on build, version under docs/api/{service}/v1, and publish to docsite with search and Try-It-Out (mock server if needed). Ensure headers (tenant, request-id, trace-id) documented. Add contract tests for schema stability.",
            "status": "pending",
            "testStrategy": "CI runs spectral/Redocly lint; Redoc pages render without errors; schemas versioned; backward-compat scan on breaking changes; SDK generation smoke test builds."
          },
          {
            "id": 3,
            "title": "Write SERVICE_CONTRACTS.md and CONFIGURATION.md across components",
            "description": "Document inter-service contracts, headers, errors, and configuration knobs with examples.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create SERVICE_CONTRACTS.md detailing endpoints, auth, idempotency, pagination, rate limits, error models, and event schemas. Create CONFIGURATION.md enumerating env vars, defaults, secrets, and tenant overrides; include sample .env and Helm values. Cross-reference OpenAPI and ADRs. Enforce doc linting and link checks in CI. Publish to docsite with versioning and breadcrumbs.",
            "status": "pending",
            "testStrategy": "Peer review with service owners; validate examples via curl/httpie; link checker passes; configuration examples boot a dev stack end-to-end."
          },
          {
            "id": 4,
            "title": "Document PLUGIN_ARCHITECTURE.md and GUARDRAILS_AND_DLP.md",
            "description": "Capture plugin SDK/manifest, lifecycle hooks, security, guardrails, and DLP policies.",
            "dependencies": [
              1
            ],
            "details": "Produce PLUGIN_ARCHITECTURE.md: manifest schema, capabilities, lifecycle hooks (on_install/enable/event/uninstall), webhooks/signing, Flagsmith toggles, storage/secrets, example plugin. Produce GUARDRAILS_AND_DLP.md: policy types, prompt/content filtering, PII handling, tenant isolation, audit logging, configuration examples. Include code snippets and JSON Schemas. Lint docs; publish to docsite; add review workflow with plugin and security owners.",
            "status": "pending",
            "testStrategy": "Validate manifest JSON Schema with examples; run example plugin against lifecycle walkthrough; security review sign-off; link and lint checks pass."
          },
          {
            "id": 5,
            "title": "Create user/admin guides and tenant onboarding playbook",
            "description": "Write step-by-step guides for users/admins and a repeatable onboarding playbook with checklists.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "User guides: Open WebUI doc chat, admin dashboard, superadmin views with screenshots, roles/permissions, troubleshooting. Admin guide: configuration, quotas, guardrails, audit. Onboarding playbook: tenant provisioning steps, domain/OIDC setup, data connectors, success checklist, rollback plan. Include glossary and FAQs. Add doc linting, PR reviews, and publish to docsite. Plan a dry-run with a new developer.",
            "status": "pending",
            "testStrategy": "Conduct onboarding dry-run with a new dev/PM; measure time-to-success; gather feedback and fix gaps; verify screenshots current; links and lints pass."
          },
          {
            "id": 6,
            "title": "Author operational runbooks and disaster recovery procedures",
            "description": "Produce actionable runbooks for common incidents and detailed DR steps aligned to RPO/RTO.",
            "dependencies": [
              3
            ],
            "details": "Create runbooks for auth issues, degraded latency, queue backlog, plugin failures, webhook retries, and Postgres incidents. DR guide: backup/restore, PITR, MinIO backups, failover, comms templates, and verification checklists. Include SLOs, dashboards, and escalation paths. Add game-day scenarios. Enforce linting and review by SRE. Publish to docsite with printable PDFs.",
            "status": "pending",
            "testStrategy": "Run a tabletop/game-day to execute steps; restore-from-backup rehearsal meets RPO/RTO; on-call can resolve a seeded incident using the runbook; lint/link checks pass."
          },
          {
            "id": 7,
            "title": "Define release notes template and end-to-end release documentation process",
            "description": "Create release notes template, automation, and workflow from commit to published notes.",
            "dependencies": [
              1,
              2
            ],
            "details": "Adopt Conventional Commits/Changesets or Release Please to generate CHANGELOG and release notes. Template sections: highlights, features, fixes, breaking changes, migrations, known issues, contributor credits. Define versioning policy (SemVer), approval gates, and rollback steps. CI job publishes notes to GitHub Releases and docsite, notifies Slack/email. Lint docs; add PR checklist; cross-link ADRs and upgrade guides.",
            "status": "pending",
            "testStrategy": "Dry-run a staged release; generated notes match template; migration guidance validated on a test environment; docsite and GitHub Release publish successfully; approvals enforced in workflow."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-10-23T17:02:30.104Z",
      "updated": "2025-10-23T23:36:03.298Z",
      "description": "Tasks for r1-beta context"
    }
  }
}